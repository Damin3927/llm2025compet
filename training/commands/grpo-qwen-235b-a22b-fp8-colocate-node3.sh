#!/bin/bash
################### Slurm åŸºæœ¬è¨­å®š ###################
#SBATCH --partition=P02
#SBATCH --nodes=3                        # â˜…å…¨3ãƒãƒ¼ãƒ‰ã‚’ã™ã¹ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨
#SBATCH --gpus-per-node=8
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=64
#SBATCH --nodelist=osk-gpu[54,56,91]
#SBATCH --job-name=grpo-qwen235b-a22b-fp8-colo
#SBATCH --time=4:00:00
#SBATCH --mem=0
#SBATCH --output=/home/Competition2025/P02/P02U017/llm2025compet/training/logs/grpo-qwen235b_a22b_fp8_colo.out
#SBATCH --error=/home/Competition2025/P02/P02U017/llm2025compet/training/logs/grpo-qwen235b_a22b_fp8_colo.err

################### ç’°å¢ƒ ###################
#export WANDB_DISABLED=true

export REQUIRE_FULL_PREFETCH=1

module unload cuda || true
module unload nccl || true

module purge

module load cuda/12.6
module load nccl/2.24.3

# ã“ã‚Œã‚’æœ€ä¸Šéƒ¨ã® Python ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã®å‰ã«ç§»å‹•
source ~/openr1/bin/activate
python -c "import hf_transfer,sys; print('hf_transfer',hf_transfer.__version__,'@',sys.prefix)"

# ã“ã“ãŒ â€œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ƒ(æ­£)â€ ã«ãªã‚Šã¾ã™
export PREFETCH_DIR="$HOME/.cache/huggingface_mydir/models/Qwen3-235B-A22B"
export HF_HUB_ENABLE_HF_TRANSFER=0      # openr1 venv ã« hf_transfer ãŒå…¥ã£ã¦ã„ã‚Œã°OK
unset HF_HUB_OFFLINE TRANSFORMERS_OFFLINE  # äº‹å‰DLã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¿…é ˆ

# â–¼â–¼â–¼ ã“ã®æœ€çµ‚ç‰ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å·®ã—æ›¿ãˆã¦ãã ã•ã„ â–¼â–¼â–¼
python - <<'PY'
import os, json, sys, pathlib
from huggingface_hub import snapshot_download

repo = "Qwen/Qwen3-235B-A22B"
dest = os.environ["PREFETCH_DIR"]

print(f"[Prefetch] Starting download for {repo} -> {dest}")
try:
    snapshot_download(
        repo_id=repo,
        local_dir=dest,
        allow_patterns=[
            "config.json","*.json","*.txt","*.model","*tokenizer*",
            "model-*.safetensors","safetensors.index.json","pytorch_model.bin.index.json"
        ],
        max_workers=8,
    )
except Exception as e:
    print(f"[ABORT] snapshot_download failed with an exception: {e}", file=sys.stderr)
    sys.exit(43)

# --- å …ç‰¢ãªãƒã‚§ãƒƒã‚¯æ©Ÿæ§‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿®æ­£æ¸ˆã¿ï¼‰ ---
# â˜…â˜…â˜… ä¿®æ­£ç‚¹ â˜…â˜…â˜…
index_path = pathlib.Path(dest) / "model.safetensors.index.json"
if not index_path.is_file():
    print(f"[ABORT] Verification failed: '{index_path.name}' not found in {dest}", file=sys.stderr)
    sys.exit(44)

with open(index_path) as f:
    index_data = json.load(f)

# set() ã‚’ä½¿ã£ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
expected_shards = len(set(index_data["weight_map"].values()))
actual_shards = len(list(pathlib.Path(dest).glob("model-*.safetensors")))

print(f"Shard check: Found {actual_shards} shards, Expected {expected_shards} shards.")
if actual_shards != expected_shards:
    print(f"[ABORT] Shard count mismatch! Expected {expected_shards}, but found {actual_shards}.", file=sys.stderr)
    sys.exit(45)

print(f"âœ… [Prefetch OK] All {actual_shards} shards and index file are present in {dest}")
PY

################### HFã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’NVMeã¸å›ºå®š + äº‹å‰ãƒã‚§ãƒƒã‚¯ ###################
# å…¨ãƒãƒ¼ãƒ‰ã§å…±é€šã—ã¦ãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹ /nvme56 ã‚’å„ªå…ˆï¼ˆç„¡ã„å ´åˆã¯ä»–å€™è£œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
export NVME_BASE=/nvme56
for _p in /nvme56 /nvme12 /nvme34 /nvme78 /nvme /local "$HOME"; do
  if mountpoint -q "$_p"; then NVME_BASE="$_p"; break; fi
done

export HF_HOME="$NVME_BASE/hf-home"
export HF_HUB_CACHE="$HF_HOME/hub"
# TRANSFORMERS_CACHE ã¯éæ¨å¥¨ãªã®ã§ä½¿ã‚ãªã„
unset TRANSFORMERS_CACHE

# ãƒ¢ãƒ‡ãƒ«IDï¼ˆå¿…è¦ãªã‚‰å¤–ã‹ã‚‰ä¸Šæ›¸ãå¯èƒ½ï¼‰
export MODEL_ID="${MODEL_ID:-Qwen/Qwen3-235B-A22B}"
export MODEL_BASENAME="${MODEL_BASENAME:-Qwen3-235B-A22B}"

# é«˜é€ŸDLï¼ˆå¯èƒ½ãªã‚‰ONã«ï¼‰
export HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-1}
# ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒæ¸ˆã¿ã®2å›ç›®ä»¥é™ã ã‘ã€å­¦ç¿’ç›´å‰ã« OFFLINE ã‚’ONã«ã™ã‚‹ï¼ˆâ€»å ´æ‰€ã¯å¾Œè¿°ï¼‰

# ä¿å­˜å…ˆï¼ˆNVMeãªã©æ—©ã„ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
export WANDB_DIR="${NVME_BASE:-/tmp}/wandb"
srun --nodes=3 --ntasks-per-node=1 --gpus=0 -l bash -lc 'mkdir -p "'"$WANDB_DIR"'"'

# ğŸ”„ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ä¿å­˜ã«åˆ‡ã‚Šæ›¿ãˆ
unset WANDB_DISABLED
export WANDB_MODE=offline
# é€ã‚Šå…ˆï¼ˆãƒãƒ¼ãƒ /ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰
export WANDB_ENTITY="neko-llm"
export WANDB_PROJECT="qwen235b-grpo"
# ä½™è¨ˆãªå‡ºåŠ›ã‚’æŠ‘ãˆã‚‹ï¼ˆä»»æ„ï¼‰
export WANDB_CONSOLE=off
export WANDB_SILENT=true
export WANDB_DISABLE_CODE=true
export WANDB_START_METHOD=thread 
export WANDB_GROUP="job-${SLURM_JOB_ID}"

srun -N 3 -n 3 --ntasks-per-node=1 --gpus=0 -l bash -lc '
  test -w "'"$WANDB_DIR"'" \
    && echo "[W&B dir OK] $HOSTNAME -> '"$WANDB_DIR"'" \
    || { echo "[W&B dir NG] $HOSTNAME"; exit 46; }
'

# NVMeç©ºãç¢ºèªï¼†ä½œæˆ
export REQUIRED_FREE_GB=${REQUIRED_FREE_GB:-800}  # ç›®å®‰: 235Bã®fp16 shardsã§~470GBã€ä½™è£•ã‚’æŒã£ã¦
srun --gpus=0 --cpus-per-task=1 -l bash -lc '
  echo "[NVMe/HF] node=$HOSTNAME  NVME_BASE='$NVME_BASE'"
  FREE_GB=$(df -BG "'$NVME_BASE'" | awk "NR==2{gsub(\"G\",\"\",\$4); print \$4}")
  if (( FREE_GB < '$REQUIRED_FREE_GB' )); then
    echo "[ABORT] $HOSTNAME: free ${FREE_GB}GB < '$REQUIRED_FREE_GB'GB on '$NVME_BASE'" >&2
    exit 42
  fi
  mkdir -p "'$HF_HOME'/hub"
  echo "HF_HOME='$HF_HOME'  HF_HUB_CACHE='$HF_HUB_CACHE'"
'

################### æ—©æœŸãƒ‡ãƒãƒƒã‚°ï¼ˆç’°å¢ƒã®è¦‹ãˆã‚‹åŒ–ï¼‰ ###################

srun --gpus=0 --cpus-per-task=1 --mem-per-cpu=8G bash -c '
  echo "[Before cleanup]"; env | grep -E "PYTHON|LD_|CUDA"
  
  # æ˜ç¤ºçš„ã«ä¸è¦ãªå¤‰æ•°ã‚’unsetã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹
  unset PYTHONPATH
  unset LD_PRELOAD
  export LD_LIBRARY_PATH=/home/appli/cuda/12.6/lib64:/home/appli/nccl/2.24.3/lib

  # ä»®æƒ³ç’°å¢ƒã® activate
  source ~/openr1/bin/activate
  echo "[After cleanup and activate]"; env | grep -E "PYTHON|LD_|CUDA"

  # å®Ÿè¡Œ
  python -c "import deepspeed; print(deepspeed.__version__)"
'

# Take a look at the contents of the following environment variables first.
# PATH lists the locations of the executables and LD_LIBRARY_PATH lists where to look for shared libraries.
# Earlier entries are prioritized over later ones, and : is used to separate multiple entries.
# To find a specific CUDA toolkit, insert the correct path to list first.
# In addition, you should also check that the assigned directories actually exist.
# (https://huggingface.co/docs/transformers/debugging#deepspeed-cuda-issues)

export CUDA_HOME=/home/appli/cuda/12.6
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64:$CUDA_HOME/targets/x86_64-linux/lib/stubs:$CUDA_HOME/targets/x86_64-linux/lib:/home/appli/nccl/2.24.3/lib:$LD_LIBRARY_PATH
export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATH

source /home/Competition2025/P02/P02U017/openr1/bin/activate

################## ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ ###################

echo -e "\nğŸ” [DEBUG] CUDA/NCCL ç’°å¢ƒç¢ºèª"

# nvcc ã®ç¢ºèª
echo -n "CUDA nvcc version: "
if ! which nvcc >/dev/null 2>&1; then
  echo "âŒ nvcc ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (PATHã« $CUDA_HOME/bin ã‚’å«ã‚ãŸã‹ç¢ºèª)"
else
  nvcc --version | grep release
fi
echo -e "\nğŸ” nvcc å€™è£œä¸€è¦§:"; which -a nvcc

# Python ç’°å¢ƒç¢ºèª
echo -n "ğŸ§ª Python: "; which python
python -c "import sys; print(f'Venv Prefix: {sys.prefix}')"

# PyTorch
python -c "import torch; print(f'Torch Version: {torch.__version__} | CUDA Available: {torch.cuda.is_available()}')"

# CUDA_HOME ãƒã‚§ãƒƒã‚¯
if [ ! -d "$CUDA_HOME" ]; then
  echo "âŒ CUDA_HOME ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: $CUDA_HOME"
  exit 1
else
  echo "âœ… CUDA_HOME OK: $CUDA_HOME"
fi

# libcudart.so ãƒã‚§ãƒƒã‚¯ï¼ˆfindã®ã¿ä½¿ç”¨ã€ãƒ’ãƒƒãƒˆã—ãŸãƒ‘ã‚¹ã‚‚è¡¨ç¤ºï¼‰
echo -n "ğŸ” libcudart.so check: "
LIBCUDART_PATHS=$(find ${LD_LIBRARY_PATH//:/ } -name "libcudart.so*" 2>/dev/null)

if [ -n "$LIBCUDART_PATHS" ]; then
  echo "âœ… found"
  echo "$LIBCUDART_PATHS" | sed 's/^/   â””â”€ /'
else
  echo "âŒ not found (LD_LIBRARY_PATHã‚’å†ç¢ºèª)"
fi

# NCCL ãƒã‚§ãƒƒã‚¯
if [ -f "/home/appli/nccl/2.24.3/lib/libnccl.so" ]; then
  echo "âœ… NCCLãƒ©ã‚¤ãƒ–ãƒ©ãƒª OK: libnccl.so found"
else
  echo "âŒ NCCLãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
fi

# ç’°å¢ƒå¤‰æ•°
echo -e "\nğŸ§¾ [ENV] PATH:"
echo $PATH | tr ':' '\n' | grep -E "cuda|nccl"

echo -e "\nğŸ§¾ [ENV] LD_LIBRARY_PATH:"
echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -E "cuda|nccl"

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§
echo -e "\nğŸ“¦ [Module List]"
module list 2>&1

# Deepspeed
python -c "import deepspeed; print(f'Deepspeed Version: {deepspeed.__version__}')"

echo -e "\nâœ… ç’°å¢ƒãƒã‚§ãƒƒã‚¯å®Œäº† (ç¶šè¡Œå¯èƒ½)\n"

################### CPU/NUMA å¯è¦–åŒ– ###################

echo -e "\nğŸ” [CPU TOPOLOGY per node]"
srun --gpus=0 --cpus-per-task=1 --kill-on-bad-exit=1 -l bash -lc '
  echo "node: $HOSTNAME"
  lscpu | egrep "CPU\\(s\\):|Socket|Core|Thread|Model name"
  echo "--- NUMA ---"
  numactl -H | sed -n "1,8p"
  echo "--- SLURM vars ---"
  echo SLURM_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK
  echo SLURM_CPUS_ON_NODE=$SLURM_CPUS_ON_NODE
  echo SLURM_JOB_CPUS_PER_NODE=$SLURM_JOB_CPUS_PER_NODE
  echo
'

################### BAR1 ãƒ—ãƒªãƒ•ãƒ©ã‚¤ãƒˆ & ç›£è¦–ãƒ­ã‚° ###################

export MIN_BAR1_FREE_MB=${MIN_BAR1_FREE_MB:-2048}    # 1GPUã‚ãŸã‚Šæœ€ä½2GBã®BAR1ç©ºãã‚’è¦æ±‚
# -q ã§BAR1ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚ãªãŸã®ç’°å¢ƒã¯ --query ã§bar1ä¸å¯ã®ãŸã‚ï¼‰

srun --nodes=3 --ntasks-per-node=1 --gpus=0 --cpus-per-task=1 -l bash -lc '
  echo "[BAR1 preflight] node=$HOSTNAME"
  MIN_FREE='"$MIN_BAR1_FREE_MB"'
  ok=1

  # 1) ã¾ãš query API ã‚’è©¦ã™ï¼ˆæ•°å­—ã ã‘ / GPUã”ã¨1è¡Œï¼‰
  if frees=$(nvidia-smi --query-gpu=bar1.memory.free --format=csv,noheader,nounits 2>/dev/null); then
    :
  else
    # 2) ãƒ‰ãƒ©ã‚¤ãƒã«ã‚ˆã£ã¦ã¯ query ãŒç„¡ã„ã®ã§ -q ã‚’ãƒ‘ãƒ¼ã‚¹
    frees=$(nvidia-smi -q | awk '"'"'
      /BAR1 Memory Usage/ {b=1; next}
      b && /FB Memory Usage/ {b=0}
      b && /Free/ {print $3}
    '"'"')
  fi

  echo "[BAR1 Free list] ${frees//$'\n'/ } (MiB)"
  while read -r f; do
    [[ "$f" =~ ^[0-9]+$ ]] || continue
    if (( f < MIN_FREE )); then
      echo "[ABORT] $HOSTNAME: BAR1 free ${f}MiB < ${MIN_FREE}MiB"
      ok=0
    fi
  done <<< "$frees"

  (( ok )) && echo "[OK] BAR1 free >= ${MIN_FREE}MiB on all GPUs" \
         || { echo "[WARN] BAR1 free < ${MIN_FREE}MiB on some GPUs (ç¶šè¡Œ)"; :; }
'

# èµ·å‹•å‰ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆå„ãƒãƒ¼ãƒ‰ã§VRAM/BAR1ã®ç¾æ³ã ã‘ç¢ºèªï¼‰
srun --nodes=3 --ntasks-per-node=1 --gpus=0 --cpus-per-task=1 -l bash -lc '
  echo "[VRAM one-shot] $(hostname)"
  nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader
  echo "[BAR1 one-shot] $(hostname)"
  nvidia-smi -q | sed -n "/BAR1 Memory Usage/,/FB Memory Usage/p" | sed -n "1,6p"
'

################### é€šä¿¡ãƒ»ä¸¦åˆ—ã®å®Ÿè¡Œæ™‚è¨­å®š ###################

export TRL_UPDATE_NAMED_PARAM_CONCURRENCY=4
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export TORCH_NCCL_BLOCKING_WAIT=1
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export OPENBLAS_NUM_THREADS=8
export NUMEXPR_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false

export NCCL_DEBUG=WARN
#export NCCL_DEBUG_SUBSYS=ALL

export NCCL_P2P_DISABLE=0          # P2Pæœ‰åŠ¹åŒ–ï¼ˆæ˜ç¤ºï¼‰
export NCCL_P2P_LEVEL=NVL          # NVLinkã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
#export NCCL_IB_GID_INDEX=3         # IBãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨­å®šï¼ˆInfinibandåˆ©ç”¨æ™‚ï¼‰

ulimit -n 65536
ulimit -v unlimited
ulimit -m unlimited

################### ãƒ¬ãƒ/ã‚³ãƒ³ãƒ•ã‚£ã‚°ã®å­˜åœ¨ç¢ºèªï¼ˆè½ä¸‹é˜²æ­¢ï¼‰ ###################

REPO_DIR=/home/Competition2025/P02/P02U017/llm2025compet/training/open-r1/src
cd "$REPO_DIR" || exit 1

################### ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆå–å¾— ###################
NODELIST=($(scontrol show hostnames "$SLURM_JOB_NODELIST"))
MAIN_IP="${NODELIST[0]}"

################### vLLMï¼ˆã‚³ãƒ­ã‚±ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰ ###################
# â€»åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§ã®vLLMã‚µãƒ¼ãƒèµ·å‹•ã¯ä¸è¦ï¼ˆå„Trainerå†…ã§vLLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ï¼‰

echo "[Node check]"
srun --ntasks=3 --nodes=3 --ntasks-per-node=1 --nodelist="$SLURM_JOB_NODELIST" --gpus=0 --kill-on-bad-exit=1 hostname

# ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒæ¸ˆã¿ãªã‚‰æœ¬ç•ªã§ã¯ãƒãƒƒãƒˆé®æ–­æ¨å¥¨
# export HF_HUB_OFFLINE=1

# torch.compile ã‚’â€œå®Œå…¨â€ã‚ªãƒ•ï¼ˆDynamo/Inductor/JIT/NVFuserï¼‰
export TORCH_COMPILE_DISABLE=1
export TORCHDYNAMO_DISABLE=1
export TORCHINDUCTOR_DISABLE=1
export TORCHINDUCTOR_MAX_WORKERS=1  
export PYTORCH_JIT=0
export NVFUSER_DISABLE=1

# å¿µã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥å…ˆã¯NVMeï¼ˆç„¡å®³ï¼‰
export TORCHINDUCTOR_CACHE_DIR="${NVME_BASE}/torchinductor-cache"
mkdir -p "$TORCHINDUCTOR_CACHE_DIR"

# ===== å„ãƒãƒ¼ãƒ‰ã¸å±•é–‹ï¼ˆPREFETCH_DIR â†’ NVMeï¼‰=====  # â˜…ã‚³ãƒ¡ãƒ³ãƒˆå¤‰æ›´
srun --ntasks-per-node=1 --ntasks=$SLURM_NNODES -l bash -lc '
  set -euo pipefail
  SRC="'"$PREFETCH_DIR"'"
  if [[ -z "$SRC" || ! -d "$SRC" ]]; then
    echo "[ABORT] invalid SRC: ${SRC:-<empty>}"; exit 41
  fi
  # â˜…â˜…â˜… ä¿®æ­£ç‚¹: DSTã‚’NVMeä¸Šã®ã‚¸ãƒ§ãƒ–å›ºæœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å¤‰æ›´ â˜…â˜…â˜…
  DST="${NVME_BASE}/slurm-job-${SLURM_JOB_ID}/Qwen3-235B-A22B"

  mkdir -p "$DST"
  rsync -aL --info=progress2 "$SRC"/ "$DST"/
  echo "[copied] $HOSTNAME -> $DST"
'

# â˜…â˜…â˜… ä¿®æ­£ç‚¹: MODEL_PATHã‚’æ–°ã—ã„DSTã«åˆã‚ã›ã‚‹ â˜…â˜…â˜…
export MODEL_PATH="${NVME_BASE}/slurm-job-${SLURM_JOB_ID}/Qwen3-235B-A22B"
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1

################### GRPO Trainerï¼ˆã‚³ãƒ­ã‚±ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼‰ ###################
srun --nodes=3 --ntasks-per-node=1 --ntasks=3 \
     --kill-on-bad-exit=1 \
     --cpus-per-task=$SLURM_CPUS_PER_TASK \
     --nodelist="$SLURM_JOB_NODELIST" \
     --cpu-bind=cores \
     --gpus=8 --exclusive --chdir="$REPO_DIR" \
     bash -lc "
       source /home/Competition2025/P02/P02U017/openr1/bin/activate
       if [[ \"\$SLURM_NODEID\" != \"0\" ]]; then export WANDB_DISABLED=true; fi
       HUB=\"${HF_HUB_CACHE:-${HF_HOME:+$HF_HOME/hub}}\"; HUB=\"${HUB:-$HOME/.cache/huggingface/hub}\"
       if ls -d \"$HUB/models--${MODEL_ID//\//--}/snapshots\"/* >/dev/null 2>&1; then
         export HF_HUB_OFFLINE=1
         echo \"[HF] offline mode ON (prefetched detected)\"
       fi

       export TORCH_COMPILE_DISABLE=1
       export TORCHDYNAMO_DISABLE=1
       export TORCHINDUCTOR_DISABLE=1
       export PYTORCH_JIT=0
       export NVFUSER_DISABLE=1

       echo \"[GRPO-Colo] on \$HOSTNAME  (node rank \$SLURM_NODEID, proc \$SLURM_PROCID)\"
       export TRL_UPDATE_NAMED_PARAM_CONCURRENCY=4
       export NCCL_ASYNC_ERROR_HANDLING=1
       accelerate launch \\
         --config_file ../recipes/accelerate_configs/zero3.yaml \\
         --num_machines 3 \\
         --num_processes 24 \\
         --main_process_ip ${MAIN_IP} \\
         --main_process_port 29500 \\
         --rdzv_backend c10d \\
         --machine_rank \$SLURM_NODEID \\
         /home/Competition2025/P02/P02U017/llm2025compet/training/open-r1/src/open_r1/grpo_235b_a22b_fp8.py \\
         --config /home/Competition2025/P02/P02U017/llm2025compet/training/configs/Qwen3-32b/grpo/config_grpo_235b-a22b-fp8.yaml \\
         --model_name_or_path "$MODEL_PATH" \
         --use_vllm true \\
         --vllm_mode colocate \\
         --report_to wandb
      "

wait
echo '[Job] all processes finished.'