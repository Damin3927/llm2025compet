#!/bin/bash
################### Slurm åŸºæœ¬è¨­å®š ###################
#SBATCH --partition=P02
#SBATCH --nodes=3                        # â˜…å…¨3ãƒãƒ¼ãƒ‰ã‚’ã™ã¹ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨
#SBATCH --gpus-per-node=8
#SBATCH --ntasks-per-node=1
#SBATCH --nodelist=osk-gpu[54,56,91]
#SBATCH --job-name=grpo-qwen235b-colo
#SBATCH --time=4:00:00
#SBATCH --mem=0
#SBATCH --output=/home/Competition2025/P02/P02U017/llm2025compet/training/logs/grpo-qwen235b_colo.out
#SBATCH --error=/home/Competition2025/P02/P02U017/llm2025compet/training/logs/grpo-qwen235b_colo.err

################### ç’°å¢ƒ ###################
export WANDB_DISABLED=true

module unload cuda || true
module unload nccl || true

module purge

module load cuda/12.6
module load nccl/2.24.3

srun bash -c '
  echo "[Before cleanup]"; env | grep -E "PYTHON|LD_|CUDA"
  
  # æ˜ç¤ºçš„ã«ä¸è¦ãªå¤‰æ•°ã‚’unsetã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹
  unset PYTHONPATH
  unset LD_PRELOAD
  export LD_LIBRARY_PATH=/home/appli/cuda/12.6/lib64:/home/appli/nccl/2.24.3/lib

  # ä»®æƒ³ç’°å¢ƒã® activate
  source ~/openr1/bin/activate
  echo "[After cleanup and activate]"; env | grep -E "PYTHON|LD_|CUDA"

  # å®Ÿè¡Œ
  python -c "import deepspeed; print(deepspeed.__version__)"
'

# Take a look at the contents of the following environment variables first.
# PATH lists the locations of the executables and LD_LIBRARY_PATH lists where to look for shared libraries.
# Earlier entries are prioritized over later ones, and : is used to separate multiple entries.
# To find a specific CUDA toolkit, insert the correct path to list first.
# In addition, you should also check that the assigned directories actually exist.
# (https://huggingface.co/docs/transformers/debugging#deepspeed-cuda-issues)

export CUDA_HOME=/home/appli/cuda/12.6
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64:$CUDA_HOME/targets/x86_64-linux/lib/stubs:$CUDA_HOME/targets/x86_64-linux/lib:/home/appli/nccl/2.24.3/lib:$LD_LIBRARY_PATH
export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATH

source /home/Competition2025/P02/P02U017/openr1/bin/activate

################## ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ ###################

echo -e "\nğŸ” [DEBUG] CUDA/NCCL ç’°å¢ƒç¢ºèª"

# nvcc ã®ç¢ºèª
echo -n "CUDA nvcc version: "
if ! which nvcc >/dev/null 2>&1; then
  echo "âŒ nvcc ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (PATHã« $CUDA_HOME/bin ã‚’å«ã‚ãŸã‹ç¢ºèª)"
else
  nvcc --version | grep release
fi
echo -e "\nğŸ” nvcc å€™è£œä¸€è¦§:"; which -a nvcc

# Python ç’°å¢ƒç¢ºèª
echo -n "ğŸ§ª Python: "; which python
python -c "import sys; print(f'Venv Prefix: {sys.prefix}')"

# PyTorch
python -c "import torch; print(f'Torch Version: {torch.__version__} | CUDA Available: {torch.cuda.is_available()}')"

# CUDA_HOME ãƒã‚§ãƒƒã‚¯
if [ ! -d "$CUDA_HOME" ]; then
  echo "âŒ CUDA_HOME ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: $CUDA_HOME"
  exit 1
else
  echo "âœ… CUDA_HOME OK: $CUDA_HOME"
fi

# libcudart.so ãƒã‚§ãƒƒã‚¯ï¼ˆfindã®ã¿ä½¿ç”¨ã€ãƒ’ãƒƒãƒˆã—ãŸãƒ‘ã‚¹ã‚‚è¡¨ç¤ºï¼‰
echo -n "ğŸ” libcudart.so check: "
LIBCUDART_PATHS=$(find ${LD_LIBRARY_PATH//:/ } -name "libcudart.so*" 2>/dev/null)

if [ -n "$LIBCUDART_PATHS" ]; then
  echo "âœ… found"
  echo "$LIBCUDART_PATHS" | sed 's/^/   â””â”€ /'
else
  echo "âŒ not found (LD_LIBRARY_PATHã‚’å†ç¢ºèª)"
fi

# NCCL ãƒã‚§ãƒƒã‚¯
if [ -f "/home/appli/nccl/2.24.3/lib/libnccl.so" ]; then
  echo "âœ… NCCLãƒ©ã‚¤ãƒ–ãƒ©ãƒª OK: libnccl.so found"
else
  echo "âŒ NCCLãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
fi

# ç’°å¢ƒå¤‰æ•°
echo -e "\nğŸ§¾ [ENV] PATH:"
echo $PATH | tr ':' '\n' | grep -E "cuda|nccl"

echo -e "\nğŸ§¾ [ENV] LD_LIBRARY_PATH:"
echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -E "cuda|nccl"

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§
echo -e "\nğŸ“¦ [Module List]"
module list 2>&1

# Deepspeed
python -c "import deepspeed; print(f'Deepspeed Version: {deepspeed.__version__}')"

echo -e "\nâœ… ç’°å¢ƒãƒã‚§ãƒƒã‚¯å®Œäº† (ç¶šè¡Œå¯èƒ½)\n"

export TRL_UPDATE_NAMED_PARAM_CONCURRENCY=4
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1

export NCCL_DEBUG=WARN
export NCCL_DEBUG_SUBSYS=ALL

export NCCL_P2P_DISABLE=0          # P2Pæœ‰åŠ¹åŒ–ï¼ˆæ˜ç¤ºï¼‰
export NCCL_P2P_LEVEL=NVL          # NVLinkã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
#export NCCL_IB_GID_INDEX=3         # IBãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨­å®šï¼ˆInfinibandåˆ©ç”¨æ™‚ï¼‰
#export NCCL_SOCKET_IFNAME=eth0     # é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

ulimit -v unlimited
ulimit -m unlimited

REPO_DIR=/home/Competition2025/P02/P02U017/llm2025compet/training/open-r1/src
cd "$REPO_DIR" || exit 1

################### ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆå–å¾— ###################
NODELIST=($(scontrol show hostnames "$SLURM_JOB_NODELIST"))
MAIN_IP="${NODELIST[0]}"

################### vLLMï¼ˆã‚³ãƒ­ã‚±ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰ ###################
# â€»åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§ã®vLLMã‚µãƒ¼ãƒèµ·å‹•ã¯ä¸è¦ï¼ˆå„Trainerå†…ã§vLLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ï¼‰

################### GRPO Trainerï¼ˆã‚³ãƒ­ã‚±ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼‰ ###################
srun --ntasks=3 --nodelist="${NODELIST[*]}" \
     --gres=gpu:8 --exclusive --chdir="$REPO_DIR" \
     bash -c "
       source /home/Competition2025/P02/P02U017/openr1/bin/activate
       echo \"[GRPO-Colo] on \$HOSTNAME  (node rank \$SLURM_NODEID, proc \$SLURM_PROCID)\"
       export TRL_UPDATE_NAMED_PARAM_CONCURRENCY=4
       export NCCL_ASYNC_ERROR_HANDLING=1
       accelerate launch \\
         --config_file ../recipes/accelerate_configs/zero3.yaml \\
         --num_machines 3 \\
         --num_processes 24 \\
         --main_process_ip ${MAIN_IP} \\
         --main_process_port 29500 \\
         --rdzv_backend c10d \\
         --machine_rank \$SLURM_NODEID \\
         /home/Competition2025/P02/P02U017/llm2025compet/training/open-r1/src/open_r1/grpo.py \\
         --config /home/Competition2025/P02/P02U017/llm2025compet/training/configs/Qwen3-32b/grpo/config_grpo_235b.yaml \\
         --use_vllm true \\
         --vllm_mode colocate \\
         --report_to none
      "

wait
echo '[Job] all processes finished.'
