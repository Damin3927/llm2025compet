# ==========================================================
#  Qwen/Qwen2.5-1.5B‑Instruct ── GRPO + vLLM‑colocate + ZeRO‑3 (accelerate)
# ==========================================================

# ----------------------------------------------------------
# Model / Tokenizer
# ----------------------------------------------------------
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2
trust_remote_code: true          # tokenizer.chat_template を正しく取得

# ----------------------------------------------------------
# Data
# ----------------------------------------------------------
dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem

system_prompt: |
  You are a helpful AI Assistant that provides well‑reasoned and detailed responses.
  First think in <think>…</think>, then answer in <answer>…</answer>.

# Qwen 既定の chat_template を使用（上書き不要）
# chat_template: null
#eos_token: <|im_end|>

# ----------------------------------------------------------
# GRPO Trainer
# ----------------------------------------------------------
# GPU 80 GB × 8 / ZeRO‑3:
# Parameters that control generation
per_device_train_batch_size: 8      # モデルが小さいのでバッチを拡大
gradient_accumulation_steps: 8     # global_batch ≈ 8×8×8*2 = 1024
per_device_eval_batch_size: 16
# Parameters that control the data preprocessing
num_generations: 8
max_prompt_length: 512
max_completion_length: 512          # vLLM サーバ側 --max-model-len=4096 内

bf16: true
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Parameters that control the model and reference model
disable_dropout: true

# Parameters that control generation acceleration powered by vLLM
#use_vllm: true
#vllm_mode: colocate                   # train_grpo.py 側引数が override
#vllm_server_base_url: "http://osk-gpu91:8000"

# 学習ハイパーパラメータ
learning_rate: 2.0e-5              # パラメータ数が少ないのでやや高めに設定
warmup_ratio: 0.05
lr_scheduler_type: cosine
num_train_epochs: 1                # 例：1 epoch（ステップ指定なら max_steps へ）
max_steps: -1

# Parameters that control the training
loss_type: bnpo
# beta: 0.1                         # KL divergence coefficient (デフォルト 0.0)
epsilon: 0.2 
epsilon_high: 0.28
mask_truncated_completions: true
top_entropy_quantile: 0.2

reward_funcs: [accuracy, format, tag_count]
reward_weights: [1.0, 1.0, 1.0]

# ----------------------------------------------------------
# ロギング & 保存
# ----------------------------------------------------------
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps

save_strategy: epoch
save_total_limit: 1
output_dir: data/Qwen2.5-1.5B-Open-R1-GRPO
overwrite_output_dir: true

push_to_hub: false
# report_to:
# - wandb
hub_model_id: Qwen2.5-1.5B-Open-R1-GRPO
hub_strategy: every_save

seed: 42