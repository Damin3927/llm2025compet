++ date
+ echo '===== ジョブ開始: Wed Aug  6 06:45:58 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 329566'
+ echo 'NODES= osk-gpu[54,56,91]'
+ BASE_LOG_DIR=/home/Competition2025/P02/P02U006/ColossalAI/logs
+ LOG_ROOT=/home/Competition2025/P02/P02U006/ColossalAI/logs/329566
+ export LOG_ROOT
+ mkdir -p /home/Competition2025/P02/P02U006/ColossalAI/logs/329566 /home/Competition2025/P02/P02U006/ColossalAI/logs/329566/tb
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseeksft310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseeksft310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseeksft310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseeksft310
+ ask_conda='. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/~cuda-nvcc_deactivate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/libglib_deactivate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gxx_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gcc_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-binutils_linux-64.sh"
unset _CE_M
unset _CE_CONDA
PS1='\''(deepseeksft310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseeksft310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\''
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-binutils_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gcc_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gxx_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/libglib_activate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/~cuda-nvcc_activate.sh"'
+ eval '. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/~cuda-nvcc_deactivate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/libglib_deactivate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gxx_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gcc_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-binutils_linux-64.sh"
unset _CE_M
unset _CE_CONDA
PS1='\''(deepseeksft310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseeksft310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\''
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-binutils_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gcc_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gxx_linux-64.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/libglib_activate.sh"
. "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/~cuda-nvcc_activate.sh"'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/~cuda-nvcc_deactivate.sh
+++ [[ ! -z '' ]]
+++ [[ ! -z '' ]]
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/libglib_deactivate.sh
+++ export GSETTINGS_SCHEMA_DIR=
+++ GSETTINGS_SCHEMA_DIR=
+++ unset GSETTINGS_SCHEMA_DIR_CONDA_BACKUP
+++ '[' -z ']'
+++ unset GSETTINGS_SCHEMA_DIR
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gxx_linux-64.sh
+++ '[' 0 = 1 ']'
+++ CXXFLAGS_USED='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CXXFLAGS_USED='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ '[' 0 = 1 ']'
+++ _tc_activation deactivate x86_64-conda-linux-gnu- c++ g++ 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ local act_nature=deactivate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' deactivate = activate ']'
+++ from=CONDA_BACKUP_
+++ to=
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXXFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CXXFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXX_FOR_BUILD
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo c++
++++ tr a-z+- A-ZX_
+++ thing=CXX
+++ eval 'oldval=$CONDA_BACKUP_CXX'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CXX
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ ']'
+++ eval export ''\''CONDA_BACKUP_CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'\'''
++++ export CONDA_BACKUP_CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ CONDA_BACKUP_CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo g++
++++ tr a-z+- A-ZX_
+++ thing=GXX
+++ eval 'oldval=$CONDA_BACKUP_GXX'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GXX
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ ']'
+++ eval export ''\''CONDA_BACKUP_GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++'\'''
++++ export CONDA_BACKUP_GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
++++ CONDA_BACKUP_GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXXFLAGS
+++ '[' apply = apply ']'
++++ echo CXXFLAGS
++++ tr a-z+- A-ZX_
+++ thing=CXXFLAGS
+++ eval 'oldval=$CONDA_BACKUP_CXXFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CXXFLAGS
+++ '[' -n '-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' ']'
+++ eval export ''\''CONDA_BACKUP_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'\'''
++++ export 'CONDA_BACKUP_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ CONDA_BACKUP_CXXFLAGS='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CXXFLAGS
+++ '[' apply = apply ']'
++++ echo DEBUG_CXXFLAGS
++++ tr a-z+- A-ZX_
+++ thing=DEBUG_CXXFLAGS
+++ eval 'oldval=$CONDA_BACKUP_DEBUG_CXXFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset DEBUG_CXXFLAGS
+++ '[' -n '-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CONDA_BACKUP_DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CONDA_BACKUP_DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CONDA_BACKUP_DEBUG_CXXFLAGS='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXX_FOR_BUILD
+++ '[' apply = apply ']'
++++ echo CXX_FOR_BUILD
++++ tr a-z+- A-ZX_
+++ thing=CXX_FOR_BUILD
+++ eval 'oldval=$CONDA_BACKUP_CXX_FOR_BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CXX_FOR_BUILD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ ']'
+++ eval export ''\''CONDA_BACKUP_CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'\'''
++++ export CONDA_BACKUP_CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ CONDA_BACKUP_CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ return 0
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-gcc_linux-64.sh
+++ '[' 0 = 1 ']'
+++ CFLAGS_USED='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CFLAGS_USED='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ CPPFLAGS_USED='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CPPFLAGS_USED='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ LDFLAGS_USED='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
+++ CMAKE_PREFIX_PATH_USED=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
+++ '[' 0 = 1 ']'
+++ '[' '' = 1 ']'
+++ _tc_activation deactivate x86_64-conda-linux-gnu- HOST,x86_64-conda-linux-gnu BUILD,x86_64-conda-linux-gnu cc cpp gcc gcc-ar gcc-nm gcc-ranlib 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs' 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr _CONDA_PYTHON_SYSCONFIGDATA_NAME, CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot CONDA_BUILD_CROSS_COMPILATION, CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc build_alias,x86_64-conda-linux-gnu host_alias,x86_64-conda-linux-gnu CMAKE_ARGS,
+++ local act_nature=deactivate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' deactivate = activate ']'
+++ from=CONDA_BACKUP_
+++ to=
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=HOST
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=BUILD
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo cc
++++ tr a-z+- A-ZX_
+++ thing=CC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ echo cpp
++++ tr a-z+- A-ZX_
+++ thing=CPP
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ echo gcc
++++ tr a-z+- A-ZX_
+++ thing=GCC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ echo gcc-ar
++++ tr a-z+- A-ZX_
+++ thing=GCC_AR
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ echo gcc-nm
++++ tr a-z+- A-ZX_
+++ thing=GCC_NM
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ echo gcc-ranlib
++++ tr a-z+- A-ZX_
+++ thing=GCC_RANLIB
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CPPFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=LDFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CPPFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_PREFIX_PATH
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=_CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_SYSROOT
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_CROSS_COMPILATION
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CC_FOR_BUILD
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=build_alias
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=host_alias
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_ARGS,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CMAKE_ARGS,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_ARGS
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=HOST
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_HOST'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset HOST
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''CONDA_BACKUP_HOST=x86_64-conda-linux-gnu'\'''
++++ export CONDA_BACKUP_HOST=x86_64-conda-linux-gnu
++++ CONDA_BACKUP_HOST=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=BUILD
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset BUILD
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''CONDA_BACKUP_BUILD=x86_64-conda-linux-gnu'\'''
++++ export CONDA_BACKUP_BUILD=x86_64-conda-linux-gnu
++++ CONDA_BACKUP_BUILD=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo cc
++++ tr a-z+- A-ZX_
+++ thing=CC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CC'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CC
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc ']'
+++ eval export ''\''CONDA_BACKUP_CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc'\'''
++++ export CONDA_BACKUP_CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ CONDA_BACKUP_CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ echo cpp
++++ tr a-z+- A-ZX_
+++ thing=CPP
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CPP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CPP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp ']'
+++ eval export ''\''CONDA_BACKUP_CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp'\'''
++++ export CONDA_BACKUP_CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ CONDA_BACKUP_CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ echo gcc
++++ tr a-z+- A-ZX_
+++ thing=GCC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_GCC'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GCC
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc ']'
+++ eval export ''\''CONDA_BACKUP_GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc'\'''
++++ export CONDA_BACKUP_GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ CONDA_BACKUP_GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ echo gcc-ar
++++ tr a-z+- A-ZX_
+++ thing=GCC_AR
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_GCC_AR'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GCC_AR
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar ']'
+++ eval export ''\''CONDA_BACKUP_GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar'\'''
++++ export CONDA_BACKUP_GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ CONDA_BACKUP_GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ echo gcc-nm
++++ tr a-z+- A-ZX_
+++ thing=GCC_NM
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_GCC_NM'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GCC_NM
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm ']'
+++ eval export ''\''CONDA_BACKUP_GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm'\'''
++++ export CONDA_BACKUP_GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ CONDA_BACKUP_GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ echo gcc-ranlib
++++ tr a-z+- A-ZX_
+++ thing=GCC_RANLIB
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_GCC_RANLIB'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GCC_RANLIB
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib ']'
+++ eval export ''\''CONDA_BACKUP_GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib'\'''
++++ export CONDA_BACKUP_GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ CONDA_BACKUP_GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CPPFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CPPFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CPPFLAGS
+++ '[' -n '-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' ']'
+++ eval export ''\''CONDA_BACKUP_CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'\'''
++++ export 'CONDA_BACKUP_CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ CONDA_BACKUP_CPPFLAGS='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CFLAGS
+++ '[' -n '-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include' ']'
+++ eval export ''\''CONDA_BACKUP_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'\'''
++++ export 'CONDA_BACKUP_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
++++ CONDA_BACKUP_CFLAGS='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=LDFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_LDFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset LDFLAGS
+++ '[' -n '-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs' ']'
+++ eval export ''\''CONDA_BACKUP_LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'\'''
++++ export 'CONDA_BACKUP_LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
++++ CONDA_BACKUP_LDFLAGS='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CPPFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_DEBUG_CPPFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset DEBUG_CPPFLAGS
+++ '[' -n '-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CONDA_BACKUP_DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CONDA_BACKUP_DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CONDA_BACKUP_DEBUG_CPPFLAGS='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_DEBUG_CFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset DEBUG_CFLAGS
+++ '[' -n '-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CONDA_BACKUP_DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CONDA_BACKUP_DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CONDA_BACKUP_DEBUG_CFLAGS='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_PREFIX_PATH
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CMAKE_PREFIX_PATH'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CMAKE_PREFIX_PATH
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr ']'
+++ eval export ''\''CONDA_BACKUP_CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr'\'''
++++ export CONDA_BACKUP_CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ CONDA_BACKUP_CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=_CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP__CONDA_PYTHON_SYSCONFIGDATA_NAME'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset _CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' -n '' ']'
+++ eval unset '${from}${thing}'
++++ unset CONDA_BACKUP__CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_SYSROOT
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CONDA_BUILD_SYSROOT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BUILD_SYSROOT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot ']'
+++ eval export ''\''CONDA_BACKUP_CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot'\'''
++++ export CONDA_BACKUP_CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ CONDA_BACKUP_CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_CROSS_COMPILATION
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CONDA_BUILD_CROSS_COMPILATION'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BUILD_CROSS_COMPILATION
+++ '[' -n '' ']'
+++ eval unset '${from}${thing}'
++++ unset CONDA_BACKUP_CONDA_BUILD_CROSS_COMPILATION
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CC_FOR_BUILD
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CC_FOR_BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CC_FOR_BUILD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc ']'
+++ eval export ''\''CONDA_BACKUP_CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc'\'''
++++ export CONDA_BACKUP_CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ CONDA_BACKUP_CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=build_alias
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_build_alias'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset build_alias
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''CONDA_BACKUP_build_alias=x86_64-conda-linux-gnu'\'''
++++ export CONDA_BACKUP_build_alias=x86_64-conda-linux-gnu
++++ CONDA_BACKUP_build_alias=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=host_alias
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_host_alias'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset host_alias
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''CONDA_BACKUP_host_alias=x86_64-conda-linux-gnu'\'''
++++ export CONDA_BACKUP_host_alias=x86_64-conda-linux-gnu
++++ CONDA_BACKUP_host_alias=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_ARGS,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CMAKE_ARGS,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_ARGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BACKUP_CMAKE_ARGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CMAKE_ARGS
+++ '[' -n '' ']'
+++ eval unset '${from}${thing}'
++++ unset CONDA_BACKUP_CMAKE_ARGS
+++ return 0
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/deactivate.d/deactivate-binutils_linux-64.sh
+++ '[' 0 = 1 ']'
+++ echo x86_64-conda-linux-gnu
+++ grep powerpc
+++ GOLD_USED=ld.gold
+++ _tc_activation deactivate x86_64-conda-linux-gnu- addr2line ar as c++filt elfedit gprof ld ld.gold nm objcopy objdump ranlib readelf size strings strip
+++ local act_nature=deactivate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' deactivate = activate ']'
+++ from=CONDA_BACKUP_
+++ to=
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -a check = check ']'
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo addr2line
++++ tr a-z+-. A-ZX__
+++ thing=ADDR2LINE
+++ eval 'oldval=$CONDA_BACKUP_ADDR2LINE'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset ADDR2LINE
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line ']'
+++ eval export ''\''CONDA_BACKUP_ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line'\'''
++++ export CONDA_BACKUP_ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
++++ CONDA_BACKUP_ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ar
++++ tr a-z+-. A-ZX__
+++ thing=AR
+++ eval 'oldval=$CONDA_BACKUP_AR'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset AR
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar ']'
+++ eval export ''\''CONDA_BACKUP_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar'\'''
++++ export CONDA_BACKUP_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
++++ CONDA_BACKUP_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo as
++++ tr a-z+-. A-ZX__
+++ thing=AS
+++ eval 'oldval=$CONDA_BACKUP_AS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset AS
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as ']'
+++ eval export ''\''CONDA_BACKUP_AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as'\'''
++++ export CONDA_BACKUP_AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
++++ CONDA_BACKUP_AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo c++filt
++++ tr a-z+-. A-ZX__
+++ thing=CXXFILT
+++ eval 'oldval=$CONDA_BACKUP_CXXFILT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CXXFILT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt ']'
+++ eval export ''\''CONDA_BACKUP_CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt'\'''
++++ export CONDA_BACKUP_CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
++++ CONDA_BACKUP_CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo elfedit
++++ tr a-z+-. A-ZX__
+++ thing=ELFEDIT
+++ eval 'oldval=$CONDA_BACKUP_ELFEDIT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset ELFEDIT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit ']'
+++ eval export ''\''CONDA_BACKUP_ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit'\'''
++++ export CONDA_BACKUP_ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
++++ CONDA_BACKUP_ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo gprof
++++ tr a-z+-. A-ZX__
+++ thing=GPROF
+++ eval 'oldval=$CONDA_BACKUP_GPROF'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset GPROF
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof ']'
+++ eval export ''\''CONDA_BACKUP_GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof'\'''
++++ export CONDA_BACKUP_GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
++++ CONDA_BACKUP_GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ld
++++ tr a-z+-. A-ZX__
+++ thing=LD
+++ eval 'oldval=$CONDA_BACKUP_LD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset LD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld ']'
+++ eval export ''\''CONDA_BACKUP_LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld'\'''
++++ export CONDA_BACKUP_LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
++++ CONDA_BACKUP_LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ld.gold
++++ tr a-z+-. A-ZX__
+++ thing=LD_GOLD
+++ eval 'oldval=$CONDA_BACKUP_LD_GOLD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset LD_GOLD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold ']'
+++ eval export ''\''CONDA_BACKUP_LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold'\'''
++++ export CONDA_BACKUP_LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
++++ CONDA_BACKUP_LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo nm
++++ tr a-z+-. A-ZX__
+++ thing=NM
+++ eval 'oldval=$CONDA_BACKUP_NM'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset NM
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm ']'
+++ eval export ''\''CONDA_BACKUP_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm'\'''
++++ export CONDA_BACKUP_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
++++ CONDA_BACKUP_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo objcopy
++++ tr a-z+-. A-ZX__
+++ thing=OBJCOPY
+++ eval 'oldval=$CONDA_BACKUP_OBJCOPY'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset OBJCOPY
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy ']'
+++ eval export ''\''CONDA_BACKUP_OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy'\'''
++++ export CONDA_BACKUP_OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
++++ CONDA_BACKUP_OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo objdump
++++ tr a-z+-. A-ZX__
+++ thing=OBJDUMP
+++ eval 'oldval=$CONDA_BACKUP_OBJDUMP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset OBJDUMP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump ']'
+++ eval export ''\''CONDA_BACKUP_OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump'\'''
++++ export CONDA_BACKUP_OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
++++ CONDA_BACKUP_OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ranlib
++++ tr a-z+-. A-ZX__
+++ thing=RANLIB
+++ eval 'oldval=$CONDA_BACKUP_RANLIB'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset RANLIB
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib ']'
+++ eval export ''\''CONDA_BACKUP_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib'\'''
++++ export CONDA_BACKUP_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
++++ CONDA_BACKUP_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo readelf
++++ tr a-z+-. A-ZX__
+++ thing=READELF
+++ eval 'oldval=$CONDA_BACKUP_READELF'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset READELF
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf ']'
+++ eval export ''\''CONDA_BACKUP_READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf'\'''
++++ export CONDA_BACKUP_READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
++++ CONDA_BACKUP_READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo size
++++ tr a-z+-. A-ZX__
+++ thing=SIZE
+++ eval 'oldval=$CONDA_BACKUP_SIZE'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset SIZE
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size ']'
+++ eval export ''\''CONDA_BACKUP_SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size'\'''
++++ export CONDA_BACKUP_SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
++++ CONDA_BACKUP_SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo strings
++++ tr a-z+-. A-ZX__
+++ thing=STRINGS
+++ eval 'oldval=$CONDA_BACKUP_STRINGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset STRINGS
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings ']'
+++ eval export ''\''CONDA_BACKUP_STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings'\'''
++++ export CONDA_BACKUP_STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
++++ CONDA_BACKUP_STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo strip
++++ tr a-z+-. A-ZX__
+++ thing=STRIP
+++ eval 'oldval=$CONDA_BACKUP_STRIP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset STRIP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip ']'
+++ eval export ''\''CONDA_BACKUP_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip'\'''
++++ export CONDA_BACKUP_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
++++ CONDA_BACKUP_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ return 0
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseeksft310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseeksft310) '
++ CONDA_PROMPT_MODIFIER='(deepseeksft310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-binutils_linux-64.sh
+++ '[' 0 = 1 ']'
+++ echo x86_64-conda-linux-gnu
+++ grep powerpc
+++ echo x86_64-conda-linux-gnu
+++ grep s390x
+++ GOLD_USED=ld.gold
+++ _tc_activation activate x86_64-conda-linux-gnu- addr2line ar as c++filt elfedit gprof ld ld.gold nm objcopy objdump ranlib readelf size strings strip
+++ local act_nature=activate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' activate = activate ']'
+++ from=
+++ to=CONDA_BACKUP_
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -a check = check ']'
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo addr2line
++++ tr a-z+-. A-ZX__
+++ thing=ADDR2LINE
+++ eval 'oldval=$ADDR2LINE'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_ADDR2LINE
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line ']'
+++ eval export ''\''ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line'\'''
++++ export ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
++++ ADDR2LINE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-addr2line
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ar
++++ tr a-z+-. A-ZX__
+++ thing=AR
+++ eval 'oldval=$AR'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_AR
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar ']'
+++ eval export ''\''AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar'\'''
++++ export AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
++++ AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo as
++++ tr a-z+-. A-ZX__
+++ thing=AS
+++ eval 'oldval=$AS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_AS
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as ']'
+++ eval export ''\''AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as'\'''
++++ export AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
++++ AS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-as
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo c++filt
++++ tr a-z+-. A-ZX__
+++ thing=CXXFILT
+++ eval 'oldval=$CXXFILT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CXXFILT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt ']'
+++ eval export ''\''CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt'\'''
++++ export CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
++++ CXXFILT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++filt
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo elfedit
++++ tr a-z+-. A-ZX__
+++ thing=ELFEDIT
+++ eval 'oldval=$ELFEDIT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_ELFEDIT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit ']'
+++ eval export ''\''ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit'\'''
++++ export ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
++++ ELFEDIT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-elfedit
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo gprof
++++ tr a-z+-. A-ZX__
+++ thing=GPROF
+++ eval 'oldval=$GPROF'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GPROF
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof ']'
+++ eval export ''\''GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof'\'''
++++ export GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
++++ GPROF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gprof
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ld
++++ tr a-z+-. A-ZX__
+++ thing=LD
+++ eval 'oldval=$LD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_LD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld ']'
+++ eval export ''\''LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld'\'''
++++ export LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
++++ LD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ld.gold
++++ tr a-z+-. A-ZX__
+++ thing=LD_GOLD
+++ eval 'oldval=$LD_GOLD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_LD_GOLD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold ']'
+++ eval export ''\''LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold'\'''
++++ export LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
++++ LD_GOLD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld.gold
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo nm
++++ tr a-z+-. A-ZX__
+++ thing=NM
+++ eval 'oldval=$NM'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_NM
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm ']'
+++ eval export ''\''NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm'\'''
++++ export NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
++++ NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-nm
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo objcopy
++++ tr a-z+-. A-ZX__
+++ thing=OBJCOPY
+++ eval 'oldval=$OBJCOPY'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_OBJCOPY
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy ']'
+++ eval export ''\''OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy'\'''
++++ export OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
++++ OBJCOPY=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objcopy
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo objdump
++++ tr a-z+-. A-ZX__
+++ thing=OBJDUMP
+++ eval 'oldval=$OBJDUMP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_OBJDUMP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump ']'
+++ eval export ''\''OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump'\'''
++++ export OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
++++ OBJDUMP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-objdump
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo ranlib
++++ tr a-z+-. A-ZX__
+++ thing=RANLIB
+++ eval 'oldval=$RANLIB'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_RANLIB
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib ']'
+++ eval export ''\''RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib'\'''
++++ export RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
++++ RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo readelf
++++ tr a-z+-. A-ZX__
+++ thing=READELF
+++ eval 'oldval=$READELF'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_READELF
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf ']'
+++ eval export ''\''READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf'\'''
++++ export READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
++++ READELF=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-readelf
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo size
++++ tr a-z+-. A-ZX__
+++ thing=SIZE
+++ eval 'oldval=$SIZE'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_SIZE
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size ']'
+++ eval export ''\''SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size'\'''
++++ export SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
++++ SIZE=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-size
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo strings
++++ tr a-z+-. A-ZX__
+++ thing=STRINGS
+++ eval 'oldval=$STRINGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_STRINGS
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings ']'
+++ eval export ''\''STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings'\'''
++++ export STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
++++ STRINGS=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strings
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo strip
++++ tr a-z+-. A-ZX__
+++ thing=STRIP
+++ eval 'oldval=$STRIP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_STRIP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip ']'
+++ eval export ''\''STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip'\'''
++++ export STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
++++ STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip
+++ return 0
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gcc_linux-64.sh
+++ '[' 0 = 1 ']'
+++ CFLAGS_USED='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CFLAGS_USED='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ CPPFLAGS_USED='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CPPFLAGS_USED='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ LDFLAGS_USED='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
+++ CMAKE_PREFIX_PATH_USED=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
+++ '[' 0 = 1 ']'
+++ _CONDA_PYTHON_SYSCONFIGDATA_NAME_USED=_sysconfigdata_x86_64_conda_cos7_linux_gnu
+++ '[' -n _sysconfigdata_x86_64_conda_cos7_linux_gnu ']'
+++ '[' -n '' ']'
+++ _CMAKE_ARGS='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib'
+++ _CMAKE_ARGS='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip'
+++ _CMAKE_ARGS='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
+++ '[' 0 = 1 ']'
+++ '[' '' = 1 ']'
+++ _tc_activation activate x86_64-conda-linux-gnu- HOST,x86_64-conda-linux-gnu BUILD,x86_64-conda-linux-gnu cc cpp gcc gcc-ar gcc-nm gcc-ranlib 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib' 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr _CONDA_PYTHON_SYSCONFIGDATA_NAME,_sysconfigdata_x86_64_conda_cos7_linux_gnu CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot CONDA_BUILD_CROSS_COMPILATION, CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc build_alias,x86_64-conda-linux-gnu host_alias,x86_64-conda-linux-gnu 'CMAKE_ARGS,-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
+++ local act_nature=activate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' activate = activate ']'
+++ from=
+++ to=CONDA_BACKUP_
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=HOST
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=BUILD
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo cc
++++ tr a-z+- A-ZX_
+++ thing=CC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ echo cpp
++++ tr a-z+- A-ZX_
+++ thing=CPP
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ echo gcc
++++ tr a-z+- A-ZX_
+++ thing=GCC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ echo gcc-ar
++++ tr a-z+- A-ZX_
+++ thing=GCC_AR
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ echo gcc-nm
++++ tr a-z+- A-ZX_
+++ thing=GCC_NM
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ echo gcc-ranlib
++++ tr a-z+- A-ZX_
+++ thing=GCC_RANLIB
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CPPFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=LDFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CPPFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_PREFIX_PATH
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=_CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_SYSROOT
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_CROSS_COMPILATION
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CC_FOR_BUILD
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=build_alias
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=host_alias
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CMAKE_ARGS,-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ echo 'CMAKE_ARGS,-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_ARGS
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo HOST,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=HOST
+++ '[' apply = apply ']'
+++ eval 'oldval=$HOST'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_HOST
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''HOST=x86_64-conda-linux-gnu'\'''
++++ export HOST=x86_64-conda-linux-gnu
++++ HOST=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo BUILD,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=BUILD
+++ '[' apply = apply ']'
+++ eval 'oldval=$BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_BUILD
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''BUILD=x86_64-conda-linux-gnu'\'''
++++ export BUILD=x86_64-conda-linux-gnu
++++ BUILD=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo cc
++++ tr a-z+- A-ZX_
+++ thing=CC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CC'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CC
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc ']'
+++ eval export ''\''CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc'\'''
++++ export CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ CC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ echo cpp
++++ tr a-z+- A-ZX_
+++ thing=CPP
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$CPP'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CPP
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp ']'
+++ eval export ''\''CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp'\'''
++++ export CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
++++ CPP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cpp
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ echo gcc
++++ tr a-z+- A-ZX_
+++ thing=GCC
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$GCC'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GCC
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc ']'
+++ eval export ''\''GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc'\'''
++++ export GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
++++ GCC=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ echo gcc-ar
++++ tr a-z+- A-ZX_
+++ thing=GCC_AR
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$GCC_AR'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GCC_AR
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar ']'
+++ eval export ''\''GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar'\'''
++++ export GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
++++ GCC_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ar
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ echo gcc-nm
++++ tr a-z+- A-ZX_
+++ thing=GCC_NM
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$GCC_NM'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GCC_NM
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm ']'
+++ eval export ''\''GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm'\'''
++++ export GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
++++ GCC_NM=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-nm
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ echo gcc-ranlib
++++ tr a-z+- A-ZX_
+++ thing=GCC_RANLIB
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib -a apply = check ']'
+++ '[' apply = apply ']'
+++ eval 'oldval=$GCC_RANLIB'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GCC_RANLIB
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib ']'
+++ eval export ''\''GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib'\'''
++++ export GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
++++ GCC_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-gcc-ranlib
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CPPFLAGS,-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CPPFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CPPFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CPPFLAGS
+++ '[' -n '-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CPPFLAGS='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CFLAGS
+++ '[' -n '-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CFLAGS='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ echo 'LDFLAGS,-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=LDFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$LDFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_LDFLAGS
+++ '[' -n '-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib' ']'
+++ eval export ''\''LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'\'''
++++ export 'LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
++++ LDFLAGS='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CPPFLAGS,-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CPPFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$DEBUG_CPPFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_DEBUG_CPPFLAGS
+++ '[' -n '-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ DEBUG_CPPFLAGS='-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CFLAGS,-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CFLAGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$DEBUG_CFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_DEBUG_CFLAGS
+++ '[' -n '-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ DEBUG_CFLAGS='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ echo CMAKE_PREFIX_PATH,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_PREFIX_PATH
+++ '[' apply = apply ']'
+++ eval 'oldval=$CMAKE_PREFIX_PATH'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CMAKE_PREFIX_PATH
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr ']'
+++ eval export ''\''CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr'\'''
++++ export CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
++++ CMAKE_PREFIX_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot/usr
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ echo _CONDA_PYTHON_SYSCONFIGDATA_NAME,_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=_CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' apply = apply ']'
+++ eval 'oldval=$_CONDA_PYTHON_SYSCONFIGDATA_NAME'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP__CONDA_PYTHON_SYSCONFIGDATA_NAME
+++ '[' -n _sysconfigdata_x86_64_conda_cos7_linux_gnu ']'
+++ eval export ''\''_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos7_linux_gnu'\'''
++++ export _CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos7_linux_gnu
++++ _CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos7_linux_gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ echo CONDA_BUILD_SYSROOT,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_SYSROOT
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BUILD_SYSROOT'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CONDA_BUILD_SYSROOT
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot ']'
+++ eval export ''\''CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot'\'''
++++ export CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
++++ CONDA_BUILD_SYSROOT=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/x86_64-conda-linux-gnu/sysroot
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=
++++ echo CONDA_BUILD_CROSS_COMPILATION,
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CONDA_BUILD_CROSS_COMPILATION
+++ '[' apply = apply ']'
+++ eval 'oldval=$CONDA_BUILD_CROSS_COMPILATION'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CONDA_BUILD_CROSS_COMPILATION
+++ '[' -n '' ']'
+++ eval unset '${from}${thing}'
++++ unset CONDA_BUILD_CROSS_COMPILATION
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ echo CC_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CC_FOR_BUILD
+++ '[' apply = apply ']'
+++ eval 'oldval=$CC_FOR_BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CC_FOR_BUILD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc ']'
+++ eval export ''\''CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc'\'''
++++ export CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
++++ CC_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-cc
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo build_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=build_alias
+++ '[' apply = apply ']'
+++ eval 'oldval=$build_alias'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_build_alias
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''build_alias=x86_64-conda-linux-gnu'\'''
++++ export build_alias=x86_64-conda-linux-gnu
++++ build_alias=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=x86_64-conda-linux-gnu
++++ echo host_alias,x86_64-conda-linux-gnu
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=host_alias
+++ '[' apply = apply ']'
+++ eval 'oldval=$host_alias'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_host_alias
+++ '[' -n x86_64-conda-linux-gnu ']'
+++ eval export ''\''host_alias=x86_64-conda-linux-gnu'\'''
++++ export host_alias=x86_64-conda-linux-gnu
++++ host_alias=x86_64-conda-linux-gnu
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CMAKE_ARGS,-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ echo 'CMAKE_ARGS,-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CMAKE_ARGS
+++ '[' apply = apply ']'
+++ eval 'oldval=$CMAKE_ARGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CMAKE_ARGS
+++ '[' -n '-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release' ']'
+++ eval export ''\''CMAKE_ARGS=-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'\'''
++++ export 'CMAKE_ARGS=-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
++++ CMAKE_ARGS='-DCMAKE_AR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ar -DCMAKE_RANLIB=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ranlib -DCMAKE_LINKER=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-ld -DCMAKE_STRIP=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-strip -DCMAKE_BUILD_TYPE=Release'
+++ return 0
+++ unset _CMAKE_ARGS
+++ '[' '' = 1 ']'
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/activate-gxx_linux-64.sh
+++ '[' 0 = 1 ']'
+++ CXXFLAGS_USED='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ DEBUG_CXXFLAGS_USED='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ '[' 0 = 1 ']'
+++ _tc_activation activate x86_64-conda-linux-gnu- c++ g++ 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ local act_nature=activate
+++ shift
+++ local tc_prefix=x86_64-conda-linux-gnu-
+++ shift
+++ local thing
+++ local newval
+++ local from
+++ local to
+++ local pass
+++ '[' activate = activate ']'
+++ from=
+++ to=CONDA_BACKUP_
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ -a check = check ']'
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXXFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CXXFLAGS
+++ '[' check = apply ']'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXX_FOR_BUILD
+++ '[' check = apply ']'
+++ for pass in check apply
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo c++
++++ tr a-z+- A-ZX_
+++ thing=CXX
+++ eval 'oldval=$CXX'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CXX
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ ']'
+++ eval export ''\''CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'\'''
++++ export CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ CXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ for thing in "$@"
+++ case "${thing}" in
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ '[' '!' -x /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ -a apply = check ']'
+++ '[' apply = apply ']'
++++ echo g++
++++ tr a-z+- A-ZX_
+++ thing=GXX
+++ eval 'oldval=$GXX'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_GXX
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++ ']'
+++ eval export ''\''GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++'\'''
++++ export GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
++++ GXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-g++
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXXFLAGS
+++ '[' apply = apply ']'
++++ echo CXXFLAGS
++++ tr a-z+- A-ZX_
+++ thing=CXXFLAGS
+++ eval 'oldval=$CXXFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CXXFLAGS
+++ '[' -n '-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ CXXFLAGS='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ echo 'DEBUG_CXXFLAGS,-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=DEBUG_CXXFLAGS
+++ '[' apply = apply ']'
++++ echo DEBUG_CXXFLAGS
++++ tr a-z+- A-ZX_
+++ thing=DEBUG_CXXFLAGS
+++ eval 'oldval=$DEBUG_CXXFLAGS'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_DEBUG_CXXFLAGS
+++ '[' -n '-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include' ']'
+++ eval export ''\''DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'\'''
++++ export 'DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
++++ DEBUG_CXXFLAGS='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include'
+++ for thing in "$@"
+++ case "${thing}" in
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^[^\,]*\,\(.*\),\1,'
+++ newval=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ echo CXX_FOR_BUILD,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ sed 's,^\([^\,]*\)\,.*,\1,'
+++ thing=CXX_FOR_BUILD
+++ '[' apply = apply ']'
++++ echo CXX_FOR_BUILD
++++ tr a-z+- A-ZX_
+++ thing=CXX_FOR_BUILD
+++ eval 'oldval=$CXX_FOR_BUILD'
++++ oldval=
+++ '[' -n '' ']'
+++ eval unset '${to}${thing}'
++++ unset CONDA_BACKUP_CXX_FOR_BUILD
+++ '[' -n /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ ']'
+++ eval export ''\''CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'\'''
++++ export CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
++++ CXX_FOR_BUILD=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++
+++ return 0
+++ '[' 0 -ne 0 ']'
+++ '[' 0 = 1 ']'
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/libglib_activate.sh
+++ export GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
+++ GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
+++ export GSETTINGS_SCHEMA_DIR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/share/glib-2.0/schemas
+++ GSETTINGS_SCHEMA_DIR=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/share/glib-2.0/schemas
++ . /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/etc/conda/activate.d/~cuda-nvcc_activate.sh
+++ ERROR=false
+++ [[ linux-64 == \l\i\n\u\x\-\6\4 ]]
+++ targetsDir=targets/x86_64-linux
+++ [[ linux-64 == \l\i\n\u\x\-\p\p\c\6\4\l\e ]]
+++ [[ linux-64 == \l\i\n\u\x\-\a\a\r\c\h\6\4 ]]
+++ CUDA_CFLAGS=
+++ CUDA_LDFLAGS=
+++ '[' 0 = 1 ']'
+++ CUDA_CFLAGS=' -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ CUDA_LDFLAGS=' -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib'
+++ CUDA_LDFLAGS=' -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
+++ export 'CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ CFLAGS='-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ export 'CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ CPPFLAGS='-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ export 'CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ CXXFLAGS='-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include  -I/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include'
+++ export 'LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
+++ LDFLAGS='-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -Wl,-rpath-link,/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib  -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib -L/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib/stubs'
+++ '[' -z x ']'
+++ [[ ! -z x ]]
+++ export 'NVCC_PREPEND_FLAGS_BACKUP= -ccbin=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'
+++ NVCC_PREPEND_FLAGS_BACKUP=' -ccbin=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'
+++ NVCC_PREPEND_FLAGS=' -ccbin=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++ -ccbin=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/x86_64-conda-linux-gnu-c++'
+++ export NVCC_PREPEND_FLAGS
+++ [[ ! -z '' ]]
+++ '[' 0 = 1 ']'
+++ export NVCC_APPEND_FLAGS
+++ false
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ export CUDA_HOME=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
+ CUDA_HOME=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
+ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
+ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
+ export LD_LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ LD_LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ export LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ export CPATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/nvidia/cublas/include:
+ CPATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/nvidia/cublas/include:
+ export CUDACXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/nvcc
+ CUDACXX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/nvcc
+ export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ export NCCL_SOCKET_IFNAME=enp92s0np0
+ NCCL_SOCKET_IFNAME=enp92s0np0
+ export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
+ NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
+ export GLOO_SOCKET_IFNAME=enp92s0np0
+ GLOO_SOCKET_IFNAME=enp92s0np0
+ export NCCL_TIMEOUT=3600
+ NCCL_TIMEOUT=3600
+ export TORCHELASTIC_TIMEOUT=3600
+ TORCHELASTIC_TIMEOUT=3600
+ export TORCH_DISTRIBUTED_TIMEOUT=3600
+ TORCH_DISTRIBUTED_TIMEOUT=3600
+ export TORCH_ELASTIC_STORE_TIMEOUT=3600
+ TORCH_ELASTIC_STORE_TIMEOUT=3600
+ export TORCH_DISTRIBUTED_STORE_TIMEOUT=3600
+ TORCH_DISTRIBUTED_STORE_TIMEOUT=3600
+ export TORCH_NCCL_BLOCKING_WAIT=1
+ TORCH_NCCL_BLOCKING_WAIT=1
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
+ TORCH_NCCL_ASYNC_ERROR_HANDLING=1
+ unset NCCL_ASYNC_ERROR_HANDLING
+ export PYTHONFAULTHANDLER=1
+ PYTHONFAULTHANDLER=1
+ ulimit -c unlimited
+ ulimit -v unlimited
+ ulimit -m unlimited
+ trap 'echo "=== SIGUSR1 on $(hostname) ==="; pkill -USR1 -f lora_finetune.py' USR1
+ MON_LOG=/home/Competition2025/P02/P02U006/ColossalAI/logs/329566/gpu_0.log
+ MON_PID=2076022
+ export FLASH_ATTENTION_DISABLE=1
+ FLASH_ATTENTION_DISABLE=1
+ export HF_TRANSFORMERS_CACHE_DISABLE_FLASH_ATTN_2=1
+ HF_TRANSFORMERS_CACHE_DISABLE_FLASH_ATTN_2=1
+ true
+ echo FLASH_ATTENTION_DISABLE=1
+ date '+[%F %T] ===== GPU util ====='
+ echo HF_TRANSFORMERS_CACHE_DISABLE_FLASH_ATTN_2=1
+ echo '=== CUDA環境 ==='
+ echo CUDA_HOME=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
+ which nvcc
+ alias
+ eval declare -f
++ declare -f
+ /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot nvcc
+ nvcc --version
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ echo '=== Pythonライブラリのバージョン ==='
+ python -c 'import sys; print('\''python'\'', sys.version)'
+ python -c 'import torch; print('\''torch'\'', torch.__version__)'
+ sleep 60
+ python -c 'import torchvision; print('\''torchvision'\'', torchvision.__version__)'
+ python -c 'import torchaudio; print('\''torchaudio'\'', torchaudio.__version__)'
+ python -c 'import numpy; print('\''numpy'\'', numpy.__version__)'
+ python -c 'import colossalai; print('\''colossalai'\'', colossalai.__version__)'
+ python -c 'import transformers; print('\''transformers'\'', transformers.__version__)'
+ echo PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
+ echo LD_LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ echo CPATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/nvidia/cublas/include:
+ echo LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
+ echo '[after unset NCCL_NET_PLUGIN]'
+ env
+ grep NCCL
++ scontrol show hostnames 'osk-gpu[54,56,91]'
++ head -n1
+ export MASTER_ADDR=osk-gpu54
+ MASTER_ADDR=osk-gpu54
+ export MASTER_PORT=21566
+ MASTER_PORT=21566
+ echo 'MASTER_ADDR=osk-gpu54  MASTER_PORT=21566'
+ echo '== [Pre-launch NCCL env] =='
+ env
+ grep NCCL
+ srun --ntasks=24 --ntasks-per-node=8 bash -c '
  source ~/miniconda3/etc/profile.d/conda.sh
  conda activate deepseeksft310
  export NCCL_SOCKET_IFNAME='\''enp92s0np0'\''
  export MASTER_ADDR=osk-gpu54
  export MASTER_PORT=21566

  torchrun     --nnodes 3     --node_rank $SLURM_NODEID     --nproc_per_node 8     --master_addr osk-gpu54     --master_port 21566     /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py       --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16       --dataset /home/Competition2025/P02/shareP02/hci_colossalai_deepseekr10528_lorasft.jsonl       --plugin moe       --pp 3 --ep 8       --batch_size 8       --lr 2e-5       --max_length 32       --lora_rank 8 --lora_alpha 16       --num_epochs 2 --warmup_steps 8       --mixed_precision bf16       --use_grad_checkpoint       --tensorboard_dir /home/Competition2025/P02/P02U006/ColossalAI/logs/329566/tb       --save_dir /home/Competition2025/P02/P02U006/ColossalAI/logs/329566/DeepSeek-R1-0528-lora
'
W0806 18:46:21.000000 22843102799680 torch/distributed/run.py:779] 
W0806 18:46:21.000000 22843102799680 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.000000 22843102799680 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.000000 22843102799680 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.000000 23315201894208 torch/distributed/run.py:779] 
W0806 18:46:21.000000 23315201894208 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.000000 23315201894208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.000000 23315201894208 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
W0806 18:46:21.015000 23214137218880 torch/distributed/run.py:779] 
W0806 18:46:21.015000 23214137218880 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.015000 23214137218880 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.015000 23214137218880 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.015000 22837896316736 torch/distributed/run.py:779] 
W0806 18:46:21.015000 22837896316736 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.015000 22837896316736 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.015000 22837896316736 torch/distributed/run.py:779] *****************************************
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0806 18:46:21.018000 23305659672384 torch/distributed/run.py:779] 
W0806 18:46:21.018000 23305659672384 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.018000 23305659672384 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.018000 23305659672384 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
W0806 18:46:21.025000 22667976283968 torch/distributed/run.py:779] 
W0806 18:46:21.025000 22667976283968 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.025000 22667976283968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.025000 22667976283968 torch/distributed/run.py:779] *****************************************
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
Traceback (most recent call last):
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    sys.exit(main())
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
W0806 18:46:21.032000 22733488633664 torch/distributed/run.py:779] 
W0806 18:46:21.032000 22733488633664 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.032000 22733488633664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.032000 22733488633664 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    self._store = TCPStore(  # type: ignore[call-arg]
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    rdzv_info = spec.rdzv_handler.next_rendezvous()
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
W0806 18:46:21.044000 22656731666240 torch/distributed/run.py:779] 
W0806 18:46:21.044000 22656731666240 torch/distributed/run.py:779] *****************************************
W0806 18:46:21.044000 22656731666240 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:21.044000 22656731666240 torch/distributed/run.py:779] *****************************************
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
srun: error: osk-gpu54: tasks 1-4,6: Exited with exit code 1
srun: error: osk-gpu54: tasks 0,5: Exited with exit code 1
W0806 18:46:26.268000 22359730358080 torch/distributed/run.py:779] 
W0806 18:46:26.268000 22359730358080 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22359730358080 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 22359730358080 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22745131071296 torch/distributed/run.py:779] 
W0806 18:46:26.268000 22745131071296 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22745131071296 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 22745131071296 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22557044111168 torch/distributed/run.py:779] 
W0806 18:46:26.268000 22557044111168 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22557044111168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 22557044111168 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22571529602880 torch/distributed/run.py:779] 
W0806 18:46:26.268000 22571529602880 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 22571529602880 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 22571529602880 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23017767827264 torch/distributed/run.py:779] 
W0806 18:46:26.268000 23017767827264 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23017767827264 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 23017767827264 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23221000677184 torch/distributed/run.py:779] 
W0806 18:46:26.268000 23221000677184 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23221000677184 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 23221000677184 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23206968911680 torch/distributed/run.py:779] 
W0806 18:46:26.268000 23206968911680 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23206968911680 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 23206968911680 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23122152429376 torch/distributed/run.py:779] 
W0806 18:46:26.268000 23122152429376 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.268000 23122152429376 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.268000 23122152429376 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23054500591424 torch/distributed/run.py:779] 
W0806 18:46:26.900000 23054500591424 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23054500591424 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 23054500591424 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22807845852992 torch/distributed/run.py:779] 
W0806 18:46:26.900000 22807845852992 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22807845852992 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 22807845852992 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23448852891456 torch/distributed/run.py:779] 
W0806 18:46:26.900000 23448852891456 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23448852891456 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 23448852891456 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22911293667136 torch/distributed/run.py:779] 
W0806 18:46:26.900000 22911293667136 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22911293667136 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 22911293667136 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23444418307904 torch/distributed/run.py:779] 
W0806 18:46:26.900000 23444418307904 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23444418307904 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 23444418307904 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22377986651968 torch/distributed/run.py:779] 
W0806 18:46:26.900000 22377986651968 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 22377986651968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 22377986651968 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23277438396224 torch/distributed/run.py:779] 
W0806 18:46:26.900000 23277438396224 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23277438396224 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 23277438396224 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23349531662144 torch/distributed/run.py:779] 
W0806 18:46:26.900000 23349531662144 torch/distributed/run.py:779] *****************************************
W0806 18:46:26.900000 23349531662144 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0806 18:46:26.900000 23349531662144 torch/distributed/run.py:779] *****************************************
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:46:39.704484031 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:46:39.775938686 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:46:39.784260673 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:46:39.852845892 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:46:40.930957334 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:46:40.933289402 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:46:40.017799906 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:46:40.107422656 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:47:15.311434724 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:15.331213630 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:16.992877802 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:16.019268037 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:16.124325323 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:17.544218117 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:17.062277832 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:47:20.481815473 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:21.582305149 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:21.642801071 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:21.925536946 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:25.610910282 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:25.617042371 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:27.453615099 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:31.355295462 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:31.812582607 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:31.024675345 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:32.883326518 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:33.676741188 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:33.311871068 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:33.133574309 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:35.689112744 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:35.048698537 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:35.072048766 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:36.980224982 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:36.364534823 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:37.503770418 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:38.539302946 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:38.170653907 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:39.824367549 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:39.057716268 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:47:41.993854454 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:42.925807756 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[W806 18:47:43.831311742 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:43.834911580 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[W806 18:47:44.670507249 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:45.633307391 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[W806 18:47:46.866329330 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[W806 18:47:47.925063316 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:47.109240904 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:47.254810849 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[W806 18:47:48.596282616 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:48.256276960 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:49.939545683 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:50.656319043 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:47:54.025124783 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:56.979238785 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:56.272478316 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:47:57.837308517 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:58.546343169 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:47:59.581653301 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:47:59.727723224 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:47:59.196625437 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:00.335167361 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:00.384115345 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:00.227291438 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:00.045042050 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:00.356417143 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:01.160892436 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:01.162322819 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:02.623357867 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:02.897647963 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:02.760680022 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:03.601609181 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:03.152478201 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:04.505127398 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:05.412999753 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:05.766345226 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:06.869314976 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:07.535302903 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:07.612331795 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:07.230412258 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:07.261918351 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:08.775380716 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:48:09.787296903 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:09.809082971 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:11.501908913 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:11.961677290 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:11.201665048 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:12.676028594 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:12.929972463 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:12.936838142 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:13.302444795 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:13.708576794 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:13.718123544 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:13.842663389 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:13.056643735 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:13.115249456 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:13.184336030 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:13.244503961 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:14.530496996 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:14.634299179 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:14.659861692 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:15.618426703 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:18.180026477 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:21.826799368 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:22.716306880 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:48:27.029199556 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:28.090138274 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:29.367310443 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:29.394410174 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:30.617294846 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:48:30.037555980 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:31.292021618 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:36.379293308 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:48:39.342822347 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:41.325826625 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:48:42.428433178 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:48:47.668063157 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:52.942429438 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:48:58.042602641 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
[W806 18:49:00.082273623 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
[W806 18:49:06.208461227 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:21.894307924 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:26.135795456 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:49:28.097743237 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:49:28.177635363 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:49:29.756657344 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:49:30.710307274 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:38.998650831 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:49:40.663255632 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:42.455487483 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:49.892094695 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W806 18:49:54.193510772 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:49:55.935159119 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:49:57.153435412 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 18:49:58.093451827 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
[W806 18:49:59.176103776 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank18]:     train(args)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank18]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank18]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank18]:     model = HybridParallelModule(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank18]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank18]:     shared_params = sharder.shard()
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 44, in shard
[rank18]:     self._materialize()
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 236, in _materialize
[rank18]:     LazyInitContext.materialize(self.model)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank18]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank18]:     apply_fn(name, p)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank18]:     p.materialize()
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank18]:     target = self._materialize_data()
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank18]:     init_val = func(
[rank18]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.44 GiB of which 120.81 MiB is free. Process 3193205 has 2.34 GiB memory in use. Process 3193209 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 62.94 GiB memory in use. Process 3193200 has 2.34 GiB memory in use. Process 3193201 has 2.34 GiB memory in use. Process 3193194 has 2.34 GiB memory in use. Process 3193199 has 2.34 GiB memory in use. Process 3193192 has 2.34 GiB memory in use. Of the allocated memory 62.34 GiB is allocated by PyTorch, and 7.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
W0806 18:52:48.623000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193178 closing signal SIGTERM
W0806 18:52:48.623000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193184 closing signal SIGTERM
W0806 18:52:48.624000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193197 closing signal SIGTERM
W0806 18:52:48.625000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193203 closing signal SIGTERM
W0806 18:52:48.625000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193207 closing signal SIGTERM
W0806 18:52:48.625000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193216 closing signal SIGTERM
W0806 18:52:48.625000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193239 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
E0806 18:52:51.649000 23277438396224 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 3193193) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:52:48
  host      : osk-gpu91
  rank      : 18 (local_rank: 2)
  exitcode  : 1 (pid: 3193193)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
srun: error: osk-gpu91: task 21: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank22]:     train(args)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank22]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank22]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank22]:     model = HybridParallelModule(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank22]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank22]:     shared_params = sharder.shard()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 44, in shard
[rank22]:     self._materialize()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 236, in _materialize
[rank22]:     LazyInitContext.materialize(self.model)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank22]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 639, in _apply_to_lazy_module
[rank22]:     apply_fn(name, buf)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank22]:     p.materialize()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank22]:     target = self._materialize_data()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank22]:     init_val = func(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 380, in factory_fn
[rank22]:     return t.to(*args, **kwargs)
[rank22]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 6 has a total capacity of 79.44 GiB of which 13.50 MiB is free. Including non-PyTorch memory, this process has 65.38 GiB memory in use. Process 3193226 has 2.34 GiB memory in use. Process 3193231 has 2.34 GiB memory in use. Process 3193222 has 2.34 GiB memory in use. Process 3193234 has 2.34 GiB memory in use. Process 3193217 has 2.34 GiB memory in use. Process 3193237 has 2.34 GiB memory in use. Of the allocated memory 64.77 GiB is allocated by PyTorch, and 18.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank19]:     train(args)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank19]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank19]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank19]:     model = HybridParallelModule(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank19]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank19]:     shared_params = sharder.shard()
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 44, in shard
[rank19]:     self._materialize()
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 236, in _materialize
[rank19]:     LazyInitContext.materialize(self.model)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank19]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 639, in _apply_to_lazy_module
[rank19]:     apply_fn(name, buf)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank19]:     p.materialize()
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank19]:     target = self._materialize_data()
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank19]:     init_val = func(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 380, in factory_fn
[rank19]:     return t.to(*args, **kwargs)
[rank19]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 79.44 GiB of which 15.50 MiB is free. Including non-PyTorch memory, this process has 65.38 GiB memory in use. Process 3193213 has 2.34 GiB memory in use. Process 3193210 has 2.34 GiB memory in use. Process 3193204 has 2.34 GiB memory in use. Process 3193220 has 2.34 GiB memory in use. Process 3193195 has 2.34 GiB memory in use. Process 3193212 has 2.34 GiB memory in use. Of the allocated memory 64.77 GiB is allocated by PyTorch, and 18.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank9]:     replace_layer = target_module.from_native_module(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank9]:     LazyInitContext.materialize(module)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank9]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank9]:     apply_fn(name, p)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank9]:     p.materialize()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank9]:     target = self._materialize_data()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank9]:     init_val = func(
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 1 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645064 has 2.34 GiB memory in use. Process 1645067 has 2.34 GiB memory in use. Process 1645057 has 2.34 GiB memory in use. Process 1645061 has 34.90 GiB memory in use. Process 1645058 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 30.48 GiB memory in use. Process 1645059 has 2.34 GiB memory in use. Process 1645077 has 2.34 GiB memory in use. Of the allocated memory 29.87 GiB is allocated by PyTorch, and 15.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank9]: During handling of the above exception, another exception occurred:

[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank9]:     train(args)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank9]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank9]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank9]:     model = HybridParallelModule(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank9]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank9]:     shared_params = sharder.shard()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank9]:     self._replace_module(include=held_layers)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   [Previous line repeated 2 more times]
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank9]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank9]:     raise RuntimeError(
[rank9]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 1 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645064 has 2.34 GiB memory in use. Process 1645067 has 2.34 GiB memory in use. Process 1645057 has 2.34 GiB memory in use. Process 1645061 has 34.90 GiB memory in use. Process 1645058 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 30.48 GiB memory in use. Process 1645059 has 2.34 GiB memory in use. Process 1645077 has 2.34 GiB memory in use. Of the allocated memory 29.87 GiB is allocated by PyTorch, and 15.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank9]:     replace_layer = target_module.from_native_module(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank9]:     LazyInitContext.materialize(module)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank9]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank9]:     apply_fn(name, p)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank9]:     p.materialize()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank9]:     target = self._materialize_data()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank9]:     init_val = func(
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 1 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645064 has 2.34 GiB memory in use. Process 1645067 has 2.34 GiB memory in use. Process 1645057 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 34.90 GiB memory in use. Process 1645058 has 2.34 GiB memory in use. Process 1645060 has 30.48 GiB memory in use. Process 1645059 has 2.34 GiB memory in use. Process 1645077 has 2.34 GiB memory in use. Of the allocated memory 34.28 GiB is allocated by PyTorch, and 20.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank9]: During handling of the above exception, another exception occurred:

[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank9]:     train(args)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank9]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank9]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank9]:     model = HybridParallelModule(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank9]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank9]:     shared_params = sharder.shard()
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank9]:     self._replace_module(include=held_layers)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank9]:     self._recursive_replace_layer(
[rank9]:   [Previous line repeated 2 more times]
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank9]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank9]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank9]:     raise RuntimeError(
[rank9]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 1 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645064 has 2.34 GiB memory in use. Process 1645067 has 2.34 GiB memory in use. Process 1645057 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 34.90 GiB memory in use. Process 1645058 has 2.34 GiB memory in use. Process 1645060 has 30.48 GiB memory in use. Process 1645059 has 2.34 GiB memory in use. Process 1645077 has 2.34 GiB memory in use. Of the allocated memory 34.28 GiB is allocated by PyTorch, and 20.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank13]:     replace_layer = target_module.from_native_module(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank13]:     LazyInitContext.materialize(module)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank13]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank13]:     apply_fn(name, p)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank13]:     p.materialize()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank13]:     target = self._materialize_data()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank13]:     init_val = func(
[rank13]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 5 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645089 has 2.34 GiB memory in use. Process 1645090 has 2.34 GiB memory in use. Process 1645084 has 2.34 GiB memory in use. Process 1645097 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 33.20 GiB memory in use. Process 1645110 has 2.34 GiB memory in use. Process 1645087 has 2.34 GiB memory in use. Process 1645094 has 32.18 GiB memory in use. Of the allocated memory 32.58 GiB is allocated by PyTorch, and 19.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank13]: During handling of the above exception, another exception occurred:

[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank13]:     train(args)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank13]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank13]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank13]:     model = HybridParallelModule(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank13]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank13]:     shared_params = sharder.shard()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank13]:     self._replace_module(include=held_layers)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   [Previous line repeated 2 more times]
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank13]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank13]:     raise RuntimeError(
[rank13]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 5 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645089 has 2.34 GiB memory in use. Process 1645090 has 2.34 GiB memory in use. Process 1645084 has 2.34 GiB memory in use. Process 1645097 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 33.20 GiB memory in use. Process 1645110 has 2.34 GiB memory in use. Process 1645087 has 2.34 GiB memory in use. Process 1645094 has 32.18 GiB memory in use. Of the allocated memory 32.58 GiB is allocated by PyTorch, and 19.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank13]:     replace_layer = target_module.from_native_module(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank13]:     LazyInitContext.materialize(module)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank13]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank13]:     apply_fn(name, p)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank13]:     p.materialize()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank13]:     target = self._materialize_data()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank13]:     init_val = func(
[rank13]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 5 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645089 has 2.34 GiB memory in use. Process 1645090 has 2.34 GiB memory in use. Process 1645084 has 2.34 GiB memory in use. Process 1645097 has 2.34 GiB memory in use. Process 1645086 has 33.20 GiB memory in use. Process 1645110 has 2.34 GiB memory in use. Process 1645087 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 32.18 GiB memory in use. Of the allocated memory 31.57 GiB is allocated by PyTorch, and 15.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank13]: During handling of the above exception, another exception occurred:

[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank13]:     train(args)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank13]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank13]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank13]:     model = HybridParallelModule(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank13]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank13]:     shared_params = sharder.shard()
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank13]:     self._replace_module(include=held_layers)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank13]:     self._recursive_replace_layer(
[rank13]:   [Previous line repeated 2 more times]
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank13]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank13]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank13]:     raise RuntimeError(
[rank13]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 5 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645089 has 2.34 GiB memory in use. Process 1645090 has 2.34 GiB memory in use. Process 1645084 has 2.34 GiB memory in use. Process 1645097 has 2.34 GiB memory in use. Process 1645086 has 33.20 GiB memory in use. Process 1645110 has 2.34 GiB memory in use. Process 1645087 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 32.18 GiB memory in use. Of the allocated memory 31.57 GiB is allocated by PyTorch, and 15.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank15]:     replace_layer = target_module.from_native_module(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank15]:     LazyInitContext.materialize(module)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank15]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank15]:     apply_fn(name, p)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank15]:     p.materialize()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank15]:     target = self._materialize_data()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank15]:     init_val = func(
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645103 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 33.24 GiB memory in use. Process 1645106 has 2.34 GiB memory in use. Process 1645108 has 2.34 GiB memory in use. Process 1645093 has 32.14 GiB memory in use. Process 1645112 has 2.34 GiB memory in use. Process 1645102 has 2.34 GiB memory in use. Process 1645099 has 2.34 GiB memory in use. Of the allocated memory 32.63 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank15]: During handling of the above exception, another exception occurred:

[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank15]:     train(args)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank15]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank15]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank15]:     model = HybridParallelModule(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank15]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank15]:     shared_params = sharder.shard()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank15]:     self._replace_module(include=held_layers)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   [Previous line repeated 2 more times]
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank15]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank15]:     raise RuntimeError(
[rank15]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645103 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 33.24 GiB memory in use. Process 1645106 has 2.34 GiB memory in use. Process 1645108 has 2.34 GiB memory in use. Process 1645093 has 32.14 GiB memory in use. Process 1645112 has 2.34 GiB memory in use. Process 1645102 has 2.34 GiB memory in use. Process 1645099 has 2.34 GiB memory in use. Of the allocated memory 32.63 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank15]:     replace_layer = target_module.from_native_module(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank15]:     LazyInitContext.materialize(module)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank15]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank15]:     apply_fn(name, p)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank15]:     p.materialize()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank15]:     target = self._materialize_data()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank15]:     init_val = func(
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645103 has 2.34 GiB memory in use. Process 1645107 has 33.24 GiB memory in use. Process 1645106 has 2.34 GiB memory in use. Process 1645108 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 32.14 GiB memory in use. Process 1645112 has 2.34 GiB memory in use. Process 1645102 has 2.34 GiB memory in use. Process 1645099 has 2.34 GiB memory in use. Of the allocated memory 31.54 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank15]: During handling of the above exception, another exception occurred:

[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank15]:     train(args)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank15]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank15]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank15]:     model = HybridParallelModule(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank15]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank15]:     shared_params = sharder.shard()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank15]:     self._replace_module(include=held_layers)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   [Previous line repeated 2 more times]
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank15]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank15]:     raise RuntimeError(
[rank15]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 18.81 MiB is free. Process 1645103 has 2.34 GiB memory in use. Process 1645107 has 33.24 GiB memory in use. Process 1645106 has 2.34 GiB memory in use. Process 1645108 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 32.14 GiB memory in use. Process 1645112 has 2.34 GiB memory in use. Process 1645102 has 2.34 GiB memory in use. Process 1645099 has 2.34 GiB memory in use. Of the allocated memory 31.54 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank20]:     train(args)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank20]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank20]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank20]:     model = HybridParallelModule(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank20]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank20]:     shared_params = sharder.shard()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 44, in shard
[rank20]:     self._materialize()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 236, in _materialize
[rank20]:     LazyInitContext.materialize(self.model)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank20]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 639, in _apply_to_lazy_module
[rank20]:     apply_fn(name, buf)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank20]:     p.materialize()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank20]:     target = self._materialize_data()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank20]:     init_val = func(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 380, in factory_fn
[rank20]:     return t.to(*args, **kwargs)
[rank20]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 15.50 MiB is free. Process 3193214 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 65.38 GiB memory in use. Process 3193202 has 2.34 GiB memory in use. Process 3193208 has 2.34 GiB memory in use. Process 3193215 has 2.34 GiB memory in use. Process 3193230 has 2.34 GiB memory in use. Process 3193223 has 2.34 GiB memory in use. Of the allocated memory 64.77 GiB is allocated by PyTorch, and 18.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0806 18:53:07.092000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193181 closing signal SIGTERM
W0806 18:53:07.093000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193185 closing signal SIGTERM
W0806 18:53:07.093000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193194 closing signal SIGTERM
W0806 18:53:07.171000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193215 closing signal SIGTERM
W0806 18:53:07.171000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193221 closing signal SIGTERM
W0806 18:53:07.171000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193231 closing signal SIGTERM
W0806 18:53:07.171000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193238 closing signal SIGTERM
E0806 18:53:08.982000 22377986651968 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 3 (pid: 3193211) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:07
  host      : osk-gpu91
  rank      : 19 (local_rank: 3)
  exitcode  : 1 (pid: 3193211)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu91: task 18: Exited with exit code 1
W0806 18:53:12.535000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193182 closing signal SIGTERM
W0806 18:53:12.536000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193189 closing signal SIGTERM
W0806 18:53:12.536000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193199 closing signal SIGTERM
W0806 18:53:12.536000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193213 closing signal SIGTERM
W0806 18:53:12.536000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193232 closing signal SIGTERM
W0806 18:53:12.537000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193234 closing signal SIGTERM
W0806 18:53:12.537000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193242 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
W0806 18:53:14.341000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645055 closing signal SIGTERM
W0806 18:53:14.342000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645064 closing signal SIGTERM
W0806 18:53:14.346000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645066 closing signal SIGTERM
W0806 18:53:14.367000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645083 closing signal SIGTERM
W0806 18:53:14.372000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645085 closing signal SIGTERM
W0806 18:53:14.372000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645104 closing signal SIGTERM
W0806 18:53:14.380000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645108 closing signal SIGTERM
E0806 18:53:14.403000 23349531662144 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 4 (pid: 3193225) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:12
  host      : osk-gpu91
  rank      : 20 (local_rank: 4)
  exitcode  : 1 (pid: 3193225)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
srun: error: osk-gpu91: task 23: Exited with exit code 1
E0806 18:53:16.113000 22571529602880 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 5 (pid: 1645094) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:14
  host      : osk-gpu56
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 1645094)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu56: task 10: Exited with exit code 1
W0806 18:53:17.809000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645049 closing signal SIGTERM
W0806 18:53:17.810000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645059 closing signal SIGTERM
W0806 18:53:17.810000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645072 closing signal SIGTERM
W0806 18:53:17.810000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645088 closing signal SIGTERM
W0806 18:53:17.905000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645092 closing signal SIGTERM
W0806 18:53:17.906000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645097 closing signal SIGTERM
W0806 18:53:17.939000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645100 closing signal SIGTERM
W0806 18:53:19.011000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645054 closing signal SIGTERM
W0806 18:53:19.011000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645065 closing signal SIGTERM
W0806 18:53:19.012000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645071 closing signal SIGTERM
W0806 18:53:19.012000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645076 closing signal SIGTERM
W0806 18:53:19.012000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645090 closing signal SIGTERM
W0806 18:53:19.013000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645105 closing signal SIGTERM
W0806 18:53:19.013000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645106 closing signal SIGTERM
W0806 18:53:19.044000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645051 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645057 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645063 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645073 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645074 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645091 closing signal SIGTERM
W0806 18:53:19.045000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645093 closing signal SIGTERM
E0806 18:53:19.481000 22557044111168 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 7 (pid: 1645107) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:17
  host      : osk-gpu56
  rank      : 15 (local_rank: 7)
  exitcode  : 1 (pid: 1645107)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu56: task 8: Exited with exit code 1
E0806 18:53:20.240000 22745131071296 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1645061) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:19
  host      : osk-gpu56
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 1645061)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
E0806 18:53:20.438000 22359730358080 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 5 (pid: 1645086) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:19
  host      : osk-gpu56
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 1645086)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu56: task 15: Exited with exit code 1
srun: error: osk-gpu56: task 9: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
W0806 18:53:22.701000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193190 closing signal SIGTERM
W0806 18:53:22.746000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193196 closing signal SIGTERM
W0806 18:53:22.816000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193200 closing signal SIGTERM
W0806 18:53:22.816000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193212 closing signal SIGTERM
W0806 18:53:22.817000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193223 closing signal SIGTERM
W0806 18:53:22.826000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193227 closing signal SIGTERM
W0806 18:53:22.892000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193241 closing signal SIGTERM
E0806 18:53:23.675000 22911293667136 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 6 (pid: 3193235) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:22
  host      : osk-gpu91
  rank      : 22 (local_rank: 6)
  exitcode  : 1 (pid: 3193235)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu91: task 22: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
W0806 18:53:39.136000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645050 closing signal SIGTERM
W0806 18:53:39.137000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645068 closing signal SIGTERM
W0806 18:53:39.137000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645075 closing signal SIGTERM
W0806 18:53:39.137000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645082 closing signal SIGTERM
W0806 18:53:39.138000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645087 closing signal SIGTERM
W0806 18:53:39.138000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645096 closing signal SIGTERM
W0806 18:53:39.138000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645099 closing signal SIGTERM
E0806 18:53:40.442000 23122152429376 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1645060) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:53:39
  host      : osk-gpu56
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 1645060)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu56: task 14: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank20]:     replace_layer = target_module.from_native_module(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank20]:     LazyInitContext.materialize(module)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank20]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank20]:     apply_fn(name, p)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank20]:     p.materialize()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank20]:     target = self._materialize_data()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank20]:     init_val = func(
[rank20]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 21.56 MiB is free. Process 3193214 has 2.34 GiB memory in use. Process 3193202 has 66.23 GiB memory in use. Process 3193208 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 8.49 GiB memory in use. Of the allocated memory 7.89 GiB is allocated by PyTorch, and 7.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank20]: During handling of the above exception, another exception occurred:

[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank20]:     train(args)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank20]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank20]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank20]:     model = HybridParallelModule(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank20]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank20]:     shared_params = sharder.shard()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank20]:     self._replace_module(include=held_layers)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   [Previous line repeated 2 more times]
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank20]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank20]:     raise RuntimeError(
[rank20]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 21.56 MiB is free. Process 3193214 has 2.34 GiB memory in use. Process 3193202 has 66.23 GiB memory in use. Process 3193208 has 2.34 GiB memory in use. Including non-PyTorch memory, this process has 8.49 GiB memory in use. Of the allocated memory 7.89 GiB is allocated by PyTorch, and 7.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
W0806 18:54:00.874000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193183 closing signal SIGTERM
W0806 18:54:00.875000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193188 closing signal SIGTERM
W0806 18:54:00.876000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193209 closing signal SIGTERM
W0806 18:54:00.876000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193220 closing signal SIGTERM
W0806 18:54:00.960000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193233 closing signal SIGTERM
W0806 18:54:00.960000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193237 closing signal SIGTERM
W0806 18:54:00.960000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193240 closing signal SIGTERM
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
+ sleep 60
E0806 18:54:02.029000 23448852891456 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 4 (pid: 3193230) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:54:00
  host      : osk-gpu91
  rank      : 20 (local_rank: 4)
  exitcode  : 1 (pid: 3193230)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu91: task 20: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank20]:     replace_layer = target_module.from_native_module(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank20]:     LazyInitContext.materialize(module)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank20]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank20]:     apply_fn(name, p)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank20]:     p.materialize()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank20]:     target = self._materialize_data()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank20]:     init_val = func(
[rank20]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 24.25 MiB is free. Process 3193214 has 2.34 GiB memory in use. Process 3193202 has 66.23 GiB memory in use. Including non-PyTorch memory, this process has 10.81 GiB memory in use. Of the allocated memory 10.21 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank20]: During handling of the above exception, another exception occurred:

[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank20]:     train(args)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank20]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank20]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank20]:     model = HybridParallelModule(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank20]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank20]:     shared_params = sharder.shard()
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank20]:     self._replace_module(include=held_layers)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank20]:     self._recursive_replace_layer(
[rank20]:   [Previous line repeated 2 more times]
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank20]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank20]:     raise RuntimeError(
[rank20]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 24.25 MiB is free. Process 3193214 has 2.34 GiB memory in use. Process 3193202 has 66.23 GiB memory in use. Including non-PyTorch memory, this process has 10.81 GiB memory in use. Of the allocated memory 10.21 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank22]:     replace_layer = target_module.from_native_module(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank22]:     LazyInitContext.materialize(module)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank22]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank22]:     apply_fn(name, p)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank22]:     p.materialize()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank22]:     target = self._materialize_data()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank22]:     init_val = func(
[rank22]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 6 has a total capacity of 79.44 GiB of which 22.25 MiB is free. Process 3193226 has 66.23 GiB memory in use. Including non-PyTorch memory, this process has 10.81 GiB memory in use. Process 3193217 has 2.34 GiB memory in use. Of the allocated memory 10.21 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank22]: During handling of the above exception, another exception occurred:

[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank22]:     train(args)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank22]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank22]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank22]:     model = HybridParallelModule(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank22]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank22]:     shared_params = sharder.shard()
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank22]:     self._replace_module(include=held_layers)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank22]:     self._recursive_replace_layer(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank22]:     self._recursive_replace_layer(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank22]:     self._recursive_replace_layer(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank22]:     self._recursive_replace_layer(
[rank22]:   [Previous line repeated 2 more times]
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank22]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank22]:     raise RuntimeError(
[rank22]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 6 has a total capacity of 79.44 GiB of which 22.25 MiB is free. Process 3193226 has 66.23 GiB memory in use. Including non-PyTorch memory, this process has 10.81 GiB memory in use. Process 3193217 has 2.34 GiB memory in use. Of the allocated memory 10.21 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank8]:     replace_layer = target_module.from_native_module(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank8]:     LazyInitContext.materialize(module)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank8]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank8]:     apply_fn(name, p)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank8]:     p.materialize()
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank8]:     target = self._materialize_data()
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank8]:     init_val = func(
[rank8]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 24.25 MiB is free. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Process 1645053 has 2.34 GiB memory in use. Process 1645056 has 67.60 GiB memory in use. Of the allocated memory 8.84 GiB is allocated by PyTorch, and 7.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank8]: During handling of the above exception, another exception occurred:

[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank8]:     train(args)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank8]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank8]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank8]:     model = HybridParallelModule(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank8]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank8]:     shared_params = sharder.shard()
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank8]:     self._replace_module(include=held_layers)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank8]:     self._recursive_replace_layer(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank8]:     self._recursive_replace_layer(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank8]:     self._recursive_replace_layer(
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank8]:     self._recursive_replace_layer(
[rank8]:   [Previous line repeated 2 more times]
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank8]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank8]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank8]:     raise RuntimeError(
[rank8]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 24.25 MiB is free. Including non-PyTorch memory, this process has 9.45 GiB memory in use. Process 1645053 has 2.34 GiB memory in use. Process 1645056 has 67.60 GiB memory in use. Of the allocated memory 8.84 GiB is allocated by PyTorch, and 7.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
W0806 18:54:38.432000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193179 closing signal SIGTERM
W0806 18:54:38.433000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193186 closing signal SIGTERM
W0806 18:54:38.433000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193205 closing signal SIGTERM
W0806 18:54:38.433000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193210 closing signal SIGTERM
W0806 18:54:38.434000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193214 closing signal SIGTERM
W0806 18:54:38.434000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193218 closing signal SIGTERM
W0806 18:54:38.434000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193228 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[rank8]:[W806 18:54:40.535297749 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0806 18:54:41.555000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645067 closing signal SIGTERM
W0806 18:54:41.556000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645069 closing signal SIGTERM
W0806 18:54:41.586000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645079 closing signal SIGTERM
W0806 18:54:41.616000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645081 closing signal SIGTERM
W0806 18:54:41.654000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645089 closing signal SIGTERM
W0806 18:54:41.667000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645098 closing signal SIGTERM
W0806 18:54:41.703000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645103 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
E0806 18:54:43.209000 23054500591424 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 6 (pid: 3193222) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:54:38
  host      : osk-gpu91
  rank      : 22 (local_rank: 6)
  exitcode  : 1 (pid: 3193222)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0806 18:54:43.393000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193191 closing signal SIGTERM
W0806 18:54:43.394000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193198 closing signal SIGTERM
W0806 18:54:43.394000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193201 closing signal SIGTERM
W0806 18:54:43.395000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193204 closing signal SIGTERM
W0806 18:54:43.395000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193219 closing signal SIGTERM
W0806 18:54:43.395000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193226 closing signal SIGTERM
W0806 18:54:43.395000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193236 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
srun: error: osk-gpu91: task 17: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
E0806 18:54:46.097000 22807845852992 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 4 (pid: 3193208) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:54:43
  host      : osk-gpu91
  rank      : 20 (local_rank: 4)
  exitcode  : 1 (pid: 3193208)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu91: task 19: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
E0806 18:54:46.685000 23221000677184 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1645052) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:54:41
  host      : osk-gpu56
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 1645052)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
srun: error: osk-gpu56: task 12: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank10]:     replace_layer = target_module.from_native_module(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank10]:     LazyInitContext.materialize(module)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank10]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank10]:     apply_fn(name, p)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank10]:     p.materialize()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank10]:     target = self._materialize_data()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank10]:     init_val = func(
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 2 has a total capacity of 79.44 GiB of which 26.94 MiB is free. Process 1645062 has 59.25 GiB memory in use. Including non-PyTorch memory, this process has 20.15 GiB memory in use. Of the allocated memory 19.55 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank10]: During handling of the above exception, another exception occurred:

[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank10]:     train(args)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank10]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank10]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank10]:     model = HybridParallelModule(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank10]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank10]:     shared_params = sharder.shard()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank10]:     self._replace_module(include=held_layers)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank10]:     self._recursive_replace_layer(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank10]:     self._recursive_replace_layer(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank10]:     self._recursive_replace_layer(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank10]:     self._recursive_replace_layer(
[rank10]:   [Previous line repeated 2 more times]
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank10]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank10]:     raise RuntimeError(
[rank10]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 2 has a total capacity of 79.44 GiB of which 26.94 MiB is free. Process 1645062 has 59.25 GiB memory in use. Including non-PyTorch memory, this process has 20.15 GiB memory in use. Of the allocated memory 19.55 GiB is allocated by PyTorch, and 5.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank10]:     train(args)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank10]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank10]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank10]:     model = HybridParallelModule(
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank10]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank10]:     shared_params = sharder.shard()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 44, in shard
[rank10]:     self._materialize()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 236, in _materialize
[rank10]:     LazyInitContext.materialize(self.model)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank10]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank10]:     apply_fn(name, p)
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank10]:     p.materialize()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank10]:     target = self._materialize_data()
[rank10]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank10]:     init_val = func(
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 2 has a total capacity of 79.44 GiB of which 6.94 MiB is free. Including non-PyTorch memory, this process has 59.27 GiB memory in use. Process 1645080 has 20.15 GiB memory in use. Of the allocated memory 58.67 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 197, in _replace_sub_module
[rank15]:     replace_layer = target_module.from_native_module(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/modeling/deepseek_v3.py", line 73, in from_native_module
[rank15]:     LazyInitContext.materialize(module)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 602, in materialize
[rank15]:     return _apply_to_lazy_module(module, apply_fn, verbose)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 627, in _apply_to_lazy_module
[rank15]:     apply_fn(name, p)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 600, in apply_fn
[rank15]:     p.materialize()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 217, in materialize
[rank15]:     target = self._materialize_data()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/lazy/lazy_init.py", line 242, in _materialize_data
[rank15]:     init_val = func(
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 8.94 MiB is free. Including non-PyTorch memory, this process has 11.81 GiB memory in use. Process 1645102 has 67.60 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank15]: During handling of the above exception, another exception occurred:

[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 513, in <module>
[rank15]:     train(args)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 299, in train
[rank15]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank15]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 457, in configure
[rank15]:     model = HybridParallelModule(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 87, in __init__
[rank15]:     module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shardformer.py", line 55, in optimize
[rank15]:     shared_params = sharder.shard()
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 43, in shard
[rank15]:     self._replace_module(include=held_layers)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 67, in _replace_module
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 115, in _recursive_replace_layer
[rank15]:     self._recursive_replace_layer(
[rank15]:   [Previous line repeated 2 more times]
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 112, in _recursive_replace_layer
[rank15]:     self._replace_sub_module(module, sub_module_replacement, include)
[rank15]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/sharder.py", line 201, in _replace_sub_module
[rank15]:     raise RuntimeError(
[rank15]: RuntimeError: Failed to replace mlp of type EpDeepseekV3MoE with EpDeepseekV3MoE with the exception: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 8.94 MiB is free. Including non-PyTorch memory, this process has 11.81 GiB memory in use. Process 1645102 has 67.60 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
W0806 18:54:59.820000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645053 closing signal SIGTERM
W0806 18:54:59.821000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645077 closing signal SIGTERM
W0806 18:54:59.821000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645095 closing signal SIGTERM
W0806 18:54:59.822000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645109 closing signal SIGTERM
W0806 18:54:59.822000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645110 closing signal SIGTERM
W0806 18:54:59.914000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645111 closing signal SIGTERM
W0806 18:54:59.915000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645112 closing signal SIGTERM
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
E0806 18:55:01.341000 23017767827264 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 1645080) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:54:59
  host      : osk-gpu56
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 1645080)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
srun: error: osk-gpu56: task 11: Exited with exit code 1
W0806 18:55:02.324000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645056 closing signal SIGTERM
W0806 18:55:02.325000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645058 closing signal SIGTERM
W0806 18:55:02.359000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645070 closing signal SIGTERM
W0806 18:55:02.366000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645078 closing signal SIGTERM
W0806 18:55:02.390000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645084 closing signal SIGTERM
W0806 18:55:02.410000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645101 closing signal SIGTERM
W0806 18:55:02.418000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1645102 closing signal SIGTERM
E0806 18:55:04.969000 23206968911680 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 1645062) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_18:55:02
  host      : osk-gpu56
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 1645062)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu56: task 13: Exited with exit code 1
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
+ true
+ date '+[%F %T] ===== GPU util ====='
+ nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader
+ sleep 60
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Step:   0%|          | 0/33 [00:00<?, ?it/s]W0806 19:33:45.759000 23444418307904 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGTERM death signal, shutting down workers
W0806 19:33:45.761000 23444418307904 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193180 closing signal SIGTERM
W0806 19:33:45.761000 23444418307904 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3193187 closing signal SIGTERM
slurmstepd: error: *** JOB 329566 ON osk-gpu54 CANCELLED AT 2025-08-06T19:33:45 ***
slurmstepd: error: *** STEP 329566.0 ON osk-gpu54 CANCELLED AT 2025-08-06T19:33:45 ***
