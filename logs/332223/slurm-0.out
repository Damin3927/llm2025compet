please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
hostname            : osk-gpu54
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_SOCKET_IFNAME           = enp25s0np0
NCCL_IB_HCA                  = mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1,mlx5_11:1
NCCL_NET_PLUGIN              = none
NCCL_DEBUG                   = INFO
NCCL_TIMEOUT                 = 3600
TORCHELASTIC_TIMEOUT         = 3600
TORCH_DISTRIBUTED_TIMEOUT    = 3600
TORCH_ELASTIC_STORE_TIMEOUT  = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR                  = osk-gpu54
MASTER_PORT                  = 24223
CUDA_VISIBLE_DEVICES         = 0,1,2,3,4,5,6,7
LD_LIBRARY_PATH              = /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
=== END ENV CHECK ===

==== ColossalAI SFT script: train() Start ====
[08/07/25 12:58:10] INFO     colossalai - colossalai - INFO:                    
                             /home/Competition2025/P02/P02U006/ColossalAI/coloss
                             alai/initialize.py:75 launch                       
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 24         
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb
Training Info:
Config file: training_config.json 
Tensorboard logs: /home/Competition2025/P02/P02U006/ColossalAI/logs/332223/tb 
Model checkpoint: /home/Competition2025/P02/P02U006/ColossalAI/logs/332223/DeepSeek-R1-0528-lora
Load dataset: /home/Competition2025/P02/shareP02/hci_colossalai_deepseekr10528_lorasft.jsonl
dataset size: 2160
dataloader batch_size: 8, total batches: 33
Max device memory after data loader: 0.00 MB
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataset size: 2160
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
[08/07/25 13:01:55] WARNING  colossalai - colossalai - WARNING:                 
                             /home/Competition2025/P02/P02U006/ColossalAI/coloss
                             alai/booster/plugin/hybrid_parallel_plugin.py:1518 
                             enable_lora                                        
                    WARNING  colossalai - colossalai - WARNING: You have enabled
                             LoRa training. Please check the hyperparameters    
                             such as lr                                         
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
Gradient checkpointing enabled successfully
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
Model params: 672.13 B
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.2521026134490967 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 2.220613718032837 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 0.6216580867767334 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 1.8403422832489014 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.10812044143676758 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 0.5244379043579102 seconds
[extension] Time taken to load fused_optim_cuda op: 0.1050870418548584 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 0.20798087120056152 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now

[extension] Time taken to load cpu_adam_x86 op: 0.20433497428894043 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.25722718238830566 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load fused_optim_cuda op: 0.3050546646118164 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 1.2359964847564697 seconds
[extension] Time taken to load fused_optim_cuda op: 0.30428123474121094 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 1.5522890090942383 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.43590331077575684 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load fused_optim_cuda op: 2.607978343963623 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
Default dtype set to torch.bfloat16
=== [Debug] Booster boost completed: rank=4 ===
=== [Debug] Booster boost completed: rank=3 ===
=== [Debug] Booster boost completed: rank=5 ===
=== [Debug] Booster boost completed: rank=7 ===
=== [Debug] Booster boost completed: rank=6 ===
=== [Debug] Booster boost completed: rank=1 ===
=== [Debug] Booster boost completed: rank=2 ===
=== [Debug] Booster boost completed: rank=0 ===
