Last login: Wed Jul 23 11:37:51 on ttys054
hongohayato@hongousatsuhitononotobukkukonpyuta ~ % ssh -i ~/.ssh/id_ed25519 P02U006@10.255.255.101
Last login: Tue Jul 22 18:18:15 2025 from 100.113.255.14
[P02U006@osk-cpu01 ~]$ ls
ColossalAI  hostfile  logs  miniconda3
[P02U006@osk-cpu01 ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282794       P02 domain_a kan.hata PD       0:00      1 (Resources)
            282792       P02 domain_a kan.hata  R       0:10      1 osk-gpu55
            282444       P02 domain_a kan.hata  R    3:48:28      1 osk-gpu54
            281945       P02 domain_a kan.hata  R   12:36:13      1 osk-gpu56
[P02U006@osk-cpu01 ~]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282794
  Waiting for administrator response...
### 2025-07-23 15:27:51  scancel 282794  (partition: P02)

✓ Done. Request for canceling job 282794 has been processed.
★ Cancel request queued for JobID: 282792
  Waiting for administrator response...
### 2025-07-23 15:27:52  scancel 282792  (partition: P02)

✓ Done. Request for canceling job 282792 has been processed.
★ Cancel request queued for JobID: 282444
  Waiting for administrator response...
### 2025-07-23 15:27:53  scancel 282444  (partition: P02)

✓ Done. Request for canceling job 282444 has been processed.
★ Cancel request queued for JobID: 281945
  Waiting for administrator response...
### 2025-07-23 15:27:55  scancel 281945  (partition: P02)

✓ Done. Request for canceling job 281945 has been processed.
[P02U006@osk-cpu01 ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282840       P02 domain_a kan.hata  R       0:04      1 osk-gpu55
            282804       P02 domain_a kan.hata  R      16:04      1 osk-gpu56
            282799       P02 domain_a kan.hata  R      16:10      1 osk-gpu54
[P02U006@osk-cpu01 ~]$ ls
ColossalAI  hostfile  logs  miniconda3
[P02U006@osk-cpu01 ~]$ cd ColossalAI/
[P02U006@osk-cpu01 ColossalAI]$ ls
applications   CONTRIBUTING.md  examples    LICENSE      pytest.ini    run_lora.sh  version.txt
CHANGE_LOG.md  docker           extensions  logs         README.md     setup.py
colossalai     docs             hostfile    MANIFEST.in  requirements  tests
[P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
[P02U006@osk-cpu01 ColossalAI]$ srun run_lora.sh
srun: error: Unable to allocate resources: No partition specified or system default partition
[P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Com++ date
+ echo '===== ジョブ開始: Wed Jul 23 03:52:59 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282845'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z '' ']'
++ export CONDA_SHLVL=0
++ CONDA_SHLVL=0
++ '[' -n '' ']'
++++ dirname /home/Competition2025/P02/P02U006/miniconda3/bin/conda
+++ dirname /home/Competition2025/P02/P02U006/miniconda3/bin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ '[' -z '' ']'
++ PS1=
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_PREFIX='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''deepseek310'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition20
25/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025
/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_PREFIX='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''deepseek310'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition202
5/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/
P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P
02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02
U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_PREFIX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310
++ CONDA_PREFIX=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export CONDA_DEFAULT_ENV=deepseek310
++ CONDA_DEFAULT_ENV=deepseek310
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ srun colossalai run --master_port 34711 --nproc_per_node 8 /home/Competition2025/P02/P02U006/Coloss
alAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competit
ion2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competition2025/P02/shareP02/ColossalAI/ap
plications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 -
-batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps
 100 --mixed_precision bf16 --use_grad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-05
28-lora
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779]
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environmen
t variable for each process to be 1 in default, to avoid your system being overloaded, please further
 tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779]
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environmen
t variable for each process to be 1 in default, to avoid your system being overloaded, please further
 tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779]
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environmen
t variable for each process to be 1 in default, to avoid your system being overloaded, please further
 tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779]
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environmen
t variable for each process to be 1 in default, to avoid your system being overloaded, please further
 tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] **********************************
*******
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779]
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779] **********************************
*******
:
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ srun colossalai run --master_port 34711 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competition2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_grad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779]
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779]
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779]
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779]
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779]
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779] *****************************************
:
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ srun colossalai run --master_port 34711 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competition2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_grad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779]
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22752533890112 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779]
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23275447940160 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779]
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 22875438814272 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779]
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.429000 23110950278208 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779]
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.431000 22436384609344 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.431000 23219520787520 torch/distributed/run.py:779]
W0723 15:53:13.431000 23219520787520 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.431000 23219520787520 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.431000 23219520787520 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.432000 23116054324288 torch/distributed/run.py:779]
W0723 15:53:13.432000 23116054324288 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.432000 23116054324288 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.432000 23116054324288 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.432000 23359601902656 torch/distributed/run.py:779]
W0723 15:53:13.432000 23359601902656 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.432000 23359601902656 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.432000 23359601902656 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = agent.run()
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    result = f(*args, **kwargs)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    self._initialize_workers(self._worker_group)
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    self._initialize_workers(self._worker_group)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = agent.run()
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    result = f(*args, **kwargs)
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    result = f(*args, **kwargs)
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = f(*args, **kwargs)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
    rdzv_info = spec.rdzv_handler.next_rendezvous()
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779]
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779]
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779]
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] *****************************************
:
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = f(*args, **kwargs)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
    rdzv_info = spec.rdzv_handler.next_rendezvous()
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779]
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.500000 22573817857088 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779]
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.500000 22411627582528 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779]
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] *****************************************
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0723 15:53:13.500000 22945103193152 torch/distributed/run.py:779] *****************************************
:
petition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282804
  Waiting for administrator response...
✓ Done. Request for canceling job 282804 has been processed.
★ Cancel request queued for JobID: 282799
  Waiting for administrator response...
### 2025-07-23 15:52:47  scancel 282799  (partition: P02)

✓ Done. Request for canceling job 282799 has been processed.
[P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
[P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 282845
[P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282845       P02  lora-r1  P02U006  R       0:02      3 osk-gpu[54-56]
[P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
++ date
+ echo '===== ジョブ開始: Wed Jul 23 03:57:54 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282858'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ srun colossalai run --master_port 34711 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competiti
on2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_g
rad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779]
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779]
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779]
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    return f(*args, **kwargs)
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0723 15:58:02.230000 23422253130816 torch/distributed/run.py:779]
W0723 15:58:02.230000 23422253130816 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.230000 23422253130816 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.230000 23422253130816 torch/distributed/run.py:779] *****************************************
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
W0723 15:58:02.236000 23146331821120 torch/distributed/run.py:779]
W0723 15:58:02.236000 23146331821120 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.236000 23146331821120 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.236000 23146331821120 torch/distributed/run.py:779] *****************************************
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0723 15:58:02.239000 22858622641216 torch/distributed/run.py:779]
W0723 15:58:02.239000 22858622641216 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.239000 22858622641216 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.239000 22858622641216 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    result = agent.run()
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
W0723 15:58:02.248000 22810086618176 torch/distributed/run.py:779]
W0723 15:58:02.248000 22810086618176 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.248000 22810086618176 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.248000 22810086618176 torch/distributed/run.py:779] *****************************************
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    result = self._invoke_run(role)
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    return launch_agent(self._config, self._entrypoint, list(args))
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    elastic_launch(
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    result = f(*args, **kwargs)
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._initialize_workers(self._worker_group)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
W0723 15:58:02.257000 22682474243136 torch/distributed/run.py:779]
W0723 15:58:02.257000 22682474243136 torch/distributed/run.py:779] *****************************************
++ date
+ echo '===== ジョブ開始: Wed Jul 23 03:57:54 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282858'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr
/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr
            282856       P02 domain_a kan.hata PD       0:00      1 (Resources)
            282854       P02 domain_a kan.hata  R       0:21      1 osk-gpu56
            282851       P02 domain_a kan.hata  R       0:23      1 osk-gpu55
            282847       P02 domain_a kan.hata  R       0:26      1 osk-gpu54
[P02U006@osk-cpu01 ColossalAI]$ cd logs
[P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-280021.err  lora-r1-280187.err  lora-r1-282845.err
lora-r1-279251.out  lora-r1-280021.out  lora-r1-280187.out  lora-r1-282845.out
++ date
+ echo '===== ジョブ開始: Wed Jul 23 03:57:54 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282858'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ srun colossalai run --master_port 34711 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competiti
on2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_g
rad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779]
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.199000 22524912243776 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779]
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.209000 22443377235008 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779]
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] *****************************************
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 15:58:02.217000 23362347766848 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
lora-r1-279270.err  lora-r1-280156.err  lora-r1-280209.err  test-279249.err
lora-r1-279270.out  lora-r1-280156.out  lora-r1-280209.out  test-279249.out
[P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-280021.err  lora-r1-280187.err  lora-r1-282845.err
lora-r1-279251.out  lora-r1-280021.out  lora-r1-280187.out  lora-r1-282845.out
lora-r1-279270.err  lora-r1-280156.err  lora-r1-280209.err  test-279249.err
lora-r1-279270.out  lora-r1-280156.out  lora-r1-280209.out  test-279249.out
[P02U006@osk-cpu01 logs]$ less lora-r1-282845.err
[P02U006@osk-cpu01 logs]$
[P02U006@osk-cpu01 logs]$ conda activate deepseek310
-bash: conda: command not found
[P02U006@osk-cpu01 logs]$ source "$HOME/miniconda3/etc/profile.d/conda.sh"
[P02U006@osk-cpu01 logs]$ conda activate deepseek310
(deepseek310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  test-279249.out
(deepseek310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ ls
applications  CHANGE_LOG.md  colossalai  CONTRIBUTING.md  docker  docs  examples  extensions  hostfile  LICENSE  logs  MANIFEST.in  pytest.ini  README.md  requirements  run_lora.sh  setup.py  tests  version.txt
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282854
  Waiting for administrator response...
### 2025-07-23 15:57:36  scancel 282854  (partition: P02)

✓ Done. Request for canceling job 282854 has been processed.
★ Cancel request queued for JobID: 282847
  Waiting for administrator response...
### 2025-07-23 15:57:37  scancel 282847  (partition: P02)

✓ Done. Request for canceling job 282847 has been processed.
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
++ date
+ echo '===== ジョブ開始: Wed Jul 23 04:21:29 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282913'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ export MASTER_PORT=14913
+ MASTER_PORT=14913
+ srun colossalai run --master_port 14913 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competiti
on2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_g
rad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 16:21:36.933000 22636784038976 torch/distributed/run.py:779]
W0723 16:21:36.933000 22636784038976 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.933000 22636784038976 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.933000 22636784038976 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.945000 22516714783808 torch/distributed/run.py:779]
W0723 16:21:36.945000 22516714783808 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.945000 22516714783808 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.945000 22516714783808 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.946000 22542732010560 torch/distributed/run.py:779]
W0723 16:21:36.946000 22542732010560 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.946000 22542732010560 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.946000 22542732010560 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.942000 22518773613632 torch/distributed/run.py:779]
W0723 16:21:36.942000 22518773613632 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.942000 22518773613632 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.942000 22518773613632 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
W0723 16:21:36.956000 23355219825728 torch/distributed/run.py:779]
W0723 16:21:36.956000 23355219825728 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.956000 23355219825728 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.956000 23355219825728 torch/distributed/run.py:779] *****************************************
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
W0723 16:21:36.961000 23107702146112 torch/distributed/run.py:779]
W0723 16:21:36.961000 23107702146112 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.961000 23107702146112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.961000 23107702146112 torch/distributed/run.py:779] *****************************************
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
W0723 16:21:36.969000 23232706798656 torch/distributed/run.py:779]
W0723 16:21:36.969000 23232706798656 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.969000 23232706798656 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.969000 23232706798656 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    result = agent.run()
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    return f(*args, **kwargs)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
W0723 16:21:36.973000 22735627695168 torch/distributed/run.py:779]
W0723 16:21:36.973000 22735627695168 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.973000 22735627695168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.973000 22735627695168 torch/distributed/run.py:779] *****************************************
    run(args)
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
W0723 16:21:36.972000 22970701259840 torch/distributed/run.py:779]
W0723 16:21:36.972000 22970701259840 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.972000 22970701259840 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.972000 22970701259840 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.951000 22586215240768 torch/distributed/run.py:779]
W0723 16:21:36.951000 22586215240768 torch/distributed/run.py:779] *****************************************
W0723 16:21:36.951000 22586215240768 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 16:21:36.951000 22586215240768 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Submitted batch job 282858
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282858       P02  lora-r1  P02U006  R       0:05      3 osk-gpu[54-56]
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ ls
applications  CHANGE_LOG.md  colossalai  CONTRIBUTING.md  docker  docs  examples  extensions  hostfile  LICENSE  logs  MANIFEST.in  pytest.ini  README.md  requirements  run_lora.sh  setup.py  tests  version.txt
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282866       P02 domain_a kan.hata  R       4:51      1 osk-gpu56
            282860       P02 domain_a kan.hata  R       4:57      1 osk-gpu54
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd logs
(deepseek310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  lora-r1-282858.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  lora-r1-282858.out  test-279249.out
(deepseek310) [P02U006@osk-cpu01 logs]$ less lora-r1-282858.err
(deepseek310) [P02U006@osk-cpu01 logs]$
(deepseek310) [P02U006@osk-cpu01 logs]$ cd .
(deepseek310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd l
-bash: cd: l: No such file or directory
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseek310) [P02U006@osk-cpu01 logs]$ less lora-r1-282858.err
(deepseek310) [P02U006@osk-cpu01 logs]$
(deepseek310) [P02U006@osk-cpu01 logs]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseek310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ nano r
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282866       P02 domain_a kan.hata  R      22:17      1 osk-gpu56
            282860       P02 domain_a kan.hata  R      22:23      1 osk-gpu54
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282866
  Waiting for administrator response...
✓ Done. Request for canceling job 282866 has been processed.
★ Cancel request queued for JobID: 282860
  Waiting for administrator response...
✓ Done. Request for canceling job 282860 has been processed.
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 282913
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282913       P02  lora-r1  P02U006  R       0:05      3 osk-gpu[54-56]
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseek310) [P02U006@osk-cpu01 logs]$ ls
++ date
+ echo '===== ジョブ開始: Wed Jul 23 04:36:27 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 282931'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr
/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/
local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/s
bin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  lora-r1-282858.err  lora-r1-282913.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  lora-r1-282858.out  lora-r1-282913.out  test-279249.out
(deepseek310) [P02U006@osk-cpu01 logs]$ less lora-r1-282913.err
(deepseek310) [P02U006@osk-cpu01 logs]$
(deepseek310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 282931
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282931       P02  lora-r1  P02U006 PD       0:00      3 (Resources)
            282919       P02 domain_a kan.hata  R      11:44      1 osk-gpu56
            282915       P02 domain_a kan.hata  R      11:50      1 osk-gpu54
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282919
  Waiting for administrator response...
### 2025-07-23 16:36:22  scancel 282919  (partition: P02)

✓ Done. Request for canceling job 282919 has been processed.
★ Cancel request queued for JobID: 282915
  Waiting for administrator response...
### 2025-07-23 16:36:23  scancel 282915  (partition: P02)

✓ Done. Request for canceling job 282915 has been processed.
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282915       P02 domain_a kan.hata CG      11:59      1 osk-gpu54
            282931       P02  lora-r1  P02U006 PD       0:00      3 (Resources)
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282931       P02  lora-r1  P02U006  R       0:10      3 osk-gpu[54-56]
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282940       P02 domain_a kan.hata  R       0:10      1 osk-gpu55
            282938       P02 domain_a kan.hata  R       0:41      1 osk-gpu56
            282933       P02 domain_a kan.hata  R       0:46      1 osk-gpu54
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseek310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  lora-r1-282858.err  lora-r1-282913.err  lora-r1-282931.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  lora-r1-282858.out  lora-r1-282913.out  lora-r1-282931.out  test-279249.out
(deepseek310) [P02U006@osk-cpu01 logs]$ # ジョブの詳細 (Resource usage / Reason / Node 等)
scontrol show job -dd 282931
slurm_load_jobs error: Invalid job id specified
(deepseek310) [P02U006@osk-cpu01 logs]$ scontrol show job --dd 282931
scontrol: unrecognized option '--dd'
Try "scontrol --help" for more information
(deepseek310) [P02U006@osk-cpu01 logs]$ scontrol show job 282931
slurm_load_jobs error: Invalid job id specified
(deepseek310) [P02U006@osk-cpu01 logs]$ sacct -j 282931 -o JobID,JobName%20,State,ExitCode,Elapsed,NodeList
JobID                     JobName      State ExitCode    Elapsed        NodeList
------------ -------------------- ---------- -------- ---------- ---------------
282931                    lora-r1     FAILED      1:0   00:00:16  osk-gpu[54-56]
282931.batch                batch     FAILED      1:0   00:00:16       osk-gpu54
282931.exte+               extern  COMPLETED      0:0   00:00:16  osk-gpu[54-56]
282931.0               colossalai     FAILED      1:0   00:00:15  osk-gpu[54-56]
(deepseek310) [P02U006@osk-cpu01 logs]$ less lora-r1-282931.err
(deepseek310) [P02U006@osk-cpu01 logs]$
(deepseek310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseek310) [P02U006@osk-cpu01 ColossalAI]$ cd ~/ColossalAI/applications/ColossalChat
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ pip install -e .
Obtaining file:///home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
  Preparing metadata (setup.py) ... done
Requirement already satisfied: transformers==4.39.3 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.39.3)
Requirement already satisfied: tqdm in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.67.1)
Collecting datasets==2.14.7 (from coati==1.0.0)
  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)
Collecting loralib (from coati==1.0.0)
  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)
Requirement already satisfied: colossalai>=0.4.7 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.4.7)
Requirement already satisfied: torch>=2.1.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (2.4.1+cu124)
Collecting langchain (from coati==1.0.0)
  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)
Requirement already satisfied: tokenizers in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.15.2)
Requirement already satisfied: fastapi in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.116.1)
Collecting sse_starlette (from coati==1.0.0)
  Downloading sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)
Collecting wandb (from coati==1.0.0)
  Downloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: sentencepiece in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.2.0)
Collecting gpustat (from coati==1.0.0)
  Downloading gpustat-1.1.1.tar.gz (98 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: packaging in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (25.0)
Collecting autoflake==2.2.1 (from coati==1.0.0)
  Downloading autoflake-2.2.1-py3-none-any.whl.metadata (7.3 kB)
Collecting black==23.9.1 (from coati==1.0.0)
  Downloading black-23.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)
Collecting tensorboard (from coati==1.0.0)
  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)
Collecting six==1.16.0 (from coati==1.0.0)
  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting ninja==1.11.1 (from coati==1.0.0)
  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)
Collecting sentencepiece (from coati==1.0.0)
  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting flash-attn (from coati==1.0.0)
  Using cached flash_attn-2.8.1.tar.gz (8.2 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [19 lines of output]
      /tmp/pip-install-1h_lij2a/flash-attn_96f2c759cc2f43399e0070994017c844/setup.py:106: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.
        warnings.warn(
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-1h_lij2a/flash-attn_96f2c759cc2f43399e0070994017c844/setup.py", line 230, in <module>
          CUDAExtension(
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1076, in CUDAExtension
          library_dirs += library_paths(cuda=True)
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1207, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2416, in _join_cuda_home
          raise OSError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.


      torch.__version__  = 2.4.1+cu124


      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ export FLASH_ATTENTION_SKIP_COMPILE=1     # または FLASH_ATTENTION_DISABLE=1
pip install -e .                          # flash‑attn スキップ → coati だけ入る
Obtaining file:///home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
  Preparing metadata (setup.py) ... done
Requirement already satisfied: transformers==4.39.3 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.39.3)
Requirement already satisfied: tqdm in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.67.1)
Collecting datasets==2.14.7 (from coati==1.0.0)
  Using cached datasets-2.14.7-py3-none-any.whl.metadata (19 kB)
Collecting loralib (from coati==1.0.0)
  Using cached loralib-0.1.2-py3-none-any.whl.metadata (15 kB)
Requirement already satisfied: colossalai>=0.4.7 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.4.7)
Requirement already satisfied: torch>=2.1.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (2.4.1+cu124)
Collecting langchain (from coati==1.0.0)
  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)
Requirement already satisfied: tokenizers in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.15.2)
Requirement already satisfied: fastapi in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.116.1)
Collecting sse_starlette (from coati==1.0.0)
  Using cached sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)
Collecting wandb (from coati==1.0.0)
  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: sentencepiece in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.2.0)
Collecting gpustat (from coati==1.0.0)
  Using cached gpustat-1.1.1.tar.gz (98 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: packaging in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (25.0)
Collecting autoflake==2.2.1 (from coati==1.0.0)
  Using cached autoflake-2.2.1-py3-none-any.whl.metadata (7.3 kB)
Collecting black==23.9.1 (from coati==1.0.0)
  Using cached black-23.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)
Collecting tensorboard (from coati==1.0.0)
  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)
Collecting six==1.16.0 (from coati==1.0.0)
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting ninja==1.11.1 (from coati==1.0.0)
  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)
Collecting sentencepiece (from coati==1.0.0)
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting flash-attn (from coati==1.0.0)
  Using cached flash_attn-2.8.1.tar.gz (8.2 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [19 lines of output]
      /tmp/pip-install-mfnyeqio/flash-attn_0b9f25142d45436d8db596693991fb25/setup.py:106: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.
        warnings.warn(
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-mfnyeqio/flash-attn_0b9f25142d45436d8db596693991fb25/setup.py", line 230, in <module>
          CUDAExtension(
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1076, in CUDAExtension
          library_dirs += library_paths(cuda=True)
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1207, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2416, in _join_cuda_home
          raise OSError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.


      torch.__version__  = 2.4.1+cu124


      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ export FLASH_ATTENTION_SKIP_COMPILE=1
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ pip uninstall -y flash-attn  || true
WARNING: Skipping flash-attn as it is not installed.
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ pip install -e .
Obtaining file:///home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
  Preparing metadata (setup.py) ... done
Requirement already satisfied: transformers==4.39.3 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.39.3)
Requirement already satisfied: tqdm in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (4.67.1)
Collecting datasets==2.14.7 (from coati==1.0.0)
  Using cached datasets-2.14.7-py3-none-any.whl.metadata (19 kB)
Collecting loralib (from coati==1.0.0)
  Using cached loralib-0.1.2-py3-none-any.whl.metadata (15 kB)
Requirement already satisfied: colossalai>=0.4.7 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.4.7)
Requirement already satisfied: torch>=2.1.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (2.4.1+cu124)
Collecting langchain (from coati==1.0.0)
  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)
Requirement already satisfied: tokenizers in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.15.2)
Requirement already satisfied: fastapi in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.116.1)
Collecting sse_starlette (from coati==1.0.0)
  Using cached sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)
Collecting wandb (from coati==1.0.0)
  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: sentencepiece in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (0.2.0)
Collecting gpustat (from coati==1.0.0)
  Using cached gpustat-1.1.1.tar.gz (98 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: packaging in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from coati==1.0.0) (25.0)
Collecting autoflake==2.2.1 (from coati==1.0.0)
  Using cached autoflake-2.2.1-py3-none-any.whl.metadata (7.3 kB)
Collecting black==23.9.1 (from coati==1.0.0)
  Using cached black-23.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (65 kB)
Collecting tensorboard (from coati==1.0.0)
  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)
Collecting six==1.16.0 (from coati==1.0.0)
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting ninja==1.11.1 (from coati==1.0.0)
  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)
Collecting sentencepiece (from coati==1.0.0)
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting flash-attn (from coati==1.0.0)
  Using cached flash_attn-2.8.1.tar.gz (8.2 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [19 lines of output]
      /tmp/pip-install-zr0lgna2/flash-attn_7519112d6b7348698ed6a6bd34b674f9/setup.py:106: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.
        warnings.warn(
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-zr0lgna2/flash-attn_7519112d6b7348698ed6a6bd34b674f9/setup.py", line 230, in <module>
          CUDAExtension(
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1076, in CUDAExtension
          library_dirs += library_paths(cuda=True)
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1207, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
        File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2416, in _join_cuda_home
          raise OSError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.


      torch.__version__  = 2.4.1+cu124


      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ export PYTHONPATH=~/ColossalAI:$PYTHONPATH
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ python - << 'EOF'
> q
> ^C
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ python - << 'EOF'
import sys
print("=== sys.path の一覧 ===")
for p in sys.path:
    print(p)
EOF
=== sys.path の一覧 ===

/home/Competition2025/P02/P02U006/ColossalAI
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python310.zip
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/lib-dynload
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ python - << 'EOF'
import coati.dataset.loader
print(">>> coati.dataset.loader の実体ファイル:")
print(coati.dataset.loader.__file__)
EOF
please install Colossal-AI from https://www.colossalai.org/download or from source
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/__init__.py", line 2, in <module>
    from .loader import (
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/loader.py", line 11, in <module>
    import jsonlines
ModuleNotFoundError: No module named 'jsonlines'
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ pip install jsonlines==4.0.0
Collecting jsonlines==4.0.0
  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)
Requirement already satisfied: attrs>=19.2.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages (from jsonlines==4.0.0) (25.3.0)
Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)
Installing collected packages: jsonlines
Successfully installed jsonlines-4.0.0
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ python - << 'EOF'
import coati.dataset.loader
print(">>> coati.dataset.loader の実体ファイル:")
print(coati.dataset.loader.__file__)
EOF
please install Colossal-AI from https://www.colossalai.org/download or from source
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/__init__.py", line 2, in <module>
    from .loader import (
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/loader.py", line 15, in <module>
    from datasets import Dataset as HFDataset
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/datasets/__init__.py", line 18, in <module>
    from .arrow_dataset import Dataset
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 59, in <module>
    import pandas as pd
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/pandas/_libs/__init__.py", line 13, in <module>
    from pandas._libs.interval import Interval
  File "pandas/_libs/interval.pyx", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ pip freeze
accelerate==1.9.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.14
aiosignal==1.4.0
annotated-types==0.7.0
anyio==4.9.0
async-timeout==5.0.1
attrs==25.3.0
bcrypt==4.3.0
beautifulsoup4==4.13.4
bitsandbytes==0.46.1
certifi==2025.7.14
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.2
click==8.2.1
colossalai==0.4.7
contexttimer==0.3.3
cryptography==45.0.5
datasets==2.18.0
decorator==5.2.1
Deprecated==1.2.18
diffusers==0.29.0
dill==0.3.8
distlib==0.4.0
einops==0.8.1
exceptiongroup==1.3.0
fabric==3.2.2
fastapi==0.116.1
filelock==3.13.1
frozenlist==1.7.0
fsspec==2024.2.0
galore-torch==1.0
google==3.0.0
h11==0.16.0
hf-xet==1.1.5
huggingface-hub==0.33.4
identify==2.6.12
idna==3.10
importlib_metadata==8.7.0
invoke==2.2.0
Jinja2==3.1.4
jsonlines==4.0.0
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
mpmath==1.3.0
msgpack==1.1.1
multidict==6.6.3
multiprocess==0.70.16
networkx==3.3
ninja==1.11.1.4
nodeenv==1.9.1
numpy==2.1.2
nvidia-cublas-cu12==12.4.2.65
nvidia-cuda-cupti-cu12==12.4.99
nvidia-cuda-nvrtc-cu12==12.4.99
nvidia-cuda-runtime-cu12==12.4.99
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.0.44
nvidia-curand-cu12==10.3.5.119
nvidia-cusolver-cu12==11.6.0.99
nvidia-cusparse-cu12==12.3.0.142
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.4.99
nvidia-nvtx-cu12==12.4.99
packaging==25.0
pandas==1.5.3
paramiko==3.5.1
peft==0.13.2
pillow==11.0.0
platformdirs==4.3.8
plumbum==1.9.0
pre_commit==4.2.0
prompt-toolkit==3.0.43
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
pyarrow==21.0.0
pyarrow-hotfix==0.7
pycparser==2.22
pydantic==2.11.7
pydantic_core==2.33.2
Pygments==2.19.2
PyNaCl==1.5.0
python-dateutil==2.9.0.post0
pytz==2025.2
PyYAML==6.0.2
ray==2.48.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rich==14.0.0
rpds-py==0.26.0
rpyc==6.0.0
safetensors==0.5.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.7
starlette==0.47.2
sympy==1.13.3
tokenizers==0.15.2
torch==2.4.1+cu124
torchaudio==2.4.1+cu124
torchvision==0.19.1+cu124
tqdm==4.67.1
transformers==4.39.3
triton==3.0.0
typing-inspection==0.4.1
typing_extensions==4.12.2
urllib3==2.5.0
uvicorn==0.29.0
virtualenv==20.32.0
wcwidth==0.2.13
wrapt==1.17.2
xxhash==3.5.0
yarl==1.20.1
zipp==3.23.0
(deepseek310) [P02U006@osk-cpu01 ColossalChat]$ conda deactivate
[P02U006@osk-cpu01 ColossalChat]$ conda env remove -n deepseek310
2 channel Terms of Service accepted

Remove all packages in environment /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310:


## Package Plan ##

  environment location: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310


The following packages will be REMOVED:

  _libgcc_mutex-0.1-main
  _openmp_mutex-5.1-1_gnu
  bzip2-1.0.8-h5eee18b_6
  ca-certificates-2025.7.14-hbd8a1cb_0
  expat-2.7.1-h6a678d5_0
  git-lfs-3.7.0-h59e48b9_0
  ld_impl_linux-64-2.40-h12ee557_0
  libffi-3.4.4-h6a678d5_1
  libgcc-ng-11.2.0-h1234567_1
  libgomp-11.2.0-h1234567_1
  libstdcxx-ng-11.2.0-h1234567_1
  libuuid-1.41.5-h5eee18b_0
  libxcb-1.17.0-h9b100fa_0
  ncurses-6.4-h6a678d5_0
  openssl-3.0.16-h5eee18b_0
  pip-25.1-pyhc872135_2
  pthread-stubs-0.3-h0ce48e5_1
  python-3.10.18-h1a3bd86_0
  readline-8.2-h5eee18b_0
  setuptools-78.1.1-py310h06a4308_0
  sqlite-3.50.2-hb25bd0a_1
  tk-8.6.14-h993c535_1
  tzdata-2025b-h04d1e81_0
  wheel-0.45.1-py310h06a4308_0
  xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto-2024.1-h5eee18b_1
  xz-5.6.4-h5eee18b_1
  zlib-1.2.13-h5eee18b_1


Proceed ([y]/n)? y


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
Everything found within the environment (/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310), including any conda environment configurations and any non-conda files, will be deleted. Do you wish to continue?
 (y/[n])? y

[P02U006@osk-cpu01 ColossalChat]$ cd ..
[P02U006@osk-cpu01 applications]$ c d..
-bash: c: command not found
[P02U006@osk-cpu01 applications]$ cd ..
[P02U006@osk-cpu01 ColossalAI]$ cd ..
[P02U006@osk-cpu01 ~]$ conda create -n deepseek python=3.11 -y
2 channel Terms of Service accepted
Retrieving notices: done
Channels:
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek

  added / updated specs:
    - python=3.11


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    openssl-3.0.17             |       h5eee18b_0         5.2 MB
    ------------------------------------------------------------
                                           Total:         5.2 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6
  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0
  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0
  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0
  openssl            pkgs/main/linux-64::openssl-3.0.17-h5eee18b_0
  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1
  python             pkgs/main/linux-64::python-3.11.13-h1a3bd86_0
  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0
  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py311h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.50.2-hb25bd0a_1
  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0
  wheel              pkgs/main/linux-64::wheel-0.45.1-py311h06a4308_0
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1
  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1



Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate deepseek
#
# To deactivate an active environment, use
#
#     $ conda deactivate

[P02U006@osk-cpu01 ~]$ conda env remove -n deepseek
2 channel Terms of Service accepted

Remove all packages in environment /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek:


## Package Plan ##

  environment location: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek


The following packages will be REMOVED:

  _libgcc_mutex-0.1-main
  _openmp_mutex-5.1-1_gnu
  bzip2-1.0.8-h5eee18b_6
  ca-certificates-2025.2.25-h06a4308_0
  expat-2.7.1-h6a678d5_0
  ld_impl_linux-64-2.40-h12ee557_0
  libffi-3.4.4-h6a678d5_1
  libgcc-ng-11.2.0-h1234567_1
  libgomp-11.2.0-h1234567_1
  libstdcxx-ng-11.2.0-h1234567_1
  libuuid-1.41.5-h5eee18b_0
  libxcb-1.17.0-h9b100fa_0
  ncurses-6.4-h6a678d5_0
  openssl-3.0.17-h5eee18b_0
  pip-25.1-pyhc872135_2
  pthread-stubs-0.3-h0ce48e5_1
  python-3.11.13-h1a3bd86_0
  readline-8.2-h5eee18b_0
  setuptools-78.1.1-py311h06a4308_0
  sqlite-3.50.2-hb25bd0a_1
  tk-8.6.14-h993c535_1
  tzdata-2025b-h04d1e81_0
  wheel-0.45.1-py311h06a4308_0
  xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto-2024.1-h5eee18b_1
  xz-5.6.4-h5eee18b_1
  zlib-1.2.13-h5eee18b_1


Proceed ([y]/n)? y


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
Everything found within the environment (/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek), including any conda environment configurations and any non-conda files, will be deleted. Do you wish to continue?
 (y/[n])? y

[P02U006@osk-cpu01 ~]$ conda create -n deepseeksft310 python=3.10 -y
2 channel Terms of Service accepted
Channels:
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310

  added / updated specs:
    - python=3.10


The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6
  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0
  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0
  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0
  openssl            pkgs/main/linux-64::openssl-3.0.17-h5eee18b_0
  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1
  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0
  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0
  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.50.2-hb25bd0a_1
  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0
  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1
  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1



Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate deepseeksft310
#
# To deactivate an active environment, use
#
#     $ conda deactivate

[P02U006@osk-cpu01 ~]$ conda activate deepseeksft310
(deepseeksft310) [P02U006@osk-cpu01 ~]$ ls
ColossalAI  hostfile  logs  miniconda3
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install torch==2.4.1 torchvision==0.17.1 torchaudio==2.4.1 \
  --index-url https://download.pytorch.org/whl/cu124
Looking in indexes: https://download.pytorch.org/whl/cu124
Collecting torch==2.4.1
  Using cached https://download.pytorch.org/whl/cu124/torch-2.4.1%2Bcu124-cp310-cp310-linux_x86_64.whl (797.2 MB)
ERROR: Could not find a version that satisfies the requirement torchvision==0.17.1 (from versions: 0.1.6, 0.2.0, 0.19.0+cu124, 0.19.1+cu124, 0.20.0+cu124, 0.20.1+cu124, 0.21.0+cu124)
ERROR: No matching distribution found for torchvision==0.17.1
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \
  --index-url https://download.pytorch.org/whl/cu124
Looking in indexes: https://download.pytorch.org/whl/cu124
Collecting torch==2.4.1
  Using cached https://download.pytorch.org/whl/cu124/torch-2.4.1%2Bcu124-cp310-cp310-linux_x86_64.whl (797.2 MB)
Collecting torchvision==0.19.1
  Using cached https://download.pytorch.org/whl/cu124/torchvision-0.19.1%2Bcu124-cp310-cp310-linux_x86_64.whl (7.1 MB)
Collecting torchaudio==2.4.1
  Using cached https://download.pytorch.org/whl/cu124/torchaudio-2.4.1%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)
Collecting filelock (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
Collecting typing-extensions>=4.8.0 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting sympy (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)
Collecting jinja2 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)
Collecting fsspec (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.4.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (24.7 MB)
Collecting nvidia-cuda-runtime-cu12==12.4.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (883 kB)
Collecting nvidia-cuda-cupti-cu12==12.4.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (13.8 MB)
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
Collecting nvidia-cublas-cu12==12.4.2.65 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.2.65-py3-none-manylinux2014_x86_64.whl (363.0 MB)
Collecting nvidia-cufft-cu12==11.2.0.44 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.0.44-py3-none-manylinux2014_x86_64.whl (211.5 MB)
Collecting nvidia-curand-cu12==10.3.5.119 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.119-py3-none-manylinux2014_x86_64.whl (56.3 MB)
Collecting nvidia-cusolver-cu12==11.6.0.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.0.99-py3-none-manylinux2014_x86_64.whl (128.4 MB)
Collecting nvidia-cusparse-cu12==12.3.0.142 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.0.142-py3-none-manylinux2014_x86_64.whl (207.5 MB)
Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)
Collecting nvidia-nvtx-cu12==12.4.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (99 kB)
Collecting nvidia-nvjitlink-cu12==12.4.99 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)
Collecting triton==3.0.0 (from torch==2.4.1)
  Using cached https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)
Collecting numpy (from torchvision==0.19.1)
  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.19.1)
  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.1)
  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.1)
  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)
Using cached https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)
Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)
Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)
Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)
Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)
Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)
Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio
Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.4.2.65 nvidia-cuda-cupti-cu12-12.4.99 nvidia-cuda-nvrtc-cu12-12.4.99 nvidia-cuda-runtime-cu12-12.4.99 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.0.44 nvidia-curand-cu12-10.3.5.119 nvidia-cusolver-cu12-11.6.0.99 nvidia-cusparse-cu12-12.3.0.142 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.4.99 pillow-11.0.0 sympy-1.13.3 torch-2.4.1+cu124 torchaudio-2.4.1+cu124 torchvision-0.19.1+cu124 triton-3.0.0 typing-extensions-4.12.2
(deepseeksft310) [P02U006@osk-cpu01 ~]$ python - << 'EOF'
import torch, torchvision, torchaudio
print("Torch      :", torch.__version__)
print("Torchvision:", torchvision.__version__)
print("Torchaudio :", torchaudio.__version__)
EOF
Torch      : 2.4.1+cu124
Torchvision: 0.19.1+cu124
Torchaudio : 2.4.1+cu124
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install colossalai==0.4.7
Collecting colossalai==0.4.7
  Using cached colossalai-0.4.7-py3-none-any.whl
Requirement already satisfied: numpy in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from colossalai==0.4.7) (2.1.2)
Collecting tqdm (from colossalai==0.4.7)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting psutil (from colossalai==0.4.7)
  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting packaging (from colossalai==0.4.7)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting pre-commit (from colossalai==0.4.7)
  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting rich (from colossalai==0.4.7)
  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)
Collecting click (from colossalai==0.4.7)
  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)
Collecting fabric (from colossalai==0.4.7)
  Using cached fabric-3.2.2-py3-none-any.whl.metadata (3.5 kB)
Collecting contexttimer (from colossalai==0.4.7)
  Using cached contexttimer-0.3.3-py3-none-any.whl
Collecting ninja (from colossalai==0.4.7)
  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)
Requirement already satisfied: torch<=2.4.1,>=2.2.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from colossalai==0.4.7) (2.4.1+cu124)
Collecting safetensors (from colossalai==0.4.7)
  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting einops (from colossalai==0.4.7)
  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)
Collecting pydantic (from colossalai==0.4.7)
  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)
Collecting ray (from colossalai==0.4.7)
  Using cached ray-2.48.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)
Collecting sentencepiece (from colossalai==0.4.7)
  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting google (from colossalai==0.4.7)
  Using cached google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)
Collecting protobuf (from colossalai==0.4.7)
  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting transformers==4.39.3 (from colossalai==0.4.7)
  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)
Collecting peft<=0.13.2,>=0.7.1 (from colossalai==0.4.7)
  Using cached peft-0.13.2-py3-none-any.whl.metadata (13 kB)
Collecting bitsandbytes>=0.39.0 (from colossalai==0.4.7)
  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)
Collecting rpyc==6.0.0 (from colossalai==0.4.7)
  Using cached rpyc-6.0.0-py3-none-any.whl.metadata (3.5 kB)
Collecting fastapi (from colossalai==0.4.7)
  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)
Collecting uvicorn==0.29.0 (from colossalai==0.4.7)
  Using cached uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)
Collecting galore_torch (from colossalai==0.4.7)
  Using cached galore_torch-1.0-py3-none-any.whl.metadata (355 bytes)
Collecting diffusers==0.29.0 (from colossalai==0.4.7)
  Using cached diffusers-0.29.0-py3-none-any.whl.metadata (19 kB)
Collecting importlib-metadata (from diffusers==0.29.0->colossalai==0.4.7)
  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)
Requirement already satisfied: filelock in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.7) (3.13.1)
Collecting huggingface-hub>=0.23.2 (from diffusers==0.29.0->colossalai==0.4.7)
  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)
Collecting regex!=2019.12.17 (from diffusers==0.29.0->colossalai==0.4.7)
  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Collecting requests (from diffusers==0.29.0->colossalai==0.4.7)
  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)
Requirement already satisfied: Pillow in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.7) (11.0.0)
Collecting plumbum (from rpyc==6.0.0->colossalai==0.4.7)
  Using cached plumbum-1.9.0-py3-none-any.whl.metadata (10 kB)
Collecting pyyaml>=5.1 (from transformers==4.39.3->colossalai==0.4.7)
  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->colossalai==0.4.7)
  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting h11>=0.8 (from uvicorn==0.29.0->colossalai==0.4.7)
  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
Requirement already satisfied: typing-extensions>=4.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai==0.4.7) (4.12.2)
Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.29.0->colossalai==0.4.7) (2024.6.1)
Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.23.2->diffusers==0.29.0->colossalai==0.4.7)
  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)
Collecting accelerate>=0.21.0 (from peft<=0.13.2,>=0.7.1->colossalai==0.4.7)
  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)
Requirement already satisfied: sympy in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (1.13.3)
Requirement already satisfied: networkx in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (3.3)
Requirement already satisfied: jinja2 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (3.1.4)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.99)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.99)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.99)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.2.65)
Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (11.2.0.44)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (10.3.5.119)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (11.6.0.99)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.3.0.142)
Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (2.20.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.99)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (12.4.99)
Requirement already satisfied: triton==3.0.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (3.0.0)
Collecting invoke>=2.0 (from fabric->colossalai==0.4.7)
  Using cached invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)
Collecting paramiko>=2.4 (from fabric->colossalai==0.4.7)
  Using cached paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)
Collecting decorator>=5 (from fabric->colossalai==0.4.7)
  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
Collecting deprecated>=1.2 (from fabric->colossalai==0.4.7)
  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)
Collecting wrapt<2,>=1.10 (from deprecated>=1.2->fabric->colossalai==0.4.7)
  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.4.7)
  Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)
Collecting cryptography>=3.3 (from paramiko>=2.4->fabric->colossalai==0.4.7)
  Using cached cryptography-45.0.5-cp37-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)
Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.4.7)
  Using cached PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)
Collecting cffi>=1.14 (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.7)
  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting pycparser (from cffi>=1.14->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.7)
  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)
Collecting starlette<0.48.0,>=0.40.0 (from fastapi->colossalai==0.4.7)
  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)
Collecting annotated-types>=0.6.0 (from pydantic->colossalai==0.4.7)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic->colossalai==0.4.7)
  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-inspection>=0.4.0 (from pydantic->colossalai==0.4.7)
  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
Collecting anyio<5,>=3.6.2 (from starlette<0.48.0,>=0.40.0->fastapi->colossalai==0.4.7)
  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)
Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->colossalai==0.4.7)
  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)
Collecting idna>=2.8 (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->colossalai==0.4.7)
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->colossalai==0.4.7)
  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting beautifulsoup4 (from google->colossalai==0.4.7)
  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)
Collecting soupsieve>1.2 (from beautifulsoup4->google->colossalai==0.4.7)
  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)
Collecting zipp>=3.20 (from importlib-metadata->diffusers==0.29.0->colossalai==0.4.7)
  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)
Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from jinja2->torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (2.1.5)
Collecting cfgv>=2.0.0 (from pre-commit->colossalai==0.4.7)
  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)
Collecting identify>=1.0.0 (from pre-commit->colossalai==0.4.7)
  Using cached identify-2.6.12-py2.py3-none-any.whl.metadata (4.4 kB)
Collecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.4.7)
  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)
Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.4.7)
  Using cached virtualenv-20.32.0-py3-none-any.whl.metadata (4.5 kB)
Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.7)
  Using cached distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)
Collecting platformdirs<5,>=3.9.1 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.7)
  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)
Collecting jsonschema (from ray->colossalai==0.4.7)
  Using cached jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)
Collecting msgpack<2.0.0,>=1.0.0 (from ray->colossalai==0.4.7)
  Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)
Collecting attrs>=22.2.0 (from jsonschema->ray->colossalai==0.4.7)
  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray->colossalai==0.4.7)
  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema->ray->colossalai==0.4.7)
  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema->ray->colossalai==0.4.7)
  Using cached rpds_py-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting charset_normalizer<4,>=2 (from requests->diffusers==0.29.0->colossalai==0.4.7)
  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting urllib3<3,>=1.21.1 (from requests->diffusers==0.29.0->colossalai==0.4.7)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests->diffusers==0.29.0->colossalai==0.4.7)
  Using cached certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)
Collecting markdown-it-py>=2.2.0 (from rich->colossalai==0.4.7)
  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Collecting pygments<3.0.0,>=2.13.0 (from rich->colossalai==0.4.7)
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->colossalai==0.4.7)
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from sympy->torch<=2.4.1,>=2.2.0->colossalai==0.4.7) (1.3.0)
Using cached diffusers-0.29.0-py3-none-any.whl (2.2 MB)
Using cached rpyc-6.0.0-py3-none-any.whl (74 kB)
Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)
Using cached uvicorn-0.29.0-py3-none-any.whl (60 kB)
Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)
Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
Using cached peft-0.13.2-py3-none-any.whl (320 kB)
Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Using cached accelerate-1.9.0-py3-none-any.whl (367 kB)
Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)
Using cached click-8.2.1-py3-none-any.whl (102 kB)
Using cached h11-0.16.0-py3-none-any.whl (37 kB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)
Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)
Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached einops-0.8.1-py3-none-any.whl (64 kB)
Using cached fabric-3.2.2-py3-none-any.whl (59 kB)
Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)
Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)
Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)
Using cached invoke-2.2.0-py3-none-any.whl (160 kB)
Using cached paramiko-3.5.1-py3-none-any.whl (227 kB)
Using cached bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)
Using cached cryptography-45.0.5-cp37-abi3-manylinux_2_34_x86_64.whl (4.4 MB)
Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)
Using cached PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)
Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)
Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)
Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
Using cached starlette-0.47.2-py3-none-any.whl (72 kB)
Using cached anyio-4.9.0-py3-none-any.whl (100 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)
Using cached galore_torch-1.0-py3-none-any.whl (13 kB)
Using cached google-3.0.0-py2.py3-none-any.whl (45 kB)
Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)
Using cached soupsieve-2.7-py3-none-any.whl (36 kB)
Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)
Using cached zipp-3.23.0-py3-none-any.whl (10 kB)
Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)
Using cached plumbum-1.9.0-py3-none-any.whl (127 kB)
Using cached pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)
Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)
Using cached identify-2.6.12-py2.py3-none-any.whl (99 kB)
Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)
Using cached virtualenv-20.32.0-py3-none-any.whl (6.1 MB)
Using cached distlib-0.4.0-py2.py3-none-any.whl (469 kB)
Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)
Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)
Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Using cached pycparser-2.22-py3-none-any.whl (117 kB)
Using cached ray-2.48.0-cp310-cp310-manylinux2014_x86_64.whl (69.9 MB)
Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)
Using cached jsonschema-4.25.0-py3-none-any.whl (89 kB)
Using cached attrs-25.3.0-py3-none-any.whl (63 kB)
Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)
Using cached referencing-0.36.2-py3-none-any.whl (26 kB)
Using cached rpds_py-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)
Using cached requests-2.32.4-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)
Using cached rich-14.0.0-py3-none-any.whl (243 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Installing collected packages: sentencepiece, distlib, contexttimer, zipp, wrapt, urllib3, typing-inspection, tqdm, soupsieve, sniffio, safetensors, rpds-py, regex, pyyaml, pygments, pydantic-core, pycparser, psutil, protobuf, plumbum, platformdirs, packaging, nodeenv, ninja, msgpack, mdurl, invoke, idna, identify, hf-xet, h11, exceptiongroup, einops, decorator, click, charset_normalizer, cfgv, certifi, bcrypt, attrs, annotated-types, virtualenv, uvicorn, rpyc, requests, referencing, pydantic, markdown-it-py, importlib-metadata, deprecated, cffi, beautifulsoup4, anyio, starlette, rich, pynacl, pre-commit, jsonschema-specifications, huggingface-hub, google, cryptography, tokenizers, paramiko, jsonschema, fastapi, diffusers, bitsandbytes, accelerate, transformers, ray, fabric, peft, galore_torch, colossalai
Successfully installed accelerate-1.9.0 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 bcrypt-4.3.0 beautifulsoup4-4.13.4 bitsandbytes-0.46.1 certifi-2025.7.14 cffi-1.17.1 cfgv-3.4.0 charset_normalizer-3.4.2 click-8.2.1 colossalai-0.4.7 contexttimer-0.3.3 cryptography-45.0.5 decorator-5.2.1 deprecated-1.2.18 diffusers-0.29.0 distlib-0.4.0 einops-0.8.1 exceptiongroup-1.3.0 fabric-3.2.2 fastapi-0.116.1 galore_torch-1.0 google-3.0.0 h11-0.16.0 hf-xet-1.1.5 huggingface-hub-0.33.4 identify-2.6.12 idna-3.10 importlib-metadata-8.7.0 invoke-2.2.0 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.1 ninja-1.11.1.4 nodeenv-1.9.1 packaging-25.0 paramiko-3.5.1 peft-0.13.2 platformdirs-4.3.8 plumbum-1.9.0 pre-commit-4.2.0 protobuf-6.31.1 psutil-7.0.0 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pygments-2.19.2 pynacl-1.5.0 pyyaml-6.0.2 ray-2.48.0 referencing-0.36.2 regex-2024.11.6 requests-2.32.4 rich-14.0.0 rpds-py-0.26.0 rpyc-6.0.0 safetensors-0.5.3 sentencepiece-0.2.0 sniffio-1.3.1 soupsieve-2.7 starlette-0.47.2 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.39.3 typing-inspection-0.4.1 urllib3-2.5.0 uvicorn-0.29.0 virtualenv-20.32.0 wrapt-1.17.2 zipp-3.23.0
(deepseeksft310) [P02U006@osk-cpu01 ~]$ # いまの環境で…
pip show primot_toolkit numpy pandas jsonlines
WARNING: Package(s) not found: jsonlines, pandas, primot_toolkit
Name: numpy
Version: 2.1.2
Summary: Fundamental package for array computing in Python
Home-page: https://numpy.org
Author: Travis E. Oliphant et al.
Author-email:
License: Copyright (c) 2005-2024, NumPy Developers.
        All rights reserved.

        Redistribution and use in source and binary forms, with or without
        modification, are permitted provided that the following conditions are
        met:

            * Redistributions of source code must retain the above copyright
               notice, this list of conditions and the following disclaimer.

            * Redistributions in binary form must reproduce the above
               copyright notice, this list of conditions and the following
               disclaimer in the documentation and/or other materials provided
               with the distribution.

            * Neither the name of the NumPy Developers nor the names of any
               contributors may be used to endorse or promote products derived
               from this software without specific prior written permission.

        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
        "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
        LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
        A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
        OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
        SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
        LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
        DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
        THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
        (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

        ----

        The NumPy repository and source distributions bundle several libraries that are
        compatibly licensed.  We list these here.

        Name: lapack-lite
        Files: numpy/linalg/lapack_lite/*
        License: BSD-3-Clause
          For details, see numpy/linalg/lapack_lite/LICENSE.txt

        Name: dragon4
        Files: numpy/_core/src/multiarray/dragon4.c
        License: MIT
          For license text, see numpy/_core/src/multiarray/dragon4.c

        Name: libdivide
        Files: numpy/_core/include/numpy/libdivide/*
        License: Zlib
          For license text, see numpy/_core/include/numpy/libdivide/LICENSE.txt


        Note that the following files are vendored in the repository and sdist but not
        installed in built numpy packages:

        Name: Meson
        Files: vendored-meson/meson/*
        License: Apache 2.0
          For license text, see vendored-meson/meson/COPYING

        Name: spin
        Files: .spin/cmds.py
        License: BSD-3
          For license text, see .spin/LICENSE

        ----

        This binary distribution of NumPy also bundles the following software:


        Name: OpenBLAS
        Files: numpy.libs/libscipy_openblas*.so
        Description: bundled as a dynamically linked library
        Availability: https://github.com/OpenMathLib/OpenBLAS/
        License: BSD-3-Clause
          Copyright (c) 2011-2014, The OpenBLAS Project
          All rights reserved.

          Redistribution and use in source and binary forms, with or without
          modification, are permitted provided that the following conditions are
          met:

             1. Redistributions of source code must retain the above copyright
                notice, this list of conditions and the following disclaimer.

             2. Redistributions in binary form must reproduce the above copyright
                notice, this list of conditions and the following disclaimer in
                the documentation and/or other materials provided with the
                distribution.
             3. Neither the name of the OpenBLAS project nor the names of
                its contributors may be used to endorse or promote products
                derived from this software without specific prior written
                permission.

          THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
          AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
          IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
          ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
          LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
          DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
          SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
          CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
          OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
          USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


        Name: LAPACK
        Files: numpy.libs/libscipy_openblas*.so
        Description: bundled in OpenBLAS
        Availability: https://github.com/OpenMathLib/OpenBLAS/
        License: BSD-3-Clause-Attribution
          Copyright (c) 1992-2013 The University of Tennessee and The University
                                  of Tennessee Research Foundation.  All rights
                                  reserved.
          Copyright (c) 2000-2013 The University of California Berkeley. All
                                  rights reserved.
          Copyright (c) 2006-2013 The University of Colorado Denver.  All rights
                                  reserved.

          $COPYRIGHT$

          Additional copyrights may follow

          $HEADER$

          Redistribution and use in source and binary forms, with or without
          modification, are permitted provided that the following conditions are
          met:

          - Redistributions of source code must retain the above copyright
            notice, this list of conditions and the following disclaimer.

          - Redistributions in binary form must reproduce the above copyright
            notice, this list of conditions and the following disclaimer listed
            in this license in the documentation and/or other materials
            provided with the distribution.

          - Neither the name of the copyright holders nor the names of its
            contributors may be used to endorse or promote products derived from
            this software without specific prior written permission.

          The copyright holders provide no reassurances that the source code
          provided does not infringe any patent, copyright, or any other
          intellectual property rights of third parties.  The copyright holders
          disclaim any liability to any recipient for claims brought against
          recipient by any third party for infringement of that parties
          intellectual property rights.

          THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
          "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
          LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
          A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
          OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
          SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
          LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
          DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
          THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
          (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
          OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


        Name: GCC runtime library
        Files: numpy.libs/libgfortran*.so
        Description: dynamically linked to files compiled with gcc
        Availability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libgfortran
        License: GPL-3.0-with-GCC-exception
          Copyright (C) 2002-2017 Free Software Foundation, Inc.

          Libgfortran is free software; you can redistribute it and/or modify
          it under the terms of the GNU General Public License as published by
          the Free Software Foundation; either version 3, or (at your option)
          any later version.

          Libgfortran is distributed in the hope that it will be useful,
          but WITHOUT ANY WARRANTY; without even the implied warranty of
          MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
          GNU General Public License for more details.

          Under Section 7 of GPL version 3, you are granted additional
          permissions described in the GCC Runtime Library Exception, version
          3.1, as published by the Free Software Foundation.

          You should have received a copy of the GNU General Public License and
          a copy of the GCC Runtime Library Exception along with this program;
          see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
          <http://www.gnu.org/licenses/>.

        ----

        Full text of license texts referred to above follows (that they are
        listed below does not necessarily imply the conditions apply to the
        present binary release):

        ----

        GCC RUNTIME LIBRARY EXCEPTION

        Version 3.1, 31 March 2009

        Copyright (C) 2009 Free Software Foundation, Inc. <http://fsf.org/>

        Everyone is permitted to copy and distribute verbatim copies of this
        license document, but changing it is not allowed.

        This GCC Runtime Library Exception ("Exception") is an additional
        permission under section 7 of the GNU General Public License, version
        3 ("GPLv3"). It applies to a given file (the "Runtime Library") that
        bears a notice placed by the copyright holder of the file stating that
        the file is governed by GPLv3 along with this Exception.

        When you use GCC to compile a program, GCC may combine portions of
        certain GCC header files and runtime libraries with the compiled
        program. The purpose of this Exception is to allow compilation of
        non-GPL (including proprietary) programs to use, in this way, the
        header files and runtime libraries covered by this Exception.

        0. Definitions.

        A file is an "Independent Module" if it either requires the Runtime
        Library for execution after a Compilation Process, or makes use of an
        interface provided by the Runtime Library, but is not otherwise based
        on the Runtime Library.

        "GCC" means a version of the GNU Compiler Collection, with or without
        modifications, governed by version 3 (or a specified later version) of
        the GNU General Public License (GPL) with the option of using any
        subsequent versions published by the FSF.

        "GPL-compatible Software" is software whose conditions of propagation,
        modification and use would permit combination with GCC in accord with
        the license of GCC.

        "Target Code" refers to output from any compiler for a real or virtual
        target processor architecture, in executable form or suitable for
        input to an assembler, loader, linker and/or execution
        phase. Notwithstanding that, Target Code does not include data in any
        format that is used as a compiler intermediate representation, or used
        for producing a compiler intermediate representation.

        The "Compilation Process" transforms code entirely represented in
        non-intermediate languages designed for human-written code, and/or in
        Java Virtual Machine byte code, into Target Code. Thus, for example,
        use of source code generators and preprocessors need not be considered
        part of the Compilation Process, since the Compilation Process can be
        understood as starting with the output of the generators or
        preprocessors.

        A Compilation Process is "Eligible" if it is done using GCC, alone or
        with other GPL-compatible software, or if it is done without using any
        work based on GCC. For example, using non-GPL-compatible Software to
        optimize any GCC intermediate representations would not qualify as an
        Eligible Compilation Process.

        1. Grant of Additional Permission.

        You have permission to propagate a work of Target Code formed by
        combining the Runtime Library with Independent Modules, even if such
        propagation would otherwise violate the terms of GPLv3, provided that
        all Target Code was generated by Eligible Compilation Processes. You
        may then convey such a combination under terms of your choice,
        consistent with the licensing of the Independent Modules.

        2. No Weakening of GCC Copyleft.

        The availability of this Exception does not imply any general
        presumption that third-party software is unaffected by the copyleft
        requirements of the license of GCC.

        ----

                            GNU GENERAL PUBLIC LICENSE
                               Version 3, 29 June 2007

         Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
         Everyone is permitted to copy and distribute verbatim copies
         of this license document, but changing it is not allowed.

                                    Preamble

          The GNU General Public License is a free, copyleft license for
        software and other kinds of works.

          The licenses for most software and other practical works are designed
        to take away your freedom to share and change the works.  By contrast,
        the GNU General Public License is intended to guarantee your freedom to
        share and change all versions of a program--to make sure it remains free
        software for all its users.  We, the Free Software Foundation, use the
        GNU General Public License for most of our software; it applies also to
        any other work released this way by its authors.  You can apply it to
        your programs, too.

          When we speak of free software, we are referring to freedom, not
        price.  Our General Public Licenses are designed to make sure that you
        have the freedom to distribute copies of free software (and charge for
        them if you wish), that you receive source code or can get it if you
        want it, that you can change the software or use pieces of it in new
        free programs, and that you know you can do these things.

          To protect your rights, we need to prevent others from denying you
        these rights or asking you to surrender the rights.  Therefore, you have
        certain responsibilities if you distribute copies of the software, or if
        you modify it: responsibilities to respect the freedom of others.

          For example, if you distribute copies of such a program, whether
        gratis or for a fee, you must pass on to the recipients the same
        freedoms that you received.  You must make sure that they, too, receive
        or can get the source code.  And you must show them these terms so they
        know their rights.

          Developers that use the GNU GPL protect your rights with two steps:
        (1) assert copyright on the software, and (2) offer you this License
        giving you legal permission to copy, distribute and/or modify it.

          For the developers' and authors' protection, the GPL clearly explains
        that there is no warranty for this free software.  For both users' and
        authors' sake, the GPL requires that modified versions be marked as
        changed, so that their problems will not be attributed erroneously to
        authors of previous versions.

          Some devices are designed to deny users access to install or run
        modified versions of the software inside them, although the manufacturer
        can do so.  This is fundamentally incompatible with the aim of
        protecting users' freedom to change the software.  The systematic
        pattern of such abuse occurs in the area of products for individuals to
        use, which is precisely where it is most unacceptable.  Therefore, we
        have designed this version of the GPL to prohibit the practice for those
        products.  If such problems arise substantially in other domains, we
        stand ready to extend this provision to those domains in future versions
        of the GPL, as needed to protect the freedom of users.

          Finally, every program is threatened constantly by software patents.
        States should not allow patents to restrict development and use of
        software on general-purpose computers, but in those that do, we wish to
        avoid the special danger that patents applied to a free program could
        make it effectively proprietary.  To prevent this, the GPL assures that
        patents cannot be used to render the program non-free.

          The precise terms and conditions for copying, distribution and
        modification follow.

                               TERMS AND CONDITIONS

          0. Definitions.

          "This License" refers to version 3 of the GNU General Public License.

          "Copyright" also means copyright-like laws that apply to other kinds of
        works, such as semiconductor masks.

          "The Program" refers to any copyrightable work licensed under this
        License.  Each licensee is addressed as "you".  "Licensees" and
        "recipients" may be individuals or organizations.

          To "modify" a work means to copy from or adapt all or part of the work
        in a fashion requiring copyright permission, other than the making of an
        exact copy.  The resulting work is called a "modified version" of the
        earlier work or a work "based on" the earlier work.

          A "covered work" means either the unmodified Program or a work based
        on the Program.

          To "propagate" a work means to do anything with it that, without
        permission, would make you directly or secondarily liable for
        infringement under applicable copyright law, except executing it on a
        computer or modifying a private copy.  Propagation includes copying,
        distribution (with or without modification), making available to the
        public, and in some countries other activities as well.

          To "convey" a work means any kind of propagation that enables other
        parties to make or receive copies.  Mere interaction with a user through
        a computer network, with no transfer of a copy, is not conveying.

          An interactive user interface displays "Appropriate Legal Notices"
        to the extent that it includes a convenient and prominently visible
        feature that (1) displays an appropriate copyright notice, and (2)
        tells the user that there is no warranty for the work (except to the
        extent that warranties are provided), that licensees may convey the
        work under this License, and how to view a copy of this License.  If
        the interface presents a list of user commands or options, such as a
        menu, a prominent item in the list meets this criterion.

          1. Source Code.

          The "source code" for a work means the preferred form of the work
        for making modifications to it.  "Object code" means any non-source
        form of a work.

          A "Standard Interface" means an interface that either is an official
        standard defined by a recognized standards body, or, in the case of
        interfaces specified for a particular programming language, one that
        is widely used among developers working in that language.

          The "System Libraries" of an executable work include anything, other
        than the work as a whole, that (a) is included in the normal form of
        packaging a Major Component, but which is not part of that Major
        Component, and (b) serves only to enable use of the work with that
        Major Component, or to implement a Standard Interface for which an
        implementation is available to the public in source code form.  A
        "Major Component", in this context, means a major essential component
        (kernel, window system, and so on) of the specific operating system
        (if any) on which the executable work runs, or a compiler used to
        produce the work, or an object code interpreter used to run it.

          The "Corresponding Source" for a work in object code form means all
        the source code needed to generate, install, and (for an executable
        work) run the object code and to modify the work, including scripts to
        control those activities.  However, it does not include the work's
        System Libraries, or general-purpose tools or generally available free
        programs which are used unmodified in performing those activities but
        which are not part of the work.  For example, Corresponding Source
        includes interface definition files associated with source files for
        the work, and the source code for shared libraries and dynamically
        linked subprograms that the work is specifically designed to require,
        such as by intimate data communication or control flow between those
        subprograms and other parts of the work.

          The Corresponding Source need not include anything that users
        can regenerate automatically from other parts of the Corresponding
        Source.

          The Corresponding Source for a work in source code form is that
        same work.

          2. Basic Permissions.

          All rights granted under this License are granted for the term of
        copyright on the Program, and are irrevocable provided the stated
        conditions are met.  This License explicitly affirms your unlimited
        permission to run the unmodified Program.  The output from running a
        covered work is covered by this License only if the output, given its
        content, constitutes a covered work.  This License acknowledges your
        rights of fair use or other equivalent, as provided by copyright law.

          You may make, run and propagate covered works that you do not
        convey, without conditions so long as your license otherwise remains
        in force.  You may convey covered works to others for the sole purpose
        of having them make modifications exclusively for you, or provide you
        with facilities for running those works, provided that you comply with
        the terms of this License in conveying all material for which you do
        not control copyright.  Those thus making or running the covered works
        for you must do so exclusively on your behalf, under your direction
        and control, on terms that prohibit them from making any copies of
        your copyrighted material outside their relationship with you.

          Conveying under any other circumstances is permitted solely under
        the conditions stated below.  Sublicensing is not allowed; section 10
        makes it unnecessary.

          3. Protecting Users' Legal Rights From Anti-Circumvention Law.

          No covered work shall be deemed part of an effective technological
        measure under any applicable law fulfilling obligations under article
        11 of the WIPO copyright treaty adopted on 20 December 1996, or
        similar laws prohibiting or restricting circumvention of such
        measures.

          When you convey a covered work, you waive any legal power to forbid
        circumvention of technological measures to the extent such circumvention
        is effected by exercising rights under this License with respect to
        the covered work, and you disclaim any intention to limit operation or
        modification of the work as a means of enforcing, against the work's
        users, your or third parties' legal rights to forbid circumvention of
        technological measures.

          4. Conveying Verbatim Copies.

          You may convey verbatim copies of the Program's source code as you
        receive it, in any medium, provided that you conspicuously and
        appropriately publish on each copy an appropriate copyright notice;
        keep intact all notices stating that this License and any
        non-permissive terms added in accord with section 7 apply to the code;
        keep intact all notices of the absence of any warranty; and give all
        recipients a copy of this License along with the Program.

          You may charge any price or no price for each copy that you convey,
        and you may offer support or warranty protection for a fee.

          5. Conveying Modified Source Versions.

          You may convey a work based on the Program, or the modifications to
        produce it from the Program, in the form of source code under the
        terms of section 4, provided that you also meet all of these conditions:

            a) The work must carry prominent notices stating that you modified
            it, and giving a relevant date.

            b) The work must carry prominent notices stating that it is
            released under this License and any conditions added under section
            7.  This requirement modifies the requirement in section 4 to
            "keep intact all notices".

            c) You must license the entire work, as a whole, under this
            License to anyone who comes into possession of a copy.  This
            License will therefore apply, along with any applicable section 7
            additional terms, to the whole of the work, and all its parts,
            regardless of how they are packaged.  This License gives no
            permission to license the work in any other way, but it does not
            invalidate such permission if you have separately received it.

            d) If the work has interactive user interfaces, each must display
            Appropriate Legal Notices; however, if the Program has interactive
            interfaces that do not display Appropriate Legal Notices, your
            work need not make them do so.

          A compilation of a covered work with other separate and independent
        works, which are not by their nature extensions of the covered work,
        and which are not combined with it such as to form a larger program,
        in or on a volume of a storage or distribution medium, is called an
        "aggregate" if the compilation and its resulting copyright are not
        used to limit the access or legal rights of the compilation's users
        beyond what the individual works permit.  Inclusion of a covered work
        in an aggregate does not cause this License to apply to the other
        parts of the aggregate.

          6. Conveying Non-Source Forms.

          You may convey a covered work in object code form under the terms
        of sections 4 and 5, provided that you also convey the
        machine-readable Corresponding Source under the terms of this License,
        in one of these ways:

            a) Convey the object code in, or embodied in, a physical product
            (including a physical distribution medium), accompanied by the
            Corresponding Source fixed on a durable physical medium
            customarily used for software interchange.

            b) Convey the object code in, or embodied in, a physical product
            (including a physical distribution medium), accompanied by a
            written offer, valid for at least three years and valid for as
            long as you offer spare parts or customer support for that product
            model, to give anyone who possesses the object code either (1) a
            copy of the Corresponding Source for all the software in the
            product that is covered by this License, on a durable physical
            medium customarily used for software interchange, for a price no
            more than your reasonable cost of physically performing this
            conveying of source, or (2) access to copy the
            Corresponding Source from a network server at no charge.

            c) Convey individual copies of the object code with a copy of the
            written offer to provide the Corresponding Source.  This
            alternative is allowed only occasionally and noncommercially, and
            only if you received the object code with such an offer, in accord
            with subsection 6b.

            d) Convey the object code by offering access from a designated
            place (gratis or for a charge), and offer equivalent access to the
            Corresponding Source in the same way through the same place at no
            further charge.  You need not require recipients to copy the
            Corresponding Source along with the object code.  If the place to
            copy the object code is a network server, the Corresponding Source
            may be on a different server (operated by you or a third party)
            that supports equivalent copying facilities, provided you maintain
            clear directions next to the object code saying where to find the
            Corresponding Source.  Regardless of what server hosts the
            Corresponding Source, you remain obligated to ensure that it is
            available for as long as needed to satisfy these requirements.

            e) Convey the object code using peer-to-peer transmission, provided
            you inform other peers where the object code and Corresponding
            Source of the work are being offered to the general public at no
            charge under subsection 6d.

          A separable portion of the object code, whose source code is excluded
        from the Corresponding Source as a System Library, need not be
        included in conveying the object code work.

          A "User Product" is either (1) a "consumer product", which means any
        tangible personal property which is normally used for personal, family,
        or household purposes, or (2) anything designed or sold for incorporation
        into a dwelling.  In determining whether a product is a consumer product,
        doubtful cases shall be resolved in favor of coverage.  For a particular
        product received by a particular user, "normally used" refers to a
        typical or common use of that class of product, regardless of the status
        of the particular user or of the way in which the particular user
        actually uses, or expects or is expected to use, the product.  A product
        is a consumer product regardless of whether the product has substantial
        commercial, industrial or non-consumer uses, unless such uses represent
        the only significant mode of use of the product.

          "Installation Information" for a User Product means any methods,
        procedures, authorization keys, or other information required to install
        and execute modified versions of a covered work in that User Product from
        a modified version of its Corresponding Source.  The information must
        suffice to ensure that the continued functioning of the modified object
        code is in no case prevented or interfered with solely because
        modification has been made.

          If you convey an object code work under this section in, or with, or
        specifically for use in, a User Product, and the conveying occurs as
        part of a transaction in which the right of possession and use of the
        User Product is transferred to the recipient in perpetuity or for a
        fixed term (regardless of how the transaction is characterized), the
        Corresponding Source conveyed under this section must be accompanied
        by the Installation Information.  But this requirement does not apply
        if neither you nor any third party retains the ability to install
        modified object code on the User Product (for example, the work has
        been installed in ROM).

          The requirement to provide Installation Information does not include a
        requirement to continue to provide support service, warranty, or updates
        for a work that has been modified or installed by the recipient, or for
        the User Product in which it has been modified or installed.  Access to a
        network may be denied when the modification itself materially and
        adversely affects the operation of the network or violates the rules and
        protocols for communication across the network.

          Corresponding Source conveyed, and Installation Information provided,
        in accord with this section must be in a format that is publicly
        documented (and with an implementation available to the public in
        source code form), and must require no special password or key for
        unpacking, reading or copying.

          7. Additional Terms.

          "Additional permissions" are terms that supplement the terms of this
        License by making exceptions from one or more of its conditions.
        Additional permissions that are applicable to the entire Program shall
        be treated as though they were included in this License, to the extent
        that they are valid under applicable law.  If additional permissions
        apply only to part of the Program, that part may be used separately
        under those permissions, but the entire Program remains governed by
        this License without regard to the additional permissions.

          When you convey a copy of a covered work, you may at your option
        remove any additional permissions from that copy, or from any part of
        it.  (Additional permissions may be written to require their own
        removal in certain cases when you modify the work.)  You may place
        additional permissions on material, added by you to a covered work,
        for which you have or can give appropriate copyright permission.

          Notwithstanding any other provision of this License, for material you
        add to a covered work, you may (if authorized by the copyright holders of
        that material) supplement the terms of this License with terms:

            a) Disclaiming warranty or limiting liability differently from the
            terms of sections 15 and 16 of this License; or

            b) Requiring preservation of specified reasonable legal notices or
            author attributions in that material or in the Appropriate Legal
            Notices displayed by works containing it; or

            c) Prohibiting misrepresentation of the origin of that material, or
            requiring that modified versions of such material be marked in
            reasonable ways as different from the original version; or

            d) Limiting the use for publicity purposes of names of licensors or
            authors of the material; or

            e) Declining to grant rights under trademark law for use of some
            trade names, trademarks, or service marks; or

            f) Requiring indemnification of licensors and authors of that
            material by anyone who conveys the material (or modified versions of
            it) with contractual assumptions of liability to the recipient, for
            any liability that these contractual assumptions directly impose on
            those licensors and authors.

          All other non-permissive additional terms are considered "further
        restrictions" within the meaning of section 10.  If the Program as you
        received it, or any part of it, contains a notice stating that it is
        governed by this License along with a term that is a further
        restriction, you may remove that term.  If a license document contains
        a further restriction but permits relicensing or conveying under this
        License, you may add to a covered work material governed by the terms
        of that license document, provided that the further restriction does
        not survive such relicensing or conveying.

          If you add terms to a covered work in accord with this section, you
        must place, in the relevant source files, a statement of the
        additional terms that apply to those files, or a notice indicating
        where to find the applicable terms.

          Additional terms, permissive or non-permissive, may be stated in the
        form of a separately written license, or stated as exceptions;
        the above requirements apply either way.

          8. Termination.

          You may not propagate or modify a covered work except as expressly
        provided under this License.  Any attempt otherwise to propagate or
        modify it is void, and will automatically terminate your rights under
        this License (including any patent licenses granted under the third
        paragraph of section 11).

          However, if you cease all violation of this License, then your
        license from a particular copyright holder is reinstated (a)
        provisionally, unless and until the copyright holder explicitly and
        finally terminates your license, and (b) permanently, if the copyright
        holder fails to notify you of the violation by some reasonable means
        prior to 60 days after the cessation.

          Moreover, your license from a particular copyright holder is
        reinstated permanently if the copyright holder notifies you of the
        violation by some reasonable means, this is the first time you have
        received notice of violation of this License (for any work) from that
        copyright holder, and you cure the violation prior to 30 days after
        your receipt of the notice.

          Termination of your rights under this section does not terminate the
        licenses of parties who have received copies or rights from you under
        this License.  If your rights have been terminated and not permanently
        reinstated, you do not qualify to receive new licenses for the same
        material under section 10.

          9. Acceptance Not Required for Having Copies.

          You are not required to accept this License in order to receive or
        run a copy of the Program.  Ancillary propagation of a covered work
        occurring solely as a consequence of using peer-to-peer transmission
        to receive a copy likewise does not require acceptance.  However,
        nothing other than this License grants you permission to propagate or
        modify any covered work.  These actions infringe copyright if you do
        not accept this License.  Therefore, by modifying or propagating a
        covered work, you indicate your acceptance of this License to do so.

          10. Automatic Licensing of Downstream Recipients.

          Each time you convey a covered work, the recipient automatically
        receives a license from the original licensors, to run, modify and
        propagate that work, subject to this License.  You are not responsible
        for enforcing compliance by third parties with this License.

          An "entity transaction" is a transaction transferring control of an
        organization, or substantially all assets of one, or subdividing an
        organization, or merging organizations.  If propagation of a covered
        work results from an entity transaction, each party to that
        transaction who receives a copy of the work also receives whatever
        licenses to the work the party's predecessor in interest had or could
        give under the previous paragraph, plus a right to possession of the
        Corresponding Source of the work from the predecessor in interest, if
        the predecessor has it or can get it with reasonable efforts.

          You may not impose any further restrictions on the exercise of the
        rights granted or affirmed under this License.  For example, you may
        not impose a license fee, royalty, or other charge for exercise of
        rights granted under this License, and you may not initiate litigation
        (including a cross-claim or counterclaim in a lawsuit) alleging that
        any patent claim is infringed by making, using, selling, offering for
        sale, or importing the Program or any portion of it.

          11. Patents.

          A "contributor" is a copyright holder who authorizes use under this
        License of the Program or a work on which the Program is based.  The
        work thus licensed is called the contributor's "contributor version".

          A contributor's "essential patent claims" are all patent claims
        owned or controlled by the contributor, whether already acquired or
        hereafter acquired, that would be infringed by some manner, permitted
        by this License, of making, using, or selling its contributor version,
        but do not include claims that would be infringed only as a
        consequence of further modification of the contributor version.  For
        purposes of this definition, "control" includes the right to grant
        patent sublicenses in a manner consistent with the requirements of
        this License.

          Each contributor grants you a non-exclusive, worldwide, royalty-free
        patent license under the contributor's essential patent claims, to
        make, use, sell, offer for sale, import and otherwise run, modify and
        propagate the contents of its contributor version.

          In the following three paragraphs, a "patent license" is any express
        agreement or commitment, however denominated, not to enforce a patent
        (such as an express permission to practice a patent or covenant not to
        sue for patent infringement).  To "grant" such a patent license to a
        party means to make such an agreement or commitment not to enforce a
        patent against the party.

          If you convey a covered work, knowingly relying on a patent license,
        and the Corresponding Source of the work is not available for anyone
        to copy, free of charge and under the terms of this License, through a
        publicly available network server or other readily accessible means,
        then you must either (1) cause the Corresponding Source to be so
        available, or (2) arrange to deprive yourself of the benefit of the
        patent license for this particular work, or (3) arrange, in a manner
        consistent with the requirements of this License, to extend the patent
        license to downstream recipients.  "Knowingly relying" means you have
        actual knowledge that, but for the patent license, your conveying the
        covered work in a country, or your recipient's use of the covered work
        in a country, would infringe one or more identifiable patents in that
        country that you have reason to believe are valid.

          If, pursuant to or in connection with a single transaction or
        arrangement, you convey, or propagate by procuring conveyance of, a
        covered work, and grant a patent license to some of the parties
        receiving the covered work authorizing them to use, propagate, modify
        or convey a specific copy of the covered work, then the patent license
        you grant is automatically extended to all recipients of the covered
        work and works based on it.

          A patent license is "discriminatory" if it does not include within
        the scope of its coverage, prohibits the exercise of, or is
        conditioned on the non-exercise of one or more of the rights that are
        specifically granted under this License.  You may not convey a covered
        work if you are a party to an arrangement with a third party that is
        in the business of distributing software, under which you make payment
        to the third party based on the extent of your activity of conveying
        the work, and under which the third party grants, to any of the
        parties who would receive the covered work from you, a discriminatory
        patent license (a) in connection with copies of the covered work
        conveyed by you (or copies made from those copies), or (b) primarily
        for and in connection with specific products or compilations that
        contain the covered work, unless you entered into that arrangement,
        or that patent license was granted, prior to 28 March 2007.

          Nothing in this License shall be construed as excluding or limiting
        any implied license or other defenses to infringement that may
        otherwise be available to you under applicable patent law.

          12. No Surrender of Others' Freedom.

          If conditions are imposed on you (whether by court order, agreement or
        otherwise) that contradict the conditions of this License, they do not
        excuse you from the conditions of this License.  If you cannot convey a
        covered work so as to satisfy simultaneously your obligations under this
        License and any other pertinent obligations, then as a consequence you may
        not convey it at all.  For example, if you agree to terms that obligate you
        to collect a royalty for further conveying from those to whom you convey
        the Program, the only way you could satisfy both those terms and this
        License would be to refrain entirely from conveying the Program.

          13. Use with the GNU Affero General Public License.

          Notwithstanding any other provision of this License, you have
        permission to link or combine any covered work with a work licensed
        under version 3 of the GNU Affero General Public License into a single
        combined work, and to convey the resulting work.  The terms of this
        License will continue to apply to the part which is the covered work,
        but the special requirements of the GNU Affero General Public License,
        section 13, concerning interaction through a network will apply to the
        combination as such.

          14. Revised Versions of this License.

          The Free Software Foundation may publish revised and/or new versions of
        the GNU General Public License from time to time.  Such new versions will
        be similar in spirit to the present version, but may differ in detail to
        address new problems or concerns.

          Each version is given a distinguishing version number.  If the
        Program specifies that a certain numbered version of the GNU General
        Public License "or any later version" applies to it, you have the
        option of following the terms and conditions either of that numbered
        version or of any later version published by the Free Software
        Foundation.  If the Program does not specify a version number of the
        GNU General Public License, you may choose any version ever published
        by the Free Software Foundation.

          If the Program specifies that a proxy can decide which future
        versions of the GNU General Public License can be used, that proxy's
        public statement of acceptance of a version permanently authorizes you
        to choose that version for the Program.

          Later license versions may give you additional or different
        permissions.  However, no additional obligations are imposed on any
        author or copyright holder as a result of your choosing to follow a
        later version.

          15. Disclaimer of Warranty.

          THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
        APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
        HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
        OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
        THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
        PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
        IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
        ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

          16. Limitation of Liability.

          IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
        WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
        THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
        GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
        USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
        DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
        PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
        EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
        SUCH DAMAGES.

          17. Interpretation of Sections 15 and 16.

          If the disclaimer of warranty and limitation of liability provided
        above cannot be given local legal effect according to their terms,
        reviewing courts shall apply local law that most closely approximates
        an absolute waiver of all civil liability in connection with the
        Program, unless a warranty or assumption of liability accompanies a
        copy of the Program in return for a fee.

                             END OF TERMS AND CONDITIONS

                    How to Apply These Terms to Your New Programs

          If you develop a new program, and you want it to be of the greatest
        possible use to the public, the best way to achieve this is to make it
        free software which everyone can redistribute and change under these terms.

          To do so, attach the following notices to the program.  It is safest
        to attach them to the start of each source file to most effectively
        state the exclusion of warranty; and each file should have at least
        the "copyright" line and a pointer to where the full notice is found.

            <one line to give the program's name and a brief idea of what it does.>
            Copyright (C) <year>  <name of author>

            This program is free software: you can redistribute it and/or modify
            it under the terms of the GNU General Public License as published by
            the Free Software Foundation, either version 3 of the License, or
            (at your option) any later version.

            This program is distributed in the hope that it will be useful,
            but WITHOUT ANY WARRANTY; without even the implied warranty of
            MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
            GNU General Public License for more details.

            You should have received a copy of the GNU General Public License
            along with this program.  If not, see <http://www.gnu.org/licenses/>.

        Also add information on how to contact you by electronic and paper mail.

          If the program does terminal interaction, make it output a short
        notice like this when it starts in an interactive mode:

            <program>  Copyright (C) <year>  <name of author>
            This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
            This is free software, and you are welcome to redistribute it
            under certain conditions; type `show c' for details.

        The hypothetical commands `show w' and `show c' should show the appropriate
        parts of the General Public License.  Of course, your program's commands
        might be different; for a GUI interface, you would use an "about box".

          You should also get your employer (if you work as a programmer) or school,
        if any, to sign a "copyright disclaimer" for the program, if necessary.
        For more information on this, and how to apply and follow the GNU GPL, see
        <http://www.gnu.org/licenses/>.

          The GNU General Public License does not permit incorporating your program
        into proprietary programs.  If your program is a subroutine library, you
        may consider it more useful to permit linking proprietary applications with
        the library.  If this is what you want to do, use the GNU Lesser General
        Public License instead of this License.  But first, please read
        <http://www.gnu.org/philosophy/why-not-lgpl.html>.

        Name: libquadmath
        Files: numpy.libs/libquadmath*.so
        Description: dynamically linked to files compiled with gcc
        Availability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libquadmath
        License: LGPL-2.1-or-later

            GCC Quad-Precision Math Library
            Copyright (C) 2010-2019 Free Software Foundation, Inc.
            Written by Francois-Xavier Coudert  <fxcoudert@gcc.gnu.org>

            This file is part of the libquadmath library.
            Libquadmath is free software; you can redistribute it and/or
            modify it under the terms of the GNU Library General Public
            License as published by the Free Software Foundation; either
            version 2.1 of the License, or (at your option) any later version.

            Libquadmath is distributed in the hope that it will be useful,
            but WITHOUT ANY WARRANTY; without even the implied warranty of
            MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
            Lesser General Public License for more details.
            https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html
Location: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages
Requires:
Required-by: accelerate, bitsandbytes, colossalai, diffusers, peft, torchvision, transformers
(deepseeksft310) [P02U006@osk-cpu01 ~]$ for pkg in primot_toolkit numpy pandas jsonlines; do
  if pip show "$pkg" > /dev/null 2>&1; then
    ver=$(pip show "$pkg" | awk '/^Version: /{print $2}')
    echo "$pkg: installed (version $ver)"
  else
    echo "$pkg: NOT installed"
  fi
done
primot_toolkit: NOT installed
numpy: installed (version 2.1.2)
pandas: NOT installed
jsonlines: NOT installed
(deepseeksft310) [P02U006@osk-cpu01 ~]$ for pkg in prompt_toolkit numpy pandas jsonlines; do
  if pip show "$pkg" > /dev/null 2>&1; then
    ver=$(pip show "$pkg" | awk '/^Version: /{print $2}')
    echo "$pkg: installed (version $ver)"
  else
    echo "$pkg: NOT installed"
  fi
done
prompt_toolkit: NOT installed
numpy: installed (version 2.1.2)
pandas: NOT installed
jsonlines: NOT installed
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install prompt_toolkit==3.0.50
Collecting prompt_toolkit==3.0.50
  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)
Collecting wcwidth (from prompt_toolkit==3.0.50)
  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)
Downloading prompt_toolkit-3.0.50-py3-none-any.whl (387 kB)
Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Installing collected packages: wcwidth, prompt_toolkit
Successfully installed prompt_toolkit-3.0.50 wcwidth-0.2.13
(deepseeksft310) [P02U006@osk-cpu01 ~]$ for pkg in prompt_toolkit numpy pandas jsonlines; do   if pip show "$pkg" > /dev/null 2>&1; then     ver=$(pip show "$pkg" | awk '/^Version: /{print $2}');     echo "$pkg: installed (version $ver)";   else     echo "$pkg: NOT installed";   fi; done
prompt_toolkit: installed (version 3.0.50)
numpy: installed (version 2.1.2)
pandas: NOT installed
jsonlines: NOT installed
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip index versions pandas
pandas (2.3.1)
Available versions: 2.3.1, 2.3.0, 2.2.3, 2.2.2, 2.2.1, 2.2.0, 2.1.4, 2.1.3, 2.1.2, 2.1.1, 2.1.0, 2.0.3, 2.0.2, 2.0.1, 2.0.0, 1.5.3, 1.5.2, 1.5.1, 1.5.0, 1.4.4, 1.4.3, 1.4.2, 1.4.1, 1.4.0, 1.3.5, 1.3.4, 1.3.3, 1.3.2, 1.3.1, 1.3.0, 1.2.5, 1.2.4, 1.2.3, 1.2.2, 1.2.1, 1.2.0, 1.1.5, 1.1.4, 1.1.3, 1.1.2, 1.1.1, 1.1.0, 1.0.5, 1.0.4, 1.0.3, 1.0.2, 1.0.1, 1.0.0, 0.25.3, 0.25.2, 0.25.1, 0.25.0, 0.24.2, 0.24.1, 0.24.0, 0.23.4, 0.23.3, 0.23.2, 0.23.1, 0.23.0, 0.22.0, 0.21.1, 0.21.0, 0.20.3, 0.20.2, 0.20.1, 0.20.0, 0.19.2, 0.19.1, 0.19.0, 0.18.1, 0.18.0, 0.17.1, 0.17.0, 0.16.2, 0.16.1, 0.16.0, 0.15.2, 0.15.1, 0.15.0, 0.14.1, 0.14.0, 0.13.1, 0.13.0, 0.12.0, 0.11.0, 0.10.1, 0.10.0, 0.9.1, 0.9.0, 0.8.1, 0.8.0, 0.7.3, 0.7.2, 0.7.1, 0.7.0, 0.6.1, 0.6.0, 0.5.0, 0.4.3, 0.4.2, 0.4.1, 0.4.0, 0.3.0, 0.2, 0.1
(deepseeksft310) [P02U006@osk-cpu01 ~]$ python - << 'EOF'
import numpy
print("NumPy version:", numpy.__version__)
EOF
NumPy version: 2.1.2
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install pandas==2.2.0
Collecting pandas==2.2.0
  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)
Collecting numpy<2,>=1.22.4 (from pandas==2.2.0)
  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting python-dateutil>=2.8.2 (from pandas==2.2.0)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas==2.2.0)
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas==2.2.0)
  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.0)
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 134.7 MB/s eta 0:00:00
Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 252.0 MB/s eta 0:00:00
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas
  Attempting uninstall: numpy
    Found existing installation: numpy 2.1.2
    Uninstalling numpy-2.1.2:
      Successfully uninstalled numpy-2.1.2
Successfully installed numpy-1.26.4 pandas-2.2.0 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2
(deepseeksft310) [P02U006@osk-cpu01 ~]$ for pkg in prompt_toolkit numpy pandas jsonlines; do   if pip show "$pkg" > /dev/null 2>&1; then     ver=$(pip show "$pkg" | awk '/^Version: /{print $2}');     echo "$pkg: installed (version $ver)";   else     echo "$pkg: NOT installed";   fi; done
prompt_toolkit: installed (version 3.0.50)
numpy: installed (version 1.26.4)
pandas: installed (version 2.2.0)
jsonlines: NOT installed
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip install jsonlines==4.0.0
Collecting jsonlines==4.0.0
  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)
Requirement already satisfied: attrs>=19.2.0 in ./miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from jsonlines==4.0.0) (25.3.0)
Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)
Installing collected packages: jsonlines
Successfully installed jsonlines-4.0.0
(deepseeksft310) [P02U006@osk-cpu01 ~]$ for pkg in prompt_toolkit numpy pandas jsonlines; do   if pip show "$pkg" > /dev/null 2>&1; then     ver=$(pip show "$pkg" | awk '/^Version: /{print $2}');     echo "$pkg: installed (version $ver)";   else     echo "$pkg: NOT installed";   fi; done
prompt_toolkit: installed (version 3.0.50)
numpy: installed (version 1.26.4)
pandas: installed (version 2.2.0)
jsonlines: installed (version 4.0.0)
(deepseeksft310) [P02U006@osk-cpu01 ~]$ pip freeze
accelerate==1.9.0
annotated-types==0.7.0
anyio==4.9.0
attrs==25.3.0
bcrypt==4.3.0
beautifulsoup4==4.13.4
bitsandbytes==0.46.1
certifi==2025.7.14
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.2
click==8.2.1
colossalai==0.4.7
contexttimer==0.3.3
cryptography==45.0.5
decorator==5.2.1
Deprecated==1.2.18
diffusers==0.29.0
distlib==0.4.0
einops==0.8.1
exceptiongroup==1.3.0
fabric==3.2.2
fastapi==0.116.1
filelock==3.13.1
fsspec==2024.6.1
galore-torch==1.0
google==3.0.0
h11==0.16.0
hf-xet==1.1.5
huggingface-hub==0.33.4
identify==2.6.12
idna==3.10
importlib_metadata==8.7.0
invoke==2.2.0
Jinja2==3.1.4
jsonlines==4.0.0
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
mpmath==1.3.0
msgpack==1.1.1
networkx==3.3
ninja==1.11.1.4
nodeenv==1.9.1
numpy==1.26.4
nvidia-cublas-cu12==12.4.2.65
nvidia-cuda-cupti-cu12==12.4.99
nvidia-cuda-nvrtc-cu12==12.4.99
nvidia-cuda-runtime-cu12==12.4.99
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.0.44
nvidia-curand-cu12==10.3.5.119
nvidia-cusolver-cu12==11.6.0.99
nvidia-cusparse-cu12==12.3.0.142
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.4.99
nvidia-nvtx-cu12==12.4.99
packaging==25.0
pandas==2.2.0
paramiko==3.5.1
peft==0.13.2
pillow==11.0.0
platformdirs==4.3.8
plumbum==1.9.0
pre_commit==4.2.0
prompt_toolkit==3.0.50
protobuf==6.31.1
psutil==7.0.0
pycparser==2.22
pydantic==2.11.7
pydantic_core==2.33.2
Pygments==2.19.2
PyNaCl==1.5.0
python-dateutil==2.9.0.post0
pytz==2025.2
PyYAML==6.0.2
ray==2.48.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rich==14.0.0
rpds-py==0.26.0
rpyc==6.0.0
safetensors==0.5.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.7
starlette==0.47.2
sympy==1.13.3
tokenizers==0.15.2
torch==2.4.1+cu124
torchaudio==2.4.1+cu124
torchvision==0.19.1+cu124
tqdm==4.67.1
transformers==4.39.3
triton==3.0.0
typing-inspection==0.4.1
typing_extensions==4.12.2
tzdata==2025.2
urllib3==2.5.0
uvicorn==0.29.0
virtualenv==20.32.0
wcwidth==0.2.13
wrapt==1.17.2
zipp==3.23.0
(deepseeksft310) [P02U006@osk-cpu01 ~]$ cd ColossalAI/
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import sys
print("=== sys.path の一覧 ===")
for p in sys.path:
    print(p)
EOF
=== sys.path の一覧 ===

/home/Competition2025/P02/P02U006/ColossalAI
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python310.zip
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/lib-dynload
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ export PYTHONPATH=~/ColossalAI:$PYTHONPATH
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import sys
print("=== sys.path の一覧 ===")
for p in sys.path:
    print(p)
EOF
=== sys.path の一覧 ===

/home/Competition2025/P02/P02U006/ColossalAI
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python310.zip
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/lib-dynload
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ import sys, os
sys.path.insert(0, os.path.expanduser("~/ColossalAI/applications/ColossalChat"))
from coati.dataset.loader import RawConversationDataset
-bash: import: command not found
-bash: syntax error near unexpected token `0,'
-bash: from: command not found
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ cd ~/ColossalChat
-bash: cd: /home/Competition2025/P02/P02U006/ColossalChat: No such file or directory
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ ls
applications  CHANGE_LOG.md  colossalai  CONTRIBUTING.md  docker  docs  examples  extensions  hostfile  LICENSE  logs  MANIFEST.in  pytest.ini  README.md  requirements  run_lora.sh  setup.py  tests  version.txt
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ cd applications/
(deepseeksft310) [P02U006@osk-cpu01 applications]$ cd Colossal
-bash: cd: Colossal: No such file or directory
(deepseeksft310) [P02U006@osk-cpu01 applications]$ cd ColossalChat/
(deepseeksft310) [P02U006@osk-cpu01 ColossalChat]$ export PYTHONPATH=~/ColossalAI:$PYTHONPATH
(deepseeksft310) [P02U006@osk-cpu01 ColossalChat]$ python - << 'EOF'
import sys
print("=== sys.path の一覧 ===")
for p in sys.path:
    print(p)
EOF
=== sys.path の一覧 ===

/home/Competition2025/P02/P02U006/ColossalAI
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python310.zip
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/lib-dynload
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages
(deepseeksft310) [P02U006@osk-cpu01 ColossalChat]$ cd ..
(deepseeksft310) [P02U006@osk-cpu01 applications]$ cd ..
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ ls
applications  CHANGE_LOG.md  colossalai  CONTRIBUTING.md  docker  docs  examples  extensions  hostfile  LICENSE  logs  MANIFEST.in  pytest.ini  README.md  requirements  run_lora.sh  setup.py  tests  version.txt
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import coati.dataset.loader
print(">>> coati.dataset.loader の実体ファイル:")
print(coati.dataset.loader.__file__)
EOF
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'coati'
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ export PYTHONPATH=~/ColossalAI/applications/ColossalChat:$PYTHONPATH
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import sys
print("=== sys.path の一覧 ===")
for p in sys.path:
    print(p)
EOF
=== sys.path の一覧 ===

/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat
/home/Competition2025/P02/P02U006/ColossalAI
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python310.zip
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/lib-dynload
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import coati.dataset.loader
print(">>> coati.dataset.loader の実体ファイル:", coati.dataset.loader.__file__)
EOF
please install Colossal-AI from https://www.colossalai.org/download or from source
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/__init__.py", line 2, in <module>
    from .loader import (
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/loader.py", line 15, in <module>
    from datasets import Dataset as HFDataset
ModuleNotFoundError: No module named 'datasets'
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ pip install datasets==2.18.0
Collecting datasets==2.18.0
  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)
Requirement already satisfied: filelock in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (3.13.1)
Requirement already satisfied: numpy>=1.17 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (1.26.4)
Collecting pyarrow>=12.0.0 (from datasets==2.18.0)
  Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting pyarrow-hotfix (from datasets==2.18.0)
  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)
Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0)
  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)
Requirement already satisfied: pandas in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (2.2.0)
Requirement already satisfied: requests>=2.19.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (2.32.4)
Requirement already satisfied: tqdm>=4.62.1 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (4.67.1)
Collecting xxhash (from datasets==2.18.0)
  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting multiprocess (from datasets==2.18.0)
  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)
Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)
  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)
Collecting aiohttp (from datasets==2.18.0)
  Using cached aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)
Requirement already satisfied: huggingface-hub>=0.19.4 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (0.33.4)
Requirement already satisfied: packaging in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (25.0)
Requirement already satisfied: pyyaml>=5.1 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from datasets==2.18.0) (6.0.2)
Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.18.0)
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.4.0 (from aiohttp->datasets==2.18.0)
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)
Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.18.0)
  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)
Requirement already satisfied: attrs>=17.3.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (25.3.0)
Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.18.0)
  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.18.0)
  Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)
Collecting propcache>=0.2.0 (from aiohttp->datasets==2.18.0)
  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.18.0)
  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)
Requirement already satisfied: typing-extensions>=4.1.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->datasets==2.18.0) (4.12.2)
Requirement already satisfied: idna>=2.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0) (3.10)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.1.5)
Requirement already satisfied: charset_normalizer<4,>=2 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2025.7.14)
INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.
Collecting multiprocess (from datasets==2.18.0)
  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)
  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2025.2)
Requirement already satisfied: six>=1.5 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)
Using cached datasets-2.18.0-py3-none-any.whl (510 kB)
Using cached dill-0.3.8-py3-none-any.whl (116 kB)
Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)
Using cached aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)
Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)
Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)
Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
++ date
+ echo '===== ジョブ開始: Wed Jul 23 06:38:03 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 283066'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310

EnvironmentNameNotFound: Could not find conda environment: deepseek310
You can list all discoverable environments with `conda info --envs`.


+ ask_conda=
+ return
...skipping...
Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)
Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)
Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)
Using cached pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)
Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)
Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)
Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets
  Attempting uninstall: fsspec
    Found existing installation: fsspec 2024.6.1
    Uninstalling fsspec-2024.6.1:
      Successfully uninstalled fsspec-2024.6.1
Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 datasets-2.18.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2024.2.0 multidict-6.6.3 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 pyarrow-hotfix-0.7 xxhash-3.5.0 yarl-1.20.1
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ python - << 'EOF'
import coati.dataset.loader
print(">>> coati.dataset.loader の実体ファイル:", coati.dataset.loader.__file__)
EOF
please install Colossal-AI from https://www.colossalai.org/download or from source
>>> coati.dataset.loader の実体ファイル: /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/coati/dataset/loader.py
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ ls
applications  CHANGE_LOG.md  colossalai  CONTRIBUTING.md  docker  docs  examples  extensions  hostfile  LICENSE  logs  MANIFEST.in  pytest.ini  README.md  requirements  run_lora.sh  setup.py  tests  version.txt
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            282938       P02 domain_a kan.hata  R    1:57:39      1 osk-gpu56
            282933       P02 domain_a kan.hata  R    1:57:44      1 osk-gpu54
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 282938
  Waiting for administrator response...
### 2025-07-23 18:37:44  scancel 282938  (partition: P02)

✓ Done. Request for canceling job 282938 has been processed.
★ Cancel request queued for JobID: 282933
  Waiting for administrator response...
### 2025-07-23 18:37:45  scancel 282933  (partition: P02)

✓ Done. Request for canceling job 282933 has been processed.
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 283066
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseeksft310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  lora-r1-282858.err  lora-r1-282913.err  lora-r1-282931.err  lora-r1-283066.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  lora-r1-282858.out  lora-r1-282913.out  lora-r1-282931.out  lora-r1-283066.out  test-279249.out
(deepseeksft310) [P02U006@osk-cpu01 logs]$ sacct -j 283066 -o JobID,JobName%20,State,ExitCode,Elapsed,NodeLis
JobID                     JobName      State ExitCode    Elapsed        NodeList
------------ -------------------- ---------- -------- ---------- ---------------
283066                    lora-r1     FAILED      1:0   00:00:02  osk-gpu[54-56]
283066.batch                batch     FAILED      1:0   00:00:02       osk-gpu54
283066.exte+               extern  COMPLETED      0:0   00:00:02  osk-gpu[54-56]
(deepseeksft310) [P02U006@osk-cpu01 logs]$ less lora-r1-283066.err
(deepseeksft310) [P02U006@osk-cpu01 logs]$
(deepseeksft310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
++ date
+ echo '===== ジョブ開始: Wed Jul 23 06:42:50 PM JST 2025 ====='
++ pwd
+ echo 'cwd  = /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'host = osk-gpu54'
+ echo 'JOB  = 283074'
+ echo 'NODES= osk-gpu[54-56]'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseeksft310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseeksft310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseeksft310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseeksft310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseeksft310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseeksft310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseeksft310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseeksft310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseeksft310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseeksft310) '
++ CONDA_PROMPT_MODIFIER='(deepseeksft310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ export MASTER_PORT=15074
+ MASTER_PORT=15074
+ srun colossalai run --master_port 15074 --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competiti
on2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --pp 3 --ep 8 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_g
rad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
W0723 18:43:00.424000 22751453615168 torch/distributed/run.py:779]
W0723 18:43:00.424000 22751453615168 torch/distributed/run.py:779] *****************************************
W0723 18:43:00.424000 22751453615168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 18:43:00.424000 22751453615168 torch/distributed/run.py:779] *****************************************
W0723 18:43:00.424000 22572569707584 torch/distributed/run.py:779]
W0723 18:43:00.424000 22572569707584 torch/distributed/run.py:779] *****************************************
W0723 18:43:00.424000 22572569707584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 18:43:00.424000 22572569707584 torch/distributed/run.py:779] *****************************************
W0723 18:43:00.424000 22420370302016 torch/distributed/run.py:779]
W0723 18:43:00.424000 22420370302016 torch/distributed/run.py:779] *****************************************
W0723 18:43:00.424000 22420370302016 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as ne
eded.
W0723 18:43:00.424000 22420370302016 torch/distributed/run.py:779] *****************************************
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            283070       P02 domain_a kan.hata  R       1:31      1 osk-gpu56
            283067       P02 domain_a kan.hata  R       1:36      1 osk-gpu54
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
★ Cancel request queued for JobID: 283070
  Waiting for administrator response...
✓ Done. Request for canceling job 283070 has been processed.
★ Cancel request queued for JobID: 283067
  Waiting for administrator response...
### 2025-07-23 18:40:48  scancel 283067  (partition: P02)

✓ Done. Request for canceling job 283067 has been processed.
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 283072
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ sacct -j 283072 -o JobID,JobName%20,State,ExitCode,Elapsed,NodeLis
JobID                     JobName      State ExitCode    Elapsed        NodeList
------------ -------------------- ---------- -------- ---------- ---------------
283072                    lora-r1     FAILED      1:0   00:00:01  osk-gpu[54-56]
283072.batch                batch     FAILED      1:0   00:00:01       osk-gpu54
283072.exte+               extern  COMPLETED      0:0   00:00:01  osk-gpu[54-56]
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseeksft310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.err  lora-r1-280021.err  lora-r1-280156.err  lora-r1-280187.err  lora-r1-280209.err  lora-r1-282845.err  lora-r1-282858.err  lora-r1-282913.err  lora-r1-282931.err  lora-r1-283066.err  lora-r1-283072.err  test-279249.err
lora-r1-279251.out  lora-r1-279270.out  lora-r1-280021.out  lora-r1-280156.out  lora-r1-280187.out  lora-r1-280209.out  lora-r1-282845.out  lora-r1-282858.out  lora-r1-282913.out  lora-r1-282931.out  lora-r1-283066.out  lora-r1-283072.out  test-279249.out
(deepseeksft310) [P02U006@osk-cpu01 logs]$ less lora-r1-283072.err
(deepseeksft310) [P02U006@osk-cpu01 logs]$
(deepseeksft310) [P02U006@osk-cpu01 logs]$ cd ..
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ nano run_lora.sh
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue -p P02 -u kan.hatakeyama -h -o "%i"   | xargs -r -n1 /home/Competition2025/P02/shareP02/scripts/scancel.sh
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ sbatch run_lora.sh
Submitted batch job 283074
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            283074       P02  lora-r1  P02U006  R       0:03      3 osk-gpu[54-56]
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            283074       P02  lora-r1  P02U006  R       0:08      3 osk-gpu[54-56]
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            283075       P02 domain_a kan.hata  R       2:15      1 osk-gpu54
(deepseeksft310) [P02U006@osk-cpu01 ColossalAI]$ cd logs/
(deepseeksft310) [P02U006@osk-cpu01 logs]$ sacct -j 283074 -o JobID,JobName%20,State,ExitCode,Elapsed,NodeLis
JobID                     JobName      State ExitCode    Elapsed        NodeList
------------ -------------------- ---------- -------- ---------- ---------------
283074                    lora-r1     FAILED      1:0   00:02:25  osk-gpu[54-56]
283074.batch                batch     FAILED      1:0   00:02:25       osk-gpu54
283074.exte+               extern  COMPLETED      0:0   00:02:25  osk-gpu[54-56]
283074.0               colossalai     FAILED      1:0   00:02:25  osk-gpu[54-56]
(deepseeksft310) [P02U006@osk-cpu01 logs]$ ls
lora-r1-279251.err  lora-r1-279270.out  lora-r1-280156.err  lora-r1-280187.out  lora-r1-282845.err  lora-r1-282858.out  lora-r1-282931.err  lora-r1-283066.out  lora-r1-283074.err  test-279249.out
lora-r1-279251.out  lora-r1-280021.err  lora-r1-280156.out  lora-r1-280209.err  lora-r1-282845.out  lora-r1-282913.err  lora-r1-282931.out  lora-r1-283072.err  lora-r1-283074.out
lora-r1-279270.err  lora-r1-280021.out  lora-r1-280187.err  lora-r1-280209.out  lora-r1-282858.err  lora-r1-282913.out  lora-r1-283066.err  lora-r1-283072.out  test-279249.err
(deepseeksft310) [P02U006@osk-cpu01 logs]$ less lora-r1-283074.err
(deepseeksft310) [P02U006@osk-cpu01 logs]$
(deepseeksft310) [P02U006@osk-cpu01 logs]$ tensorboard==2.20.0
(deepseeksft310) [P02U006@osk-cpu01 logs]$ pip install tensorboard==2.20.0
Collecting tensorboard==2.20.0
  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)
Collecting absl-py>=0.4 (from tensorboard==2.20.0)
  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)
Collecting grpcio>=1.48.2 (from tensorboard==2.20.0)
  Downloading grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting markdown>=2.6.8 (from tensorboard==2.20.0)
  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)
Requirement already satisfied: numpy>=1.12.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from tensorboard==2.20.0) (1.26.4)
Requirement already satisfied: packaging in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from tensorboard==2.20.0) (25.0)
Requirement already satisfied: pillow in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from tensorboard==2.20.0) (11.0.0)
Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from tensorboard==2.20.0) (6.31.1)
Requirement already satisfied: setuptools>=41.0.0 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from tensorboard==2.20.0) (78.1.1)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.20.0)
  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)
Collecting werkzeug>=1.0.1 (from tensorboard==2.20.0)
  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)
Requirement already satisfied: MarkupSafe>=2.1.1 in /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.20.0) (2.1.5)
Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 43.3 MB/s eta 0:00:00
Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 225.6 MB/s eta 0:00:00
Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)
Downloading grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 136.1 MB/s eta 0:00:00
Downloading markdown-3.8.2-py3-none-any.whl (106 kB)
Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)
Installing collected packages: werkzeug, tensorboard-data-server, markdown, grpcio, absl-py, tensorboard
Successfully installed absl-py-2.3.1 grpcio-1.73.1 markdown-3.8.2 tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3
(deepseeksft310) [P02U006@osk-cpu01 logs]$ pip freeze
absl-py==2.3.1
accelerate==1.9.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.14
aiosignal==1.4.0
annotated-types==0.7.0
anyio==4.9.0
async-timeout==5.0.1
attrs==25.3.0
bcrypt==4.3.0
beautifulsoup4==4.13.4
bitsandbytes==0.46.1
certifi==2025.7.14
cffi==1.17.1
cfgv==3.4.0
charset-normalizer==3.4.2
click==8.2.1
colossalai==0.4.7
contexttimer==0.3.3
cryptography==45.0.5
datasets==2.18.0
decorator==5.2.1
Deprecated==1.2.18
diffusers==0.29.0
dill==0.3.8
distlib==0.4.0
einops==0.8.1
exceptiongroup==1.3.0
fabric==3.2.2
fastapi==0.116.1
filelock==3.13.1
frozenlist==1.7.0
fsspec==2024.2.0
galore-torch==1.0
google==3.0.0
grpcio==1.73.1
h11==0.16.0
hf-xet==1.1.5
huggingface-hub==0.33.4
identify==2.6.12
idna==3.10
importlib_metadata==8.7.0
invoke==2.2.0
Jinja2==3.1.4
jsonlines==4.0.0
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
Markdown==3.8.2
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
mpmath==1.3.0
msgpack==1.1.1
multidict==6.6.3
multiprocess==0.70.16
networkx==3.3
ninja==1.11.1.4
nodeenv==1.9.1
numpy==1.26.4
nvidia-cublas-cu12==12.4.2.65
nvidia-cuda-cupti-cu12==12.4.99
nvidia-cuda-nvrtc-cu12==12.4.99
nvidia-cuda-runtime-cu12==12.4.99
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.0.44
nvidia-curand-cu12==10.3.5.119
nvidia-cusolver-cu12==11.6.0.99
nvidia-cusparse-cu12==12.3.0.142
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.4.99
nvidia-nvtx-cu12==12.4.99
packaging==25.0
pandas==2.2.0
paramiko==3.5.1
peft==0.13.2
pillow==11.0.0
platformdirs==4.3.8
plumbum==1.9.0
pre_commit==4.2.0
prompt_toolkit==3.0.50
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
pyarrow==21.0.0
pyarrow-hotfix==0.7
pycparser==2.22
pydantic==2.11.7
pydantic_core==2.33.2
Pygments==2.19.2
PyNaCl==1.5.0
python-dateutil==2.9.0.post0
pytz==2025.2
PyYAML==6.0.2
ray==2.48.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rich==14.0.0
rpds-py==0.26.0
rpyc==6.0.0
safetensors==0.5.3
sentencepiece==0.2.0
six==1.17.0
sniffio==1.3.1
soupsieve==2.7
starlette==0.47.2
sympy==1.13.3
tensorboard==2.20.0
tensorboard-data-server==0.7.2
tokenizers==0.15.2
torch==2.4.1+cu124
torchaudio==2.4.1+cu124
torchvision==0.19.1+cu124
tqdm==4.67.1
transformers==4.39.3
triton==3.0.0
typing-inspection==0.4.1
typing_extensions==4.12.2
tzdata==2025.2
urllib3==2.5.0
uvicorn==0.29.0
virtualenv==20.32.0
wcwidth==0.2.13
Werkzeug==3.1.3
wrapt==1.17.2
xxhash==3.5.0
yarl==1.20.1
zipp==3.23.0
(deepseeksft310) [P02U006@osk-cpu01 logs]$ ^C
(deepseeksft310) [P02U006@osk-cpu01 logs]$
logout
Connection to 10.255.255.101 closed.
hongohayato@hongousatsuhitononotobukkukonpyuta ~ %