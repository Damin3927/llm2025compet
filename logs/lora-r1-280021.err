++ date
+ echo '===== ジョブ開始: Tue Jul 22 12:17:42 PM JST 2025 ====='
++ pwd
+ echo '現在ディレクトリ: /home/Competition2025/P02/P02U006/ColossalAI'
++ hostname
+ echo 'ホスト名: osk-gpu54'
+ echo 'SLURM_JOB_ID: 280021'
+ echo 'SLURM_NODELIST: osk-gpu[54-56]'
+ echo 'PATH: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'
+ echo 'CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7'
+ mkdir -p logs
+ echo '[INFO] conda activate deepseek310'
+ source /home/Competition2025/P02/P02U006/miniconda3/etc/profile.d/conda.sh
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ '[' -z x ']'
+ conda activate deepseek310
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate deepseek310
+ '[' -n '' ']'
+ local ask_conda
++ PS1=
++ __conda_exe shell.posix activate deepseek310
++ '[' -n '' ']'
++ /home/Competition2025/P02/P02U006/miniconda3/bin/conda shell.posix activate deepseek310
+ ask_conda='unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
+ eval 'unset _CE_M
unset _CE_CONDA
PS1='\''(deepseek310) '\''
export PATH='\''/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin'\''
export CONDA_SHLVL='\''1'\''
export CONDA_PROMPT_MODIFIER='\''(deepseek310) '\''
export CONDA_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/conda'\''
export CONDA_PYTHON_EXE='\''/home/Competition2025/P02/P02U006/miniconda3/bin/python'\'''
++ unset _CE_M
++ unset _CE_CONDA
++ PS1='(deepseek310) '
++ export PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export CONDA_SHLVL=1
++ CONDA_SHLVL=1
++ export 'CONDA_PROMPT_MODIFIER=(deepseek310) '
++ CONDA_PROMPT_MODIFIER='(deepseek310) '
++ export CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ CONDA_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/conda
++ export CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/home/Competition2025/P02/P02U006/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ echo '[INFO] ホストファイル作成'
+ scontrol show hostname 'osk-gpu[54-56]'
+ cat hostfile
++ date
+ echo '[INFO] colossalai run 実行開始: Tue Jul 22 12:17:44 PM JST 2025'
+ srun colossalai run --hostfile ./hostfile --nproc_per_node 8 /home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py --pretrained /home/Competition2025/P02/shareP02/DeepSeek-R1-0528-BF16 --dataset /home/Competition2025/P02/shareP02/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl --plugin gemini --ep 8 --pp 3 --batch_size 24 --lr 2e-5 --max_length 256 --lora_rank 8 --lora_alpha 16 --num_epochs 2 --warmup_steps 100 --mixed_precision bf16 --use_grad_checkpoint --tensorboard_dir logs/tb --save_dir DeepSeek-R1-0528-lora
Exception (client): Error reading SSH protocol banner
Exception (client): Error reading SSH protocol banner
Exception (client): Error reading SSH protocol banner
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Exception (client): Error reading SSH protocol banner
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
Exception (client): Error reading SSH protocol banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Exception (client): Error reading SSH protocol banner
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:
Exception (client): Error reading SSH protocol banner

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

Exception (client): Error reading SSH protocol banner
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2369, in _check_banner
    buf = self.packetizer.readline(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 395, in readline
    buf += self._read_timeout(timeout)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/packet.py", line 665, in _read_timeout
    raise EOFError()
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2185, in run
    self._check_banner()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/paramiko/transport.py", line 2373, in _check_banner
    raise SSHException(
paramiko.ssh_exception.SSHException: Error reading SSH protocol banner

W0722 12:17:56.124000 23038184764480 torch/distributed/run.py:779] 
W0722 12:17:56.124000 23038184764480 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.124000 23038184764480 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.124000 23038184764480 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.124000 22963770922048 torch/distributed/run.py:779] 
W0722 12:17:56.124000 22963770922048 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.124000 22963770922048 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.124000 22963770922048 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.125000 22660930675776 torch/distributed/run.py:779] 
W0722 12:17:56.125000 22660930675776 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.125000 22660930675776 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.125000 22660930675776 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.128000 22962148394048 torch/distributed/run.py:779] 
W0722 12:17:56.128000 22962148394048 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.128000 22962148394048 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.128000 22962148394048 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = f(*args, **kwargs)
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = self._invoke_run(role)
    result = f(*args, **kwargs)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._rendezvous(worker_group)
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    self._rendezvous(worker_group)
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
W0722 12:17:56.146000 22552830207040 torch/distributed/run.py:779] 
W0722 12:17:56.146000 22552830207040 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.146000 22552830207040 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.146000 22552830207040 torch/distributed/run.py:779] *****************************************
    self._store = TCPStore(  # type: ignore[call-arg]
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
W0722 12:17:56.161000 22707869865024 torch/distributed/run.py:779] 
W0722 12:17:56.161000 22707869865024 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.161000 22707869865024 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.161000 22707869865024 torch/distributed/run.py:779] *****************************************
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0722 12:17:56.164000 22944439788608 torch/distributed/run.py:779] 
W0722 12:17:56.164000 22944439788608 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.164000 22944439788608 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.164000 22944439788608 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
W0722 12:17:56.172000 22388081161280 torch/distributed/run.py:779] 
W0722 12:17:56.172000 22388081161280 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.172000 22388081161280 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.172000 22388081161280 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    self._rendezvous(worker_group)
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    result = agent.run()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 829, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 652, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 489, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 66, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
W0722 12:17:56.333000 23344739906624 torch/distributed/run.py:779] 
W0722 12:17:56.333000 23344739906624 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 23344739906624 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 23344739906624 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22784359621696 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22784359621696 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22784359621696 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22784359621696 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22554816861248 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22554816861248 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22554816861248 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22554816861248 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22469972415552 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22469972415552 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22469972415552 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22469972415552 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22366244774976 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22366244774976 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22366244774976 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22366244774976 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22665468486720 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22665468486720 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22665468486720 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22665468486720 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22663568098368 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22663568098368 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22663568098368 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22663568098368 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22512032441408 torch/distributed/run.py:779] 
W0722 12:17:56.333000 22512032441408 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.333000 22512032441408 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.333000 22512032441408 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22915057013824 torch/distributed/run.py:779] 
W0722 12:17:56.395000 22915057013824 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22915057013824 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 22915057013824 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23413488972864 torch/distributed/run.py:779] 
W0722 12:17:56.395000 23413488972864 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23413488972864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 23413488972864 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23113340171328 torch/distributed/run.py:779] 
W0722 12:17:56.395000 23113340171328 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23113340171328 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 23113340171328 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22775914812480 torch/distributed/run.py:779] 
W0722 12:17:56.395000 22775914812480 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22775914812480 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 22775914812480 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22405557474368 torch/distributed/run.py:779] 
W0722 12:17:56.395000 22405557474368 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22405557474368 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 22405557474368 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23424477058112 torch/distributed/run.py:779] 
W0722 12:17:56.395000 23424477058112 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 23424477058112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 23424477058112 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22562961622080 torch/distributed/run.py:779] 
W0722 12:17:56.395000 22562961622080 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22562961622080 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 22562961622080 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22959893472320 torch/distributed/run.py:779] 
W0722 12:17:56.395000 22959893472320 torch/distributed/run.py:779] *****************************************
W0722 12:17:56.395000 22959893472320 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 12:17:56.395000 22959893472320 torch/distributed/run.py:779] *****************************************
srun: error: osk-gpu54: tasks 1-2,6-7: Exited with exit code 1
srun: error: osk-gpu54: tasks 3-4: Exited with exit code 1
srun: error: osk-gpu54: task 5: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
W0722 12:17:59.586000 22963770922048 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3047039 closing signal SIGTERM
W0722 12:17:59.586000 22963770922048 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3047043 closing signal SIGTERM
E0722 12:17:59.618000 22963770922048 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 3047037) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-07-22_12:17:59
  host      : osk-gpu54
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3047038)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-07-22_12:17:59
  host      : osk-gpu54
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3047040)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-07-22_12:17:59
  host      : osk-gpu54
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 3047041)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-07-22_12:17:59
  host      : osk-gpu54
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 3047042)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:17:59
  host      : osk-gpu54
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3047037)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu54: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
W0722 12:18:10.583000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890543 closing signal SIGTERM
W0722 12:18:10.583000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890566 closing signal SIGTERM
W0722 12:18:10.584000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890569 closing signal SIGTERM
W0722 12:18:10.584000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890581 closing signal SIGTERM
W0722 12:18:10.584000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890584 closing signal SIGTERM
W0722 12:18:10.584000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890591 closing signal SIGTERM
W0722 12:18:10.584000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890600 closing signal SIGTERM
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
W0722 12:18:10.587000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915012 closing signal SIGTERM
W0722 12:18:10.587000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915018 closing signal SIGTERM
W0722 12:18:10.587000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915024 closing signal SIGTERM
W0722 12:18:10.587000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915030 closing signal SIGTERM
W0722 12:18:10.587000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915031 closing signal SIGTERM
W0722 12:18:10.588000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915036 closing signal SIGTERM
W0722 12:18:10.588000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915039 closing signal SIGTERM
W0722 12:18:10.633000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890542 closing signal SIGTERM
W0722 12:18:10.633000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890549 closing signal SIGTERM
W0722 12:18:10.633000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890551 closing signal SIGTERM
W0722 12:18:10.633000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890568 closing signal SIGTERM
W0722 12:18:10.634000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890578 closing signal SIGTERM
W0722 12:18:10.634000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890582 closing signal SIGTERM
E0722 12:18:10.660000 22562961622080 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2890541) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
E0722 12:18:10.703000 23413488972864 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2890540) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
E0722 12:18:10.710000 22784359621696 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2915001) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu56
  rank      : 16 (local_rank: 0)
  exitcode  : 1 (pid: 2890541)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915014 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915028 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915033 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915037 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915045 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915052 closing signal SIGTERM
W0722 12:18:10.791000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915059 closing signal SIGTERM
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915007 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915015 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915023 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915035 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915043 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915047 closing signal SIGTERM
W0722 12:18:10.809000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915048 closing signal SIGTERM
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu56
  rank      : 18 (local_rank: 2)
  exitcode  : 1 (pid: 2890544)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu56
  rank      : 16 (local_rank: 0)
  exitcode  : 1 (pid: 2890540)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu55
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 2915001)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0722 12:18:10.886000 22665468486720 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2915006) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
E0722 12:18:10.893000 22469972415552 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2915020) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
W0722 12:18:10.950000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915002 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915009 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915034 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915041 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915051 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915056 closing signal SIGTERM
W0722 12:18:10.951000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915060 closing signal SIGTERM
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu55
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 2915006)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu55
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 2915020)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0722 12:18:11.083000 23344739906624 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2915025) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:10
  host      : osk-gpu55
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 2915025)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915011 closing signal SIGTERM
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915026 closing signal SIGTERM
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915032 closing signal SIGTERM
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915050 closing signal SIGTERM
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915055 closing signal SIGTERM
W0722 12:18:11.232000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915058 closing signal SIGTERM
W0722 12:18:11.233000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915062 closing signal SIGTERM
W0722 12:18:11.287000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890546 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890573 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890576 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890579 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890586 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890594 closing signal SIGTERM
W0722 12:18:11.288000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890597 closing signal SIGTERM
E0722 12:18:11.361000 22366244774976 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2915005) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu55
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 2915005)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0722 12:18:11.417000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890555 closing signal SIGTERM
W0722 12:18:11.417000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890560 closing signal SIGTERM
W0722 12:18:11.418000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890562 closing signal SIGTERM
W0722 12:18:11.418000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890565 closing signal SIGTERM
W0722 12:18:11.418000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890575 closing signal SIGTERM
W0722 12:18:11.418000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890592 closing signal SIGTERM
W0722 12:18:11.418000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890598 closing signal SIGTERM
E0722 12:18:11.473000 22405557474368 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 2890559) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915008 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915016 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915021 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915042 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915057 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915063 closing signal SIGTERM
W0722 12:18:11.538000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915064 closing signal SIGTERM
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu56
  rank      : 17 (local_rank: 1)
  exitcode  : 1 (pid: 2890559)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0722 12:18:11.550000 22775914812480 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2890553) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
W0722 12:18:11.555000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890545 closing signal SIGTERM
W0722 12:18:11.555000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890554 closing signal SIGTERM
W0722 12:18:11.555000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890556 closing signal SIGTERM
W0722 12:18:11.556000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890558 closing signal SIGTERM
W0722 12:18:11.556000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890572 closing signal SIGTERM
W0722 12:18:11.556000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890587 closing signal SIGTERM
W0722 12:18:11.556000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890603 closing signal SIGTERM
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu56
  rank      : 16 (local_rank: 0)
  exitcode  : 1 (pid: 2890553)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915003 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915010 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915017 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915044 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915046 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915053 closing signal SIGTERM
W0722 12:18:11.619000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915054 closing signal SIGTERM
E0722 12:18:11.663000 22915057013824 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 4 (pid: 2890570) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
E0722 12:18:11.672000 22554816861248 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 3 (pid: 2915027) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu55
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 2915027)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890552 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890561 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890563 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890571 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890577 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890589 closing signal SIGTERM
W0722 12:18:11.688000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890602 closing signal SIGTERM
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu56
  rank      : 20 (local_rank: 4)
  exitcode  : 1 (pid: 2890570)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0722 12:18:11.754000 22663568098368 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 5 (pid: 2915049) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu55
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 2915049)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu55: tasks 14-15: Exited with exit code 1
srun: error: osk-gpu56: tasks 17,23: Exited with exit code 1
E0722 12:18:11.846000 22959893472320 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 6 (pid: 2890601) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:11
  host      : osk-gpu56
  rank      : 22 (local_rank: 6)
  exitcode  : 1 (pid: 2890601)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
srun: error: osk-gpu55: task 10: Exited with exit code 1
srun: error: osk-gpu55: task 11: Exited with exit code 1
srun: error: osk-gpu55: task 12: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
srun: error: osk-gpu56: task 18: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
srun: error: osk-gpu55: tasks 9,13: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
srun: error: osk-gpu56: tasks 19,21-22: Exited with exit code 1
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
W0722 12:18:12.715000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915004 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915013 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915019 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915022 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915029 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915040 closing signal SIGTERM
W0722 12:18:12.716000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2915061 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
E0722 12:18:12.780000 22512032441408 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 5 (pid: 2915038) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:12
  host      : osk-gpu55
  rank      : 13 (local_rank: 5)
  exitcode  : 1 (pid: 2915038)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890548 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890550 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890567 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890585 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890588 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890593 closing signal SIGTERM
W0722 12:18:13.052000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890596 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 16, in <module>
    from coati.dataset.loader import RawConversationDataset
ModuleNotFoundError: No module named 'coati'
E0722 12:18:13.127000 23424477058112 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2890557) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:13
  host      : osk-gpu56
  rank      : 18 (local_rank: 2)
  exitcode  : 1 (pid: 2890557)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0722 12:18:13.154000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890547 closing signal SIGTERM
W0722 12:18:13.154000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890574 closing signal SIGTERM
W0722 12:18:13.155000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890580 closing signal SIGTERM
W0722 12:18:13.155000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890583 closing signal SIGTERM
W0722 12:18:13.155000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890590 closing signal SIGTERM
W0722 12:18:13.155000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890595 closing signal SIGTERM
W0722 12:18:13.155000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2890599 closing signal SIGTERM
E0722 12:18:13.194000 23113340171328 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 2890564) of binary: /home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/python3.10
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/Competition2025/P02/P02U006/miniconda3/envs/deepseek310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-22_12:18:13
  host      : osk-gpu56
  rank      : 17 (local_rank: 1)
  exitcode  : 1 (pid: 2890564)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu55: task 8: Exited with exit code 1
srun: error: osk-gpu56: tasks 16,20: Exited with exit code 1
