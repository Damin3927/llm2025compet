===== ジョブ開始: Mon Jul 28 12:56:15 PM JST 2025 =====
cwd  = /home/Competition2025/P02/P02U006/ColossalAI
host = osk-gpu54
JOB  = 298498
NODES= osk-gpu[54,56,91]
FLASH_ATTENTION_DISABLE=1
HF_TRANSFORMERS_CACHE_DISABLE_FLASH_ATTN_2=1
=== CUDA環境 ===
CUDA_HOME=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Tue_May_27_02:21:03_PDT_2025
Cuda compilation tools, release 12.9, V12.9.86
Build cuda_12.9.r12.9/compiler.36037853_0
=== Pythonライブラリのバージョン ===
python 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]
torch 2.4.1+cu124
torchvision 0.19.1+cu124
torchaudio 2.4.1+cu124
numpy 1.26.4
please install Colossal-AI from https://www.colossalai.org/download or from source
colossalai 0.0.0
transformers 4.46.3
PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin:/home/Competition2025/P02/P02U006/miniconda3/condabin:/home/Competition2025/P02/P02U006/.local/bin:/home/Competition2025/P02/P02U006/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
LD_LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:
CPATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/include:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/nvidia/cublas/include:
LIBRARY_PATH=/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib64:/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/targets/x86_64-linux/lib:
MASTER_ADDR=osk-gpu54  MASTER_PORT=30498
on master: osk-gpu54
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/colossalai
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/torchrun
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from sourceplease install Colossal-AI from https://www.colossalai.org/download or from sourceplease install Colossal-AI from https://www.colossalai.org/download or from source


please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
dataset size: 2160
dataloader batch_size: 8, total batches: 33
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
dataset size: 2160
dataloader batch_size: 8, total batches: 33
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=True  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
dataset size: 2160
dataloader batch_size: 8, total batches: 33
==== ColossalAI SFT script: train() Start ====
[DEBUG] Creating tensorboard dir: logs/tb
[DEBUG] Tensorboard SummaryWriter created
[DEBUG] Saving config to training_config.json
[DEBUG] is_master=False  tensorboard_dir=logs/tb
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
==== ColossalAI SFT script: train() Start ====
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
dataset size: 2160
dataloader batch_size: 8, total batches: 33
[DEBUG] is_master=False  tensorboard_dir=logs/tb
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
please install Colossal-AI from https://www.colossalai.org/download or from source
dataset size: 2160
dataloader batch_size: 8, total batches: 33
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
dataset size: 2160
dataloader batch_size: 8, total batches: 33
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[07/28/25 12:57:13] INFO     colossalai - colossalai - INFO:                    
                             /home/Competition2025/P02/P02U006/ColossalAI/coloss
                             alai/initialize.py:75 launch                       
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 24         
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
[DEBUG] is_master=False  tensorboard_dir=logs/tb
dataset size: 2160
dataloader batch_size: 8, total batches: 33
Training Info:
Config file: training_config.json 
Tensorboard logs: logs/tb 
Model checkpoint: DeepSeek-R1-0528-lora
Load dataset: /home/Competition2025/P02/shareP02/hci_colossalai_deepseekr10528_lorasft.jsonl
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
Max device memory after data loader: 0.00 MB
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
dataset size: 2160
dataloader batch_size: 8, total batches: 33
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.036959171295166016 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 1.1528301239013672 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.04015970230102539 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 1.2204067707061768 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.0050573348999023 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.09376645088195801 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.5060985088348389 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.6051998138427734 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.08502793312072754 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.2061097621917725 seconds
[extension] Time taken to load fused_optim_cuda op: 0.10390686988830566 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.08440756797790527 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.9072434902191162 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.08362627029418945 seconds
[extension] Time taken to load cpu_adam_x86 op: 2.007190227508545 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.8062469959259033 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 1.005159854888916 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.08221268653869629 seconds
[extension] Time taken to load fused_optim_cuda op: 0.10355639457702637 seconds
[extension] Time taken to load fused_optim_cuda op: 0.10355448722839355 seconds
[extension] Time taken to load cpu_adam_x86 op: 2.4088099002838135 seconds
[extension] Time taken to load cpu_adam_x86 op: 2.4090113639831543 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.07975935935974121 seconds
[extension] Time taken to load fused_optim_cuda op: 0.10363650321960449 seconds
[extension] Time taken to load cpu_adam_x86 op: 0.1042778491973877 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.07123517990112305 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.027904748916625977 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.07792139053344727 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.030838966369628906 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.0813145637512207 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.028844118118286133 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.07236433029174805 seconds
[07/28/25 13:01:02] WARNING  colossalai - colossalai - WARNING:                 
                             /home/Competition2025/P02/P02U006/ColossalAI/coloss
                             alai/booster/plugin/hybrid_parallel_plugin.py:1518 
                             enable_lora                                        
                    WARNING  colossalai - colossalai - WARNING: You have enabled
                             LoRa training. Please check the hyperparameters    
                             such as lr                                         
Gradient checkpointing enabled successfully
Model params: 674.33 B
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.25262951850891113 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 2.1173713207244873 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.20528221130371094 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.5236175060272217 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.1901381015777588 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.4343569278717041 seconds
[extension] Time taken to load cpu_adam_x86 op: 0.4247004985809326 seconds
[extension] Time taken to load cpu_adam_x86 op: 0.12038826942443848 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.44913363456726074 seconds
[extension] Time taken to load fused_optim_cuda op: 0.5053365230560303 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.19717025756835938 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.4851951599121094 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.20615673065185547 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.4907417297363281 seconds
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.20212507247924805 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.48756980895996094 seconds
