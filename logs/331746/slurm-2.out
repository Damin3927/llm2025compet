please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
please install Colossal-AI from https://www.colossalai.org/download or from source
=== CONDA ENV CHECK ====== CONDA ENV CHECK ===

sys.executable      :sys.executable      :  /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10

CONDA_DEFAULT_ENV   :CONDA_DEFAULT_ENV   :  deepseeksft310deepseeksft310

CONDA_PREFIX        :CONDA_PREFIX        :  /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310

python version      :python version      :  3.10.183.10.18

torch version       :torch version       :  2.4.1+cu1242.4.1+cu124

=== END CONDA ENV CHECK ===
=== END CONDA ENV CHECK ===


=== ENV CHECK ====== ENV CHECK ===

NCCL_TIMEOUT =NCCL_TIMEOUT =  36003600

TORCH_ELASTIC_STORE_TIMEOUT =TORCH_ELASTIC_STORE_TIMEOUT =  36003600

TORCH_DISTRIBUTED_STORE_TIMEOUT =TORCH_DISTRIBUTED_STORE_TIMEOUT =  36003600

MASTER_ADDR =MASTER_ADDR =  osk-gpu54osk-gpu54

MASTER_PORT =MASTER_PORT =  2374623746

=== END ENV CHECK ====== END ENV CHECK ===

=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
MASTER_PORT = 23746
=== END ENV CHECK ===
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
MASTER_PORT = 23746
=== END ENV CHECK ===
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
==== ColossalAI SFT script: train() Start ====MASTER_PORT =
 23746
=== END ENV CHECK ===
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
MASTER_PORT = 23746
=== END ENV CHECK ===
==== ColossalAI SFT script: train() Start ====
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
MASTER_PORT = 23746
=== END ENV CHECK ===
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
=== CONDA ENV CHECK ===
sys.executable      : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/bin/python3.10
CONDA_DEFAULT_ENV   : deepseeksft310
CONDA_PREFIX        : /home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310
python version      : 3.10.18
torch version       : 2.4.1+cu124
=== END CONDA ENV CHECK ===

=== ENV CHECK ===
NCCL_TIMEOUT = 3600
TORCH_ELASTIC_STORE_TIMEOUT = 3600
TORCH_DISTRIBUTED_STORE_TIMEOUT = 3600
MASTER_ADDR = osk-gpu54
MASTER_PORT = 23746
=== END ENV CHECK ===
==== ColossalAI SFT script: train() Start ====
==== ColossalAI SFT script: train() Start ====
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=True  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] is_master=False  tensorboard_dir=/home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] Creating tensorboard dir: /home/Competition2025/P02/P02U006/ColossalAI/logs/331746/tb
[DEBUG] Tensorboard SummaryWriter created
[DEBUG] Saving config to training_config.json
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
dataset size: 2160
dataloader batch_size: 8, total batches: 33
=== [Debug] After dataloader, before model config ===
=== [Debug] Using attention implementation: eager ===
=== [Debug] AutoConfig loaded. model_type=deepseek_v3, architectures=['DeepseekV3ForCausalLM'] ===
=== [Debug] Entered init_ctx ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] Model created: DeepseekV3ForCausalLM ===
=== [Debug] Setting up LoRA ===
=== [Debug] LoRA enabled ===
=== [Debug] Set model to train mode ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] LoRA enabled ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] LoRA enabled ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to train mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Set model to train mode ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Gradient checkpointing enabled successfully ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
=== [Debug] Set model to eval mode ===
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.8308751583099365 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 0.7059924602508545 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 1.134322166442871 seconds
[extension] Time taken to load cpu_adam_x86 op: 1.021216869354248 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 1.86083984375 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load fused_optim_cuda op: 2.0115854740142822 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load fused_optim_cuda op: 2.0161845684051514 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 4.9101762771606445 seconds
[extension] Time taken to load cpu_adam_x86 op: 5.2253642082214355 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.2551608085632324 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load fused_optim_cuda op: 4.409825086593628 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 6.960387229919434 seconds
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.3050417900085449 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Time taken to load cpu_adam_x86 op: 5.944868326187134 seconds
[extension] Time taken to load fused_optim_cuda op: 0.6049246788024902 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
[extension] Loading the JIT-built fused_optim_cuda kernel during runtime now
[extension] Time taken to load fused_optim_cuda op: 0.5054671764373779 seconds
=== [Debug] Using optimizer: HybridAdam ===
=== [Debug] warmup_steps: 8 ===
=== [Debug] Using LR scheduler: CosineAnnealingWarmupLR ===
=== [Debug] Booster boost completed: rank=17 ===
=== [Debug] Booster boost completed: rank=23 ===
=== [Debug] Booster boost completed: rank=22 ===
=== [Debug] Booster boost completed: rank=18 ===
=== [Debug] Booster boost completed: rank=20 ===
=== [Debug] Booster boost completed: rank=19 ===
=== [Debug] Booster boost completed: rank=21 ===
=== [Debug] Booster boost completed: rank=16 ===
=== [Debug] Model loaded from pretrained: rank=16 ====== [Debug] Model loaded from pretrained: rank=22 ===

[Debug] rank=16, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20973.85 MB[Debug] rank=22, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20970.96 MB

[Debug] rank=16, host=osk-gpu91, === for epoch in range(0, 2) Start ====[Debug] rank=22, host=osk-gpu91, === for epoch in range(0, 2) Start ====

=== [Debug] Model loaded from pretrained: rank=23 ===
[Debug] rank=23, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20977.25 MB
[Debug] rank=23, host=osk-gpu91, === for epoch in range(0, 2) Start ====
=== [Debug] Model loaded from pretrained: rank=17 ===
[Debug] rank=17, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20973.48 MB
[Debug] rank=17, host=osk-gpu91, === for epoch in range(0, 2) Start ====
=== [Debug] Model loaded from pretrained: rank=20 ===
[Debug] rank=20, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20972.39 MB
[Debug] rank=20, host=osk-gpu91, === for epoch in range(0, 2) Start ====
=== [Debug] Model loaded from pretrained: rank=21 ===
[Debug] rank=21, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20970.38 MB
[Debug] rank=21, host=osk-gpu91, === for epoch in range(0, 2) Start ====
=== [Debug] Model loaded from pretrained: rank=18 ===
[Debug] rank=18, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20969.68 MB
[Debug] rank=18, host=osk-gpu91, === for epoch in range(0, 2) Start ====
=== [Debug] Model loaded from pretrained: rank=19 ===
[Debug] rank=19, host=osk-gpu91, Max device mem: 67123.78 MB, Max CPU mem: 20973.48 MB
[Debug] rank=19, host=osk-gpu91, === for epoch in range(0, 2) Start ====
