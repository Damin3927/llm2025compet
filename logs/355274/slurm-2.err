/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/utils/safetensors.py:13: UserWarning: Please install the latest tensornvme to use async save. pip install git+https://github.com/hpcaitech/TensorNVMe.git
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:48: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel")
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/layer/normalization.py:93: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMSNorm kernel
  warnings.warn(
[W813 02:40:45.917845331 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.961166280 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.984729211 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.014828100 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.052440948 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.054455425 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.064775725 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W813 02:40:45.078877898 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/shardformer/shard/shard_config.py:94: UserWarning: The sequence_parallelism_mode will be ignored when enable_sequence_parallelism is False
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.9) does not match with the version (12.4) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/home/Competition2025/P02/P02U006/miniconda3/envs/deepseeksft310/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:207: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, tokens, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:229: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, output_grad):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:242: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, expert_tokens, logits, mask, dest_idx, ec):
/home/Competition2025/P02/P02U006/ColossalAI/colossalai/moe/_operation.py:270: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, tokens_grad):
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank20]:     train(args)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank20]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank20]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank20]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank20]:     super().__init__(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank20]:     super().__init__(
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank20]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank20]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank20]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank20]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank21]: Traceback (most recent call last):
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank21]:     train(args)
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank21]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank21]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank21]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank21]:     super().__init__(
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank21]:     super().__init__(
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank21]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank21]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank21]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank21]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 5 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank16]:     train(args)
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank16]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank16]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank16]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank16]:     super().__init__(
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank16]:     super().__init__(
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank16]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank16]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank16]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank16]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank23]: Traceback (most recent call last):
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank23]:     train(args)
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank23]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank23]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank23]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank23]:     super().__init__(
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank23]:     super().__init__(
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank23]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank23]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank23]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank23]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 7 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x152b007fd000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x1499aae29000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x14e3ac419000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x14845954d000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank22]:     train(args)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank22]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank22]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank22]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank22]:     super().__init__(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank22]:     super().__init__(
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank22]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank22]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank22]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank22]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 6 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank20]:[W813 02:45:19.005112055 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank17]:     train(args)
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank17]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank17]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank17]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank17]:     super().__init__(
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank17]:     super().__init__(
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank17]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank17]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank17]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank17]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 1 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank21]:[W813 02:45:20.655654659 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank18]:     train(args)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank18]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank18]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank18]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank18]:     super().__init__(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank18]:     super().__init__(
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank18]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank18]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank18]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank18]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 743, in <module>
[rank19]:     train(args)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/applications/ColossalChat/examples/training_scripts/lora_finetune.py", line 508, in train
[rank19]:     model, optimizer, _, dataloader, lr_scheduler = booster.boost(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/booster.py", line 154, in boost
[rank19]:     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 501, in configure
[rank19]:     optimizer = MoeHybridParallelZeroOptimizer(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py", line 80, in __init__
[rank19]:     super().__init__(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/booster/plugin/hybrid_parallel_plugin.py", line 703, in __init__
[rank19]:     super().__init__(
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 208, in __init__
[rank19]:     master_param_current_rank = self._create_master_param_current_rank(group_params)
[rank19]:   File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 292, in _create_master_param_current_rank
[rank19]:     splited_param_current_rank = splited_params.detach().clone().float().to(device)
[rank19]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 3 has a total capacity of 79.44 GiB of which 31.62 MiB is free. Including non-PyTorch memory, this process has 79.40 GiB memory in use. Of the allocated memory 78.71 GiB is allocated by PyTorch, and 29.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank16]:[W813 02:45:22.466280539 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W813 02:45:23.549208716 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x1530472e1000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
Exception ignored in: <function LowLevelZeroOptimizer.__del__ at 0x151d902b5000>
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/ColossalAI/colossalai/zero/low_level/low_level_optim.py", line 242, in __del__
    for hook in self.grad_handles:
AttributeError: 'MoeHybridParallelZeroOptimizer' object has no attribute 'grad_handles'
