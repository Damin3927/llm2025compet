W0817 05:20:11.882000 476715 torch/distributed/run.py:784] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
W0817 05:20:11.883000 476715 torch/distributed/run.py:792] 
W0817 05:20:11.883000 476715 torch/distributed/run.py:792] *****************************************
W0817 05:20:11.883000 476715 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0817 05:20:11.883000 476715 torch/distributed/run.py:792] *****************************************
Found cached dataset hle_sft_open_thoughts-114k (/home/Competition2025/P02/P02U006/.cache/huggingface/datasets/neko-llm___hle_sft_open_thoughts-114k/default/0.0.0/95b7ec559aefd67116876c6fe0c205c412188605)
Loading cached processed dataset at /home/Competition2025/P02/P02U006/.cache/huggingface/datasets/neko-llm___hle_sft_open_thoughts-114k/default/0.0.0/95b7ec559aefd67116876c6fe0c205c412188605/cache-8fff6bb52ea27ace_*_of_00001.arrow
Loading cached shuffled indices for dataset at /home/Competition2025/P02/P02U006/.cache/huggingface/datasets/neko-llm___hle_sft_open_thoughts-114k/default/0.0.0/95b7ec559aefd67116876c6fe0c205c412188605/cache-1eeda50561e2fa07.arrow
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file vocab.json from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/vocab.json
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file merges.txt from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/merges.txt
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file tokenizer.json from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/tokenizer.json
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file tokenizer_config.json from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/tokenizer_config.json
[INFO|tokenization_utils_base.py:2011] 2025-08-17 05:20:40,081 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2280] 2025-08-17 05:20:40,386 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:752] 2025-08-17 05:20:40,595 >> loading configuration file config.json from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/config.json
[INFO|configuration_utils.py:817] 2025-08-17 05:20:40,596 >> Model config Qwen3MoeConfig {
  "architectures": [
    "Qwen3MoeForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "decoder_sparse_step": 1,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "max_position_embeddings": 40960,
  "max_window_layers": 48,
  "mlp_only_layers": [],
  "model_type": "qwen3_moe",
  "moe_intermediate_size": 768,
  "norm_topk_prob": true,
  "num_attention_heads": 32,
  "num_experts": 128,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 48,
  "num_key_value_heads": 4,
  "output_router_logits": false,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.001,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.54.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:1271] 2025-08-17 05:20:41,207 >> loading weights file model.safetensors from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/model.safetensors.index.json
[INFO|modeling_utils.py:2374] 2025-08-17 05:20:41,236 >> Instantiating Qwen3MoeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:4306] 2025-08-17 05:20:41,237 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1103] 2025-08-17 05:20:41,247 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:26,  5.78s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:26,  5.79s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:27,  5.80s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:28,  5.89s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:27,  5.84s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:28,  5.93s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:05<01:27,  5.83s/it]Loading checkpoint shards:   6%|▋         | 1/16 [00:06<01:32,  6.18s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:30,  6.48s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:30,  6.48s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:31,  6.53s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:30,  6.50s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:31,  6.51s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:31,  6.55s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:12<01:31,  6.51s/it]Loading checkpoint shards:  12%|█▎        | 2/16 [00:13<01:33,  6.66s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:26,  6.69s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:26,  6.69s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:27,  6.71s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:27,  6.69s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:27,  6.70s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:27,  6.70s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:19<01:27,  6.73s/it]Loading checkpoint shards:  19%|█▉        | 3/16 [00:20<01:28,  6.79s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.77s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.77s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.79s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.78s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.78s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.79s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.78s/it]Loading checkpoint shards:  25%|██▌       | 4/16 [00:26<01:21,  6.82s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.82s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.83s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.83s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.82s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.83s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.83s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.83s/it]Loading checkpoint shards:  31%|███▏      | 5/16 [00:33<01:15,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.86s/it]Loading checkpoint shards:  38%|███▊      | 6/16 [00:40<01:08,  6.88s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.89s/it]Loading checkpoint shards:  44%|████▍     | 7/16 [00:47<01:02,  6.90s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.91s/it]Loading checkpoint shards:  50%|█████     | 8/16 [00:54<00:55,  6.92s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  56%|█████▋    | 9/16 [01:01<00:48,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.89s/it]Loading checkpoint shards:  62%|██████▎   | 10/16 [01:08<00:41,  6.90s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.92s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.92s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.91s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.91s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.91s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.91s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.91s/it]Loading checkpoint shards:  69%|██████▉   | 11/16 [01:15<00:34,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.93s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  75%|███████▌  | 12/16 [01:22<00:27,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:28<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:28<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:29<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:28<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:29<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:28<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:28<00:20,  6.92s/it]Loading checkpoint shards:  81%|████████▏ | 13/16 [01:29<00:20,  6.93s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.92s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.92s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.92s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.92s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.91s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.91s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:35<00:13,  6.91s/it]Loading checkpoint shards:  88%|████████▊ | 14/16 [01:36<00:13,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.92s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.92s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:42<00:06,  6.91s/it]Loading checkpoint shards:  94%|█████████▍| 15/16 [01:43<00:06,  6.91s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.47s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.47s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.47s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.48s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.48s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.48s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:43<00:00,  6.47s/it]
Loading checkpoint shards: 100%|██████████| 16/16 [01:44<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 16/16 [01:44<00:00,  6.52s/it]
[INFO|modeling_utils.py:5541] 2025-08-17 05:22:38,595 >> All model checkpoint weights were used when initializing Qwen3MoeForCausalLM.

[INFO|modeling_utils.py:5549] 2025-08-17 05:22:38,595 >> All the weights of Qwen3MoeForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-30B-A3B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3MoeForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1058] 2025-08-17 05:22:38,888 >> loading configuration file generation_config.json from cache at /home/Competition2025/P02/P02U006/.cache/huggingface/hub/models--Qwen--Qwen3-30B-A3B/snapshots/ad44e777bcd18fa416d9da3bd8f70d33ebb85d39/generation_config.json
[INFO|configuration_utils.py:1103] 2025-08-17 05:22:38,888 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Loading cached processed dataset at /home/Competition2025/P02/P02U006/.cache/huggingface/datasets/neko-llm___hle_sft_open_thoughts-114k/default/0.0.0/95b7ec559aefd67116876c6fe0c205c412188605/cache-8766f1740da88d20_*_of_00012.arrow
Concatenating 12 shards
Loading cached processed dataset at /home/Competition2025/P02/P02U006/.cache/huggingface/datasets/neko-llm___hle_sft_open_thoughts-114k/default/0.0.0/95b7ec559aefd67116876c6fe0c205c412188605/cache-9cfac388ffdb81bd_*_of_00012.arrow
Concatenating 12 shards
[INFO|trainer.py:707] 2025-08-17 05:22:40,572 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:757] 2025-08-17 05:22:40,572 >> Using auto half precision backend
[INFO|deepspeed.py:492] 2025-08-17 05:23:26,758 >> Attempting to resume from data/Qwen3-30B-test/checkpoint-4
[INFO|trainer.py:2432] 2025-08-17 05:24:41,615 >> ***** Running training *****
[INFO|trainer.py:2433] 2025-08-17 05:24:41,615 >>   Num examples = 6,000
[INFO|trainer.py:2434] 2025-08-17 05:24:41,615 >>   Num Epochs = 1
[INFO|trainer.py:2435] 2025-08-17 05:24:41,615 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2438] 2025-08-17 05:24:41,615 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2439] 2025-08-17 05:24:41,615 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2440] 2025-08-17 05:24:41,615 >>   Total optimization steps = 3
[INFO|trainer.py:2441] 2025-08-17 05:24:41,673 >>   Number of trainable parameters = 30,532,122,624
[INFO|trainer.py:2463] 2025-08-17 05:24:41,674 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:2464] 2025-08-17 05:24:41,674 >>   Continuing training from epoch 0
[INFO|trainer.py:2465] 2025-08-17 05:24:41,674 >>   Continuing training from global step 4
[INFO|trainer.py:2467] 2025-08-17 05:24:41,674 >>   Will skip the first 0 epochs then the first 32 batches in the first epoch.
  0%|          | 0/3 [00:00<?, ?it/s][rank4]: Traceback (most recent call last):
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/training/open-r1/src/open_r1/sft.py", line 250, in <module>
[rank4]:     main(script_args, training_args, model_args, data_config)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/training/open-r1/src/open_r1/sft.py", line 194, in main
[rank4]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/transformers/trainer.py", line 2237, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/transformers/trainer.py", line 2578, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", line 864, in training_step
[rank4]:     return super().training_step(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/transformers/trainer.py", line 3840, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/accelerate/accelerator.py", line 2321, in backward
[rank4]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank4]:     self.engine.backward(loss, **kwargs)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2216, in backward
[rank4]:     self._do_optimizer_backward(loss, retain_graph)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2162, in _do_optimizer_backward
[rank4]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 2280, in backward
[rank4]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank4]:     scaled_loss.backward(retain_graph=retain_graph)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 1208, in reduce_leaf_module_grads
[rank4]:     self.reduce_ready_partitions_and_remove_grads(param)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 1527, in reduce_ready_partitions_and_remove_grads
[rank4]:     self.reduce_independent_p_g_buckets_and_remove_grads(param)
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 1269, in reduce_independent_p_g_buckets_and_remove_grads
[rank4]:     self.__reduce_and_partition_ipg_grads()
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank4]:     return func(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 1319, in __reduce_and_partition_ipg_grads
[rank4]:     grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)
[rank4]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage3.py", line 1391, in __avg_scatter_grads
[rank4]:     grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
[rank4]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank4]:     return func(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 199, in reduce_scatter_coalesced
[rank4]:     tensor_partition_flat_buffer = instrument_w_nvtx(torch.cat)(tensor_partitions_lst_with_padding)
[rank4]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 952.00 MiB. GPU 4 has a total capacity of 79.44 GiB of which 946.62 MiB is free. Including non-PyTorch memory, this process has 78.51 GiB memory in use. Of the allocated memory 70.55 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0817 05:25:44.027000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 476999 closing signal SIGTERM
W0817 05:25:44.030000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477000 closing signal SIGTERM
W0817 05:25:44.033000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477001 closing signal SIGTERM
W0817 05:25:44.037000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477002 closing signal SIGTERM
W0817 05:25:44.039000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477004 closing signal SIGTERM
W0817 05:25:44.043000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477005 closing signal SIGTERM
W0817 05:25:44.046000 476715 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477006 closing signal SIGTERM
E0817 05:25:46.701000 476715 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 4 (pid: 477003) of binary: /home/Competition2025/P02/P02U006/llm2025compet/openr1/bin/python3
Traceback (most recent call last):
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/Competition2025/P02/P02U006/llm2025compet/openr1/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
open_r1/sft.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-17_05:25:44
  host      : osk-gpu54
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 477003)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: osk-gpu54: task 0: Exited with exit code 1
