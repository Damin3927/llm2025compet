import os
import pandas as pd
import requests
from datasets import Dataset
from tqdm import tqdm
import time
import json
import re
import datasets
from huggingface_hub import upload_file

# --- OpenRouter API Settings ---
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL_NAME = "deepseek/deepseek-r1-0528:free"
YOUR_SITE_URL = "http://localhost"

# --- App Name ---
APP_NAME = "GPQA add CoT Dataset"

# --- Hugging Face Upload Settings: Repository Name ---
OUTPUT_DATASET_ID = "neko-llm/HLE_SFT_GPQA_Diamond"


def generate_prompt_for_gpqa_dataset(id: int, question: str, answer: str, explanation: str, subdomain: str) -> str:
  """
  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

  - GPQA Dataset ã®ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã™ã‚‹ã€‚
  """

  return f"""
    You are an expert in {subdomain}.

    ## Rules
    - Using the information from question/explanation/answer, please generate a reasoning process (Reasoning/CoT).
      - question: {question}
      - answer: {answer}
      - explanation: {explanation}
  """


def generate_cot_with_openrouter(prompt: str) -> tuple[str, str]:
    """
    OpenRouter API ã‚’ä½¿ã£ã¦ CoT ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

    Returns:
        tuple[status, response]: (æˆåŠŸ/å¤±æ•—, APIå¿œç­”ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    if not OPENROUTER_API_KEY:
        return "failure", "âŒ OPENROUTER_API_KEY environment variable not set."

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": YOUR_SITE_URL,
        "X-Title": APP_NAME,
        "Content-Type": "application/json"
    }

    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}]
    }

    last_error = ""

    for _attempt in range(3):  # 3å›ã¾ã§å†è©¦è¡Œ
        try:
            response = requests.post(OPENROUTER_API_URL, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            json_response = response.json()
            # print(f"ğŸ” OpenRouter API ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {json_response}")

            if 'choices' in json_response and len(json_response['choices']) > 0:
                content = json_response['choices'][0]['message']['content']

                # reasoningã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆDeepSeek R1ç‰¹æœ‰ï¼‰
                reasoning = ""
                if 'reasoning' in json_response['choices'][0]:
                    reasoning = json_response['choices'][0]['reasoning']
                else:
                    print("âš ï¸ reasoning ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚contentã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

                print(f"ğŸ” OpenRouter API Content: {content}")
                if reasoning:
                    print(f"ğŸ” OpenRouter API Reasoning: {reasoning}")

                # contentã®ã¿ã‚’è¿”ã™ï¼ˆçµ±ä¸€ã•ã‚ŒãŸæˆ»ã‚Šå€¤ï¼‰
                return "success", content.strip()
            else:
                last_error = f"âŒ API response missing valid content. Response: {json_response}"
                print(f"âŒ API response missing valid content. Response:: {json_response}")

        except requests.exceptions.HTTPError as e:
            if e.response.status_code in [402, 429]:
                try:
                    error_details = e.response.json().get('error', {}).get('message', '')
                except json.JSONDecodeError:
                    error_details = e.response.text
                final_error_message = f"âŒ Possible credit exhaustion or rate limit ({e.response.status_code}): {error_details}"
                return "failure", final_error_message
            else:
                last_error = f"âŒ HTTP Error: {e}"
        except Exception as e:
            last_error = f"âŒ An unknown error occurred: {e}"

        time.sleep(1)  # å†è©¦è¡Œå‰ã«1ç§’å¾…æ©Ÿ

    return "failure", last_error


def parse_json_response(response_text: str) -> dict:
    """
    API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ JSON ã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹ã™ã‚‹é–¢æ•°

    Returns:
        dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸ JSON ãƒ‡ãƒ¼ã‚¿ã€å¤±æ•—æ™‚ã¯ç©ºã®è¾æ›¸
    """
    try:
        # ```json ... ``` ã®å½¢å¼ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’å‡¦ç†
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # JSON ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ä½“ã‚’JSONã¨ã—ã¦è©¦è¡Œ
            json_str = response_text

        return json.loads(json_str)
    except json.JSONDecodeError:
        print(f"âš ï¸ Failed to parse JSON from response: {response_text[:200]}...")
        return {}


def generate_readme_content(df: pd.DataFrame, csv_filename: str, parquet_filename: str, jsonl_filename: str, total_count: int, success_count: int) -> str:
    """
    README.md ã®å†…å®¹ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®DataFrame
        csv_filename: CSVãƒ•ã‚¡ã‚¤ãƒ«å
        parquet_filename: Parquetãƒ•ã‚¡ã‚¤ãƒ«å
        jsonl_filename: JSONLãƒ•ã‚¡ã‚¤ãƒ«å
        total_count: ç·å‡¦ç†æ•°
        success_count: æˆåŠŸæ•°

    Returns:
        str: README.mdã®å†…å®¹
    """
    # ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚’å–å¾—
    columns = list(df.columns) if not df.empty else []

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆæœ€åˆã®1ä»¶ï¼‰
    sample_data = df.iloc[0].to_dict() if not df.empty else {}

    readme_content = f"""# HLE SFT GPQA Diamond Dataset

## æ¦‚è¦

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€GPQA (Graduate-level Google-proof Q&A) Diamond ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åŸºã«ã€Chain of Thought (CoT) æ¨è«–ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã•ã‚ŒãŸSupervised Fine-Tuning (SFT) ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚

å°‚é–€çš„ãªç§‘å­¦åˆ†é‡ï¼ˆç‰©ç†å­¦ã€åŒ–å­¦ã€ç”Ÿç‰©å­¦ï¼‰ã«ãŠã‘ã‚‹é«˜åº¦ãªè³ªå•ã«å¯¾ã—ã¦ã€æ®µéšçš„ãªæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚€å›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ

- **ç·å•é¡Œæ•°**: {total_count:,} å•
- **æˆåŠŸç”Ÿæˆæ•°**: {success_count:,} å•
- **æˆåŠŸç‡**: {success_count/total_count*100:.1f}%

## ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®3ã¤ã®å½¢å¼ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š

### 1. CSVå½¢å¼ (`{csv_filename}`)
- ä¸€èˆ¬çš„ãªè¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿
- Excel ã‚„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚½ãƒ•ãƒˆã§é–‹ã‘ã¾ã™
- Pandas ã§ç°¡å˜ã«èª­ã¿è¾¼ã¿å¯èƒ½

### 2. Parquetå½¢å¼ (`{parquet_filename}`)
- é«˜åŠ¹ç‡ãªã‚«ãƒ©ãƒ å‹ãƒ‡ãƒ¼ã‚¿å½¢å¼
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿèª­ã¿è¾¼ã¿ã«æœ€é©
- Apache Arrow ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã§æ¨å¥¨

### 3. JSONLå½¢å¼ (`{jsonl_filename}`)
- JSON Lines å½¢å¼ï¼ˆ1è¡Œ1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«é©ã—ã¦ã„ã‚‹
- æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§åºƒãå¯¾å¿œ

## ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã«ã¯ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å | å‹ | èª¬æ˜ |
|-------------|----|----|"""

    # åˆ—æƒ…å ±ã‚’å‹•çš„ã«è¿½åŠ 
    for col in columns:
        if col == 'id':
            readme_content += f"\n| `{col}` | int | å•é¡Œã®ä¸€æ„è­˜åˆ¥å­ |"
        elif col == 'question':
            readme_content += f"\n| `{col}` | str | å…ƒã®è³ªå•æ–‡ |"
        elif col == 'output':
            readme_content += f"\n| `{col}` | str | CoTæ¨è«–éç¨‹ã‚’å«ã‚€å®Œå…¨ãªå›ç­”ï¼ˆ&lt;think&gt;...&lt;/think&gt;å½¢å¼ï¼‰ |"
        elif col == 'answer':
            readme_content += f"\n| `{col}` | str | æœ€çµ‚çš„ãªæ­£è§£ |"
        else:
            readme_content += f"\n| `{col}` | str | {col} |"

    readme_content += f"""

## ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿

```json
{json.dumps(sample_data, ensure_ascii=False, indent=2)}
```

## ä½¿ç”¨æ–¹æ³•

### Python (Pandas)

```python
import pandas as pd

# CSVå½¢å¼ã§èª­ã¿è¾¼ã¿
df = pd.read_csv('{csv_filename}')

# Parquetå½¢å¼ã§èª­ã¿è¾¼ã¿ï¼ˆæ¨å¥¨ï¼‰
df = pd.read_parquet('{parquet_filename}')

# JSONLå½¢å¼ã§èª­ã¿è¾¼ã¿
df = pd.read_json('{jsonl_filename}', lines=True)

print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {{len(df)}} å•")
print(df.head())
```

### Hugging Face Datasets

```python
from datasets import load_dataset

# ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
dataset = load_dataset("neko-llm/HLE_SFT_GPQA_Diamond")

# ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æŒ‡å®š
dataset = load_dataset("neko-llm/HLE_SFT_GPQA_Diamond", data_files="{parquet_filename}")
```

### PyTorch DataLoader

```python
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class GPQADataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {{
            'question': row['question'],
            'output': row['output'],
            'answer': row['answer']
        }}

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_parquet('{parquet_filename}')
dataset = GPQADataset(df)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

## ç”Ÿæˆæ–¹æ³•

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®æ‰‹é †ã§ç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼š

1. **å…ƒãƒ‡ãƒ¼ã‚¿**: [GPQA Diamond Dataset](https://huggingface.co/datasets/Idavidrein/gpqa) ã® train split ã‚’ä½¿ç”¨
2. **CoTç”Ÿæˆ**: DeepSeek-R1-0528:free ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–éç¨‹ã‚’ç”Ÿæˆ
3. **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: `<think>æ¨è«–éç¨‹</think>æœ€çµ‚å›ç­”` ã®å½¢å¼ã§æ§‹é€ åŒ–
4. **å“è³ªç®¡ç†**: APIå‘¼ã³å‡ºã—ã®æˆåŠŸ/å¤±æ•—ã‚’è¨˜éŒ²ã—ã€å“è³ªã‚’æ‹…ä¿

### ç”Ÿæˆã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«
- **ãƒ¢ãƒ‡ãƒ«**: `{MODEL_NAME}`
- **API**: OpenRouter API
- **ç”Ÿæˆæ–¹å¼**: Few-shot prompting with domain expertise

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å…ƒã®GPQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚å­¦è¡“ç ”ç©¶ç›®çš„ã§ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## å¼•ç”¨

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š

```bibtex
@dataset{{hle_sft_gpqa_diamond,
  title={{HLE SFT GPQA Diamond Dataset with Chain of Thought}},
  author={{neko-llm}},
  year={{2024}},
  url={{https://huggingface.co/datasets/neko-llm/HLE_SFT_GPQA_Diamond}}
}}
```

å…ƒã®GPQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¼•ç”¨ï¼š
```bibtex
@article{{rein2023gpqa,
  title={{GPQA: A Graduate-Level Google-Proof Q&A Benchmark}},
  author={{Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R}},
  journal={{arXiv preprint arXiv:2311.12022}},
  year={{2023}}
}}
```

## å•ã„åˆã‚ã›

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚„å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€[Issues](https://huggingface.co/datasets/neko-llm/HLE_SFT_GPQA_Diamond/discussions) ã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
"""

    return readme_content


def main():
    """
    GPQA Dataset ã® CoT (Reasoning) ç”Ÿæˆ & ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’è¡Œã† Main Scriptã€‚

    NOTE:
    é–¢æ•°ã®æ©Ÿèƒ½:
    1. GPQA Dataset ã‚’èª­ã¿è¾¼ã‚€ã€‚
    2. GPQA Dataset ã‚’å‡¦ç†ã—ã¦ã€ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    3. ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ local ã«ä¿å­˜ã™ã‚‹ã€‚
    4. ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Hugging Face ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    print("ğŸš€ GPQA Dataset CoT ç”Ÿæˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    output_filename = "gpqa_diamond_cot_dataset.csv"

    # --- Resume Logic ---
    processed_ids = []
    if os.path.exists(output_filename):
        print(f"ğŸ“„ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: '{output_filename}'")
        try:
            existing_df = pd.read_csv(output_filename)
            if 'id' in existing_df.columns:
                processed_ids = existing_df['id'].dropna().tolist()
                print(f"âœ… {len(processed_ids)} ä»¶ã®å‡¦ç†æ¸ˆã¿å•é¡Œã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        except pd.errors.EmptyDataError:
            print("âš ï¸ æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚æ–°è¦ã§é–‹å§‹ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")

    # --- GPQA Dataset Loading ---
    print("ğŸ“¥ GPQA Dataset (gpqa_diamond) ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        # GPQA ã® gpqa_diamond subset ã‚’èª­ã¿è¾¼ã¿ï¼ˆtrainãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        diamond_dataset = datasets.load_dataset("Idavidrein/gpqa", "gpqa_diamond", split='train')
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†ã€‚{len(diamond_dataset)} ä»¶ã®å•é¡Œã‚’ç™ºè¦‹ã€‚")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        print("ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
        sample = diamond_dataset[0]
        for key in sample.keys():
            print(f"  {key}: {str(sample[key])[:100]}...")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # --- Main Processing Loop ---
    batch_results = []
    total_problems = len(diamond_dataset)
    print(f"\nğŸš€ {total_problems} ä»¶ã®å•é¡Œã«å¯¾ã—ã¦CoTç”Ÿæˆã‚’é–‹å§‹...")
    print(f"ğŸ’¾ çµæœã¯ '{output_filename}' ã«5ä»¶ãšã¤ãƒãƒƒãƒã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    for i, item in enumerate(tqdm(diamond_dataset, desc="CoTç”Ÿæˆä¸­")):
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        item_id = i + 1  # IDã‚’1ã‹ã‚‰é–‹å§‹
        if item_id in processed_ids:
            continue

        print(f"\n--- å•é¡Œ {i + 1}/{total_problems} ã‚’å‡¦ç†ä¸­ ---")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = generate_prompt_for_gpqa_dataset(
            id=item_id,
            question=item['Question'],
            answer=item['Correct Answer'],
            explanation=item['Explanation'],
            subdomain=item['Subdomain']
        )
        print(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")

        # OpenRouter API ã§CoTç”Ÿæˆ
        status, content = generate_cot_with_openrouter(prompt)
        print(f"ğŸ” API ãƒ¬ã‚¹ãƒãƒ³ã‚¹(Content): {content}")
        print(f"ğŸ” API ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")

        # çµæœã‚’ä¿å­˜
        batch_results.append({
            "id": item_id,
            "question": item['Question'],
            # æ¨è«–éç¨‹(Reasoning/CoT)ã‚’ç”Ÿæˆã—ã¦ã€<think>...</think> ã‚¿ã‚°ã§å›²ã¿ã€æœ€çµ‚çš„ãªå›ç­”(answer)ã‚’ </think> ã‚¿ã‚°ã®å¾Œã«è¨˜è¼‰ã™ã‚‹ã€‚
            "output": f"<think>{content}</think>{item['Correct Answer']}",
            "answer": item['Correct Answer']
        })

        # 5ä»¶ã”ã¨ã¾ãŸã¯æœ€å¾Œã®å‡¦ç†ã§CSVã«ä¿å­˜
        if (i + 1) % 5 == 0 or (i + 1) == total_problems:
            print(f"ğŸ’¾ {len(batch_results)} ä»¶ã®çµæœã‚’CSVã«ä¿å­˜ä¸­...")
            temp_df = pd.DataFrame(batch_results)
            temp_df.to_csv(
                output_filename,
                mode='a',
                header=not os.path.exists(output_filename) or os.path.getsize(output_filename) == 0,
                index=False,
                encoding='utf-8-sig'
            )
            batch_results.clear()

        # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
        time.sleep(0.5)

    print(f"\nâœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€çµ‚çµæœã¯ '{output_filename}' ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")

    # --- è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®ä¿å­˜ã¨Hugging Face ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    try:
        print("\nğŸ”„ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
        final_df: pd.DataFrame = pd.read_csv(output_filename)

        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—ï¼ˆ'status'åˆ—ãŒã‚ã‚‹å ´åˆï¼‰
        if 'status' in final_df.columns:
            success_count = len(final_df[final_df['status'] == 'success'])
        else:
            # 'status'åˆ—ãŒãªã„å ´åˆã¯å…¨ä»¶ã‚’æˆåŠŸã¨ã—ã¦æ‰±ã†
            success_count = len(final_df)
        total_count = len(final_df)

        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’å–å¾—
        base_filename = os.path.splitext(output_filename)[0]

        # --- 2. Parquetå½¢å¼ã§ã®ä¿å­˜ ---
        parquet_filename = f"{base_filename}.parquet"
        print(f"ğŸ’¾ Parquetå½¢å¼ã§ä¿å­˜ä¸­: {parquet_filename}")
        final_df.to_parquet(parquet_filename, index=False)

        # --- 3. JSONLå½¢å¼ã§ã®ä¿å­˜ ---
        jsonl_filename = f"{base_filename}.jsonl"
        print(f"ğŸ’¾ JSONLå½¢å¼ã§ä¿å­˜ä¸­: {jsonl_filename}")
        final_df.to_json(jsonl_filename, orient='records', lines=True, force_ascii=False)

        # --- Hugging Face Datasetsã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
        print(f"ğŸ“¤ {OUTPUT_DATASET_ID} ã«3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # 1. CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“„ CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=output_filename,
            path_in_repo=output_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 2. Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“Š Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=parquet_filename,
            path_in_repo=parquet_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        # 3. JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print("  ğŸ“ JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=jsonl_filename,
            path_in_repo=jsonl_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )

        print(f"âœ… 3ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {OUTPUT_DATASET_ID}")

        # --- README.md ã®ç”Ÿæˆã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
        print("ğŸ“ README.md ã‚’ç”Ÿæˆä¸­...")
        readme_content = generate_readme_content(final_df, output_filename, parquet_filename, jsonl_filename, total_count, success_count)
        readme_filename = "README.md"

        with open(readme_filename, 'w', encoding='utf-8') as f:
            f.write(readme_content)

        print("ğŸ“¤ README.md ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        upload_file(
            path_or_fileobj=readme_filename,
            path_in_repo=readme_filename,
            repo_id=OUTPUT_DATASET_ID,
            repo_type="dataset"
        )
        print("âœ… README.md ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        # --- çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º ---
        print(f"\nğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"  - ç·å‡¦ç†æ•°: {total_count}")
        print(f"  - æˆåŠŸæ•°: {success_count}")
        print(f"  - æˆåŠŸç‡: {success_count/total_count*100:.1f}%")

        print(f"\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - CSV: {output_filename}")
        print(f"  - Parquet: {parquet_filename}")
        print(f"  - JSONL: {jsonl_filename}")
        print(f"  - README: {readme_filename}")

        print(f"\nğŸŒ Hugging Face Datasetsã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆ:")
        print(f"  - Repository: {OUTPUT_DATASET_ID}")
        print(f"    â”œâ”€ {output_filename}")
        print(f"    â”œâ”€ {parquet_filename}")
        print(f"    â”œâ”€ {jsonl_filename}")
        print(f"    â””â”€ {readme_filename}")

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¾ãŸã¯Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    print("\n--- Processing complete ---")

if __name__ == "__main__":
    main()
