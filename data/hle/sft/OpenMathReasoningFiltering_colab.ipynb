{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# OpenMathReasoning 難易度スコアリングノートブック\n",
    "\n",
    "このノートブックは、数学問題の難易度を評価するためのものです。\n",
    "ツールの使用が許可されていないため、CoT（Chain-of-Thought）とGenSelectの分割のみを使用します。\n",
    "\n",
    "## 処理の流れ：\n",
    "1. データセットの読み込み\n",
    "2. データセットをCoTとGenSelect分割のみにフィルタリング\n",
    "3. 推論LLMを使用して質問の難易度をn回スコアリング\n",
    "4. 判定LLMを使用して回答の正誤をチェック\n",
    "5. 正答率に基づいて質問の難易度をスコアリング\n",
    "6. データセットの保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a4b49c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Colabで必要なパッケージのインストール\n",
    "%pip install datasets pandas tqdm transformers vllm --quiet\n",
    "\n",
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from itertools import islice\n",
    "\n",
    "# 分散環境の設定\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"GLOO_SOCKET_IFNAME\"] = \"lo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4afc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = 1 # 複数回実行する予定ですので、結果を保存するためにrun_indexを使用します\n",
    "# モデルとパラメータの設定\n",
    "inference_model = \"Qwen/Qwen3-8B\"  # TODO: 使用したい推論モデルに変更してください\n",
    "inference_temperature = 0.3  # 温度パラメータ（低いほど決定的な出力になります）\n",
    "inference_max_tokens = 1024 # 生成する最大トークン数\n",
    "inference_batch_size = 4\n",
    "inference_tp, inference_pp, inference_dp = 1, 1, 1  # テンソル並列、パイプライン並列、データ並列の設定\n",
    "save_per_batch = 1  # `save_per_batch`バッチごとに結果を保存\n",
    "\n",
    "judgement_model = \"Qwen/Qwen3-8B\"  # TODO: 使用したい判定モデルに変更してください\n",
    "judgement_temperature = 0.1  # 温度パラメータ（低いほど決定的な出力になります）\n",
    "judgement_max_tokens = 50\n",
    "judgement_batch_size = 4\n",
    "judgement_tp, judgement_pp, judgement_dp = 1, 1, 1\n",
    "\n",
    "# データセットサイズの設定（固定値）\n",
    "cot_dataset_size = 3.3e9\n",
    "genselect_dataset_size = 5.66e5\n",
    "\n",
    "# 処理範囲の設定\n",
    "start_from_percentage = 0  # 0.5 = 50%から開始\n",
    "end_at_percentage = 1.0  # 1.0 = 100%まで処理\n",
    "\n",
    "# 出力ディレクトリの設定\n",
    "output_dir = \"./results\"\n",
    "inference_dir = f\"{output_dir}/inference/run_{run_index}\"  # 推論結果の一時保存用\n",
    "judgement_dir = f\"{output_dir}/judgement/run_{run_index}\"  # 判定結果の一時保存用\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "os.makedirs(judgement_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42c0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプトの定義\n",
    "# 推論LLM用のプロンプト（CoT形式）\n",
    "inference_cot_prompt = (\n",
    "    \"You are a highly skilled mathematician known for clear and rigorous reasoning.\\n\"\n",
    "    \"Given the following math question, provide a step-by-step analysis of your thought process, followed by the final answer.\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Please respond with only your reasoning steps and the final answer. Do not include any extraneous text or explanations outside your solution.\"\n",
    ")\n",
    "\n",
    "inference_genselect_prompt = (\n",
    "    \"You are a highly skilled mathematician known for clear and rigorous reasoning.\\n\"\n",
    "    \"You are given a math question along with several candidate answers.\\n\"\n",
    "    \"Analyze each candidate solution, explain your reasoning, and then state which candidate is correct as your final answer.\\n\"\n",
    "    \"Question and candidate solutions:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Please respond with only your analysis and the final answer. The final answer must be one of the provided candidate solutions. Do not include any extraneous text.\"\n",
    ")\n",
    "\n",
    "judgement_cot_prompt = (\n",
    "    \"You are a mathematics expert tasked with evaluating a user's solution.\\n\"\n",
    "    \"You will be given a question, the correct answer, and the user's solution (including their reasoning and final answer).\\n\"\n",
    "    \"Determine if BOTH the reasoning and the final answer in the user's solution are correct.\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Correct answer:\\n\"\n",
    "    \"{correct_answer}\\n\"\n",
    "    \"User's solution:\\n\"\n",
    "    \"{solution}\\n\"\n",
    "    \"Reply with only 'yes' if both are correct, or 'no' if either is incorrect. Do not include any other text.\"\n",
    ")\n",
    "\n",
    "judgement_genselect_prompt = (\n",
    "    \"You are a mathematics expert tasked with evaluating a user's solution.\\n\"\n",
    "    \"You will be given a question with candidate solutions, the correct answer, and the user's analysis and final answer.\\n\"\n",
    "    \"Determine if BOTH the reasoning and the final answer in the user's solution are correct.\\n\"\n",
    "    \"Question and candidate solutions:\\n\"\n",
    "    \"{question}\\n\"\n",
    "    \"Correct answer:\\n\"\n",
    "    \"{correct_answer}\\n\"\n",
    "    \"User's solution:\\n\"\n",
    "    \"{solution}\\n\"\n",
    "    \"Reply with only 'yes' if both are correct, or 'no' if either is incorrect. Do not include any other text.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d841d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLMを使用した推論と判定の関数定義\n",
    "\n",
    "def vllm_inference(llm, prompts, temperature=0.3, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    推論用の関数：与えられたプロンプトに対して推論を実行\n",
    "    \"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,  # 各プロンプトに対する生成数\n",
    "    )\n",
    "    results = []\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in outputs:\n",
    "        results.append(output.outputs[0].text)\n",
    "    return results\n",
    "\n",
    "def vllm_judgement(llm, prompts, temperature=0.1, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    判定用の関数：与えられたプロンプトに対して判定を実行\n",
    "    \"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "    )\n",
    "    results = []\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in outputs:\n",
    "        results.append(output.outputs[0].text)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f470245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット全体に対する推論処理の関数\n",
    "def inference(inf_dataset, inference_batch_size, save_per_batch, inference_temperature, inference_max_tokens, inference_prompt, inference_dir, dataset_size):\n",
    "    if not os.path.exists(inference_dir):\n",
    "        os.makedirs(inference_dir)\n",
    "\n",
    "    inference_collection = []\n",
    "    start_from_batch_index = int(dataset_size * start_from_percentage // inference_batch_size)\n",
    "    end_at_batch_index = int(dataset_size * end_at_percentage // inference_batch_size)\n",
    "\n",
    "    # filter the inf_dataset by the problem_type column to be has_answer_extracted\n",
    "    inf_dataset = inf_dataset.filter(lambda x: x['problem_type'] == 'has_answer_extracted')\n",
    "    i = 1\n",
    "    for data_batch in tqdm(inf_dataset.iter(batch_size=inference_batch_size), desc=\"Inferencing\"):\n",
    "        if i < start_from_batch_index:\n",
    "            i += 1\n",
    "            continue\n",
    "        if i >= end_at_batch_index:\n",
    "            break\n",
    "        \n",
    "        if i % (save_per_batch) == 0 and os.path.exists(f\"{inference_dir}/inference_{i}.json\"):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        inference_prompts = [inference_prompt.format(question=question) for question in data_batch[\"problem\"]]\n",
    "        inference_results = vllm_inference(llm, inference_prompts, inference_temperature, inference_max_tokens)\n",
    "        # add the inference results to data_batch, make a new column called 'inference'\n",
    "        data_batch['inference'] = inference_results\n",
    "        # data_batch is a dictionary, convert it to a list of dictionaries\n",
    "        data_batch = [{k: v[j] for k, v in data_batch.items()} for j in range(inference_batch_size)]\n",
    "        inference_collection.extend(data_batch)\n",
    "\n",
    "        # save the temporary inference results\n",
    "        if i % (save_per_batch) == 0:\n",
    "            # save as pandas dataframe\n",
    "            with open(f\"{inference_dir}/inference_{i}.json\", \"w\") as f:\n",
    "                json.dump(inference_collection, f)\n",
    "            inference_collection.clear()\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1aba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判定処理の関数\n",
    "def judgement(jud_model, judgement_batch_size, judgement_temperature, judgement_max_tokens, judgement_prompt, inference_dir, judgement_dir):\n",
    "    if not os.path.exists(judgement_dir):\n",
    "        os.makedirs(judgement_dir)\n",
    "\n",
    "    for inf_filename in tqdm(os.listdir(inference_dir)):\n",
    "        judgement_filename = inf_filename.replace(\"inference\", \"judgement\")\n",
    "        if os.path.exists(f\"{judgement_dir}/{judgement_filename}\"):\n",
    "            continue\n",
    "\n",
    "        judgement_collection = []\n",
    "        with open(f\"{inference_dir}/{inf_filename}\", \"r\") as f:\n",
    "            inf_results = json.load(f)\n",
    "        num_rows = len(inf_results)\n",
    "        for index in tqdm(range(0, num_rows, judgement_batch_size), desc=\"Judging\"):\n",
    "            batch = inf_results[index:index+judgement_batch_size]\n",
    "            question = [item['problem'] for item in batch]\n",
    "            correct_answer = [item['generated_solution'] for item in batch]\n",
    "            solution = [item['inference'] for item in batch]\n",
    "            judgement_prompts = [judgement_prompt.format(question=q, correct_answer=ca, solution=s) for q, ca, s in zip(question, correct_answer, solution)]\n",
    "            judgement_results = vllm_judgement(jud_model, judgement_prompts, judgement_temperature, judgement_max_tokens)\n",
    "            for i, item in enumerate(batch):\n",
    "                item['judgement'] = judgement_results[i]\n",
    "            judgement_collection.extend(batch)\n",
    "        with open(f\"{judgement_dir}/{judgement_filename}\", \"w\") as f:\n",
    "            json.dump(judgement_collection, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メイン処理の実行\n",
    "\n",
    "# 推論モデルのロード\n",
    "print(\"推論モデルをロード中...\")\n",
    "llm = LLM(\n",
    "    model=inference_model,\n",
    "    tensor_parallel_size=inference_tp,\n",
    "    pipeline_parallel_size=inference_pp,\n",
    "    gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "# CoTデータセットの処理\n",
    "print(\"CoTデータセットの処理を開始...\")\n",
    "cot_dataset = datasets.load_dataset(\"nvidia/OpenMathReasoning\", split='cot', streaming=True)\n",
    "inference(\n",
    "    cot_dataset,\n",
    "    inference_batch_size,\n",
    "    save_per_batch,\n",
    "    inference_temperature,\n",
    "    inference_max_tokens,\n",
    "    inference_cot_prompt,\n",
    "    inference_dir + \"/cot\",\n",
    "    cot_dataset_size\n",
    ")\n",
    "del cot_dataset  # メモリの解放\n",
    "\n",
    "# GenSelectデータセットの処理\n",
    "print(\"GenSelectデータセットの処理を開始...\")\n",
    "genselect_dataset = datasets.load_dataset(\"nvidia/OpenMathReasoning\", split='genselect', streaming=True)\n",
    "inference(\n",
    "    genselect_dataset,\n",
    "    inference_batch_size,\n",
    "    save_per_batch,\n",
    "    inference_temperature,\n",
    "    inference_max_tokens,\n",
    "    inference_genselect_prompt,\n",
    "    inference_dir + \"/genselect\",\n",
    "    genselect_dataset_size\n",
    ")\n",
    "del genselect_dataset  # メモリの解放\n",
    "del llm\n",
    "torch.cuda.empty_cache() # flush the caching allocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238ffe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "判定モデルをロード中...\n",
      "INFO 07-21 12:04:44 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 07-21 12:04:44 [config.py:1472] Using max model len 40960\n",
      "INFO 07-21 12:04:44 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-21 12:04:45 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-21 12:04:45 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-21 12:04:46 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-21 12:04:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-21 12:04:46 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen3-8B...\n",
      "INFO 07-21 12:04:46 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-21 12:04:46 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-21 12:04:46 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.15it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  1.99it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  2.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.36it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.39it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-21 12:04:49 [default_loader.py:272] Loading weights took 2.16 seconds\n",
      "INFO 07-21 12:04:49 [gpu_model_runner.py:1801] Model loading took 15.2683 GiB and 3.052762 seconds\n",
      "INFO 07-21 12:04:57 [backends.py:508] Using cache directory: /home/ljy/.cache/vllm/torch_compile_cache/28af56ab07/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-21 12:04:57 [backends.py:519] Dynamo bytecode transform time: 7.79 s\n",
      "INFO 07-21 12:05:03 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 4.980 s\n",
      "INFO 07-21 12:05:04 [monitor.py:34] torch.compile takes 7.79 s in total\n",
      "INFO 07-21 12:05:05 [gpu_worker.py:232] Available KV cache memory: 5.66 GiB\n",
      "INFO 07-21 12:05:05 [kv_cache_utils.py:716] GPU KV cache size: 41,216 tokens\n",
      "INFO 07-21 12:05:05 [kv_cache_utils.py:720] Maximum concurrency for 40,960 tokens per request: 1.01x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:17<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-21 12:05:22 [gpu_model_runner.py:2326] Graph capturing finished in 17 secs, took 0.61 GiB\n",
      "INFO 07-21 12:05:22 [core.py:172] init engine (profile, create kv cache, warmup model) took 33.08 seconds\n",
      "CoT結果の判定を開始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 81.78it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s, est. speed input: 9692.89 toks/s, output: 1.80 toks/s]\n",
      "Judging: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      " 25%|██▌       | 1/4 [00:02<00:06,  2.28s/it]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 71.12it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s, est. speed input: 9027.62 toks/s, output: 1.24 toks/s]\n",
      "Judging: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]\n",
      " 50%|█████     | 2/4 [00:05<00:05,  2.87s/it]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 52.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s, est. speed input: 9026.75 toks/s, output: 1.02 toks/s]\n",
      "Judging: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n",
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.39s/it]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 51.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it, est. speed input: 8503.77 toks/s, output: 0.84 toks/s]\n",
      "Judging: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenSelect結果の判定を開始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/inference/genselect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# GenSelect結果の判定\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenSelect結果の判定を開始...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mjudgement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudgement_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudgement_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudgement_max_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudgement_genselect_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/genselect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjudgement_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/genselect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m全ての処理が完了しました！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mjudgement\u001b[0;34m(jud_model, judgement_batch_size, judgement_temperature, judgement_max_tokens, judgement_prompt, inference_dir, judgement_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(judgement_dir):\n\u001b[1;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(judgement_dir)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inf_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_dir\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     11\u001b[0m     judgement_filename \u001b[38;5;241m=\u001b[39m inf_filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjudgement\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 既に処理済みのファイルはスキップ\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/inference/genselect'"
     ]
    }
   ],
   "source": [
    "# 判定モデルのロード\n",
    "print(\"判定モデルをロード中...\")\n",
    "llm = LLM(\n",
    "    model=judgement_model,\n",
    "    tensor_parallel_size=judgement_tp,\n",
    "    pipeline_parallel_size=judgement_pp,\n",
    "    gpu_memory_utilization=0.95\n",
    ")\n",
    "\n",
    "# CoT結果の判定\n",
    "print(\"CoT結果の判定を開始...\")\n",
    "judgement(\n",
    "    llm,\n",
    "    judgement_batch_size,\n",
    "    judgement_temperature,\n",
    "    judgement_max_tokens,\n",
    "    judgement_cot_prompt,\n",
    "    inference_dir + \"/cot\",\n",
    "    judgement_dir + \"/cot\"\n",
    ")\n",
    "\n",
    "# GenSelect結果の判定\n",
    "print(\"GenSelect結果の判定を開始...\")\n",
    "judgement(\n",
    "    llm,\n",
    "    judgement_batch_size,\n",
    "    judgement_temperature,\n",
    "    judgement_max_tokens,\n",
    "    judgement_genselect_prompt,\n",
    "    inference_dir + \"/genselect\",\n",
    "    judgement_dir + \"/genselect\"\n",
    ")\n",
    "\n",
    "print(\"全ての処理が完了しました！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
