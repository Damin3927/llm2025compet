import numpy as np
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import logging
import sys
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    # Load the WildJailbreak training set
    logger.info("Loading WildJailbreak training dataset...")
    train_ds = load_dataset("allenai/wildjailbreak", "train", delimiter="\t", keep_default_na=False)
    
    # Load the WildJailbreak evaluation set
    logger.info("Loading WildJailbreak evaluation dataset...")
    test_ds = load_dataset("allenai/wildjailbreak", "eval", delimiter="\t", keep_default_na=False)
    
    df_all = train_ds["train"].to_pandas()
    df = df_all[df_all["data_type"] == "vanilla_harmful"]
    logger.info(f"Loaded {len(df)} vanilla_harmful training samples")
    
    if len(df) == 0:
        raise ValueError("No vanilla_harmful samples found in training data")
    
    df_all_test = test_ds["train"].to_pandas()
    df_test = df_all_test[df_all_test["data_type"] == "vanilla_harmful"]
    logger.info(f"Loaded {len(df_test)} vanilla_harmful test samples")
    
except Exception as e:
    logger.error(f"Failed to load datasets: {e}")
    sys.exit(1)

def mmr_with_query(embeddings: torch.Tensor, k: int, lam: float = 0.5) -> list[int]:
    # embeddings: (N, d), æ­£è¦åŒ–æ¸ˆã¿ã‚’å‰æï¼ˆcosine ã‚’å†…ç©ã§ä½¿ã†ï¼‰
    N = embeddings.size(0)
    device = embeddings.device

    # ã‚¯ã‚¨ãƒªã‚’å…¨ä½“å¹³å‡ã¨ã—ã¦ä½œã‚‹ï¼ˆæ­£è¦åŒ–ï¼‰
    query_vec = embeddings.mean(dim=0, keepdim=True)
    query_vec = torch.nn.functional.normalize(query_vec, dim=1)  # (1, d)

    # ä»£è¡¨æ€§ã‚¹ã‚³ã‚¢ï¼šå„ç‚¹ã¨ query ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    rep_score = torch.matmul(embeddings, query_vec.T).squeeze(1)  # (N,)

    selected = []
    # åˆå›ã¯ä»£è¡¨æ€§æœ€å¤§ã‚’é¸ã¶
    first = int(rep_score.argmax().item())
    selected.append(first)

    # å„å€™è£œã«ã¤ã„ã¦æ—¢é¸ã¨ã®æœ€å¤§é¡ä¼¼åº¦ï¼ˆå†—é•·æ€§ï¼‰ã‚’ä¿æŒ
    # åˆæœŸåŒ–ï¼šä¸€åº¦é¸ã°ã‚ŒãŸã‚‚ã®ã¨ã® similarity
    redundancy = torch.zeros(N, device=device)  # max similarity to S
    redundancy = torch.maximum(
        redundancy,
        torch.matmul(embeddings, embeddings[first:first+1].T).squeeze(1)
    )
    mask = torch.ones(N, dtype=torch.bool, device=device)
    mask[first] = False

    for _ in range(k - 1):
        # MMR ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆmaskedï¼‰
        scores = (1 - lam) * rep_score - lam * redundancy  # (N,)
        scores = scores.masked_fill(~mask, float("-inf"))
        next_idx = int(scores.argmax().item())
        selected.append(next_idx)

        # å†—é•·æ€§ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼šæ—¢é¸ã¨ã®æœ€å¤§é¡ä¼¼åº¦ã‚’ä¿æŒ
        sim_to_new = torch.matmul(embeddings, embeddings[next_idx: next_idx+1].T).squeeze(1)
        redundancy = torch.maximum(redundancy, sim_to_new)

        mask[next_idx] = False

    return selected




try:
    logger.info("Loading SentenceTransformer model...")
    model = SentenceTransformer("all-MiniLM-L6-v2")
    
    logger.info("Encoding texts...")
    texts = df["vanilla"].tolist()
    if not texts:
        raise ValueError("No texts found to encode")
    
    embs = model.encode(texts, convert_to_tensor=True, normalize_embeddings=True)
    logger.info(f"Encoded {len(texts)} texts to embeddings shape: {embs.shape}")
    
    # k ãŒåˆ©ç”¨å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¶…ãˆã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
    max_k = len(texts)
    k = min(1200, max_k)
    if k < 1200:
        logger.warning(f"Requested k=1200 but only {max_k} samples available. Using k={k}")
    
    logger.info(f"Selecting {k} samples using MMR...")
    selected_idx = mmr_with_query(embs, k=k, lam=0.5)
    df_selected = df.iloc[selected_idx].reset_index(drop=True)
    logger.info(f"Selected {len(df_selected)} samples")
    
    # completionåˆ—ã®å­˜åœ¨ç¢ºèª
    if "completion" not in df_selected.columns:
        raise ValueError("'completion' column not found in selected data")
    
    df_selected["completion"] = "<think></think>" + df_selected["completion"].astype(str)
    
except Exception as e:
    logger.error(f"Failed during text encoding and selection: {e}")
    sys.exit(1) 


# ==========================================================
# Qwen3 32B ã‚’ç”¨ã„ã¦ <think> ... </think> ã« COT ã‚’æŒ¿å…¥ã™ã‚‹å‡¦ç†
# ==========================================================

try:
    logger.info("Loading Qwen3-32B tokenizer...")
    tok = AutoTokenizer.from_pretrained(
        "Qwen/Qwen3-32B",
        trust_remote_code=True,
    )
    tok.padding_side = "left"
    
    # pad_token ãŒãªã‘ã‚Œã° eos_token ã‚’æµç”¨ï¼ˆå·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãŸã‚ã«å¿…è¦ï¼‰
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
        tok.pad_token_id = tok.eos_token_id  # å¿µã®ãŸã‚æ˜ç¤º
        logger.info("Set pad_token to eos_token")
        
    additional_special_tokens = []
    for tag in ["<think>", "</think>"]:
        if tok.convert_tokens_to_ids(tag) == tok.unk_token_id:
            additional_special_tokens.append(tag)
    if additional_special_tokens:
        tok.add_special_tokens({"additional_special_tokens": additional_special_tokens})
        logger.info(f"Added special tokens: {additional_special_tokens}")
    
    logger.info("Loading Qwen3-32B model... (This may take several minutes)")
    # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"Available GPU memory: {gpu_memory:.1f} GB")
        if gpu_memory < 60:
            logger.warning("GPU memory may be insufficient for Qwen3-32B. Consider using smaller model or CPU.")
    
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen3-32B",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map="auto",
    ).eval()
    
    # tokenizer ã«æ–°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ãŸå ´åˆã¯embedæ•°ã‚’ãƒªã‚µã‚¤ã‚º
    original_vocab_size = model.get_input_embeddings().weight.size(0)
    model.resize_token_embeddings(len(tok))
    new_vocab_size = model.get_input_embeddings().weight.size(0)
    if original_vocab_size != new_vocab_size:
        logger.info(f"Resized token embeddings from {original_vocab_size} to {new_vocab_size}")
    
    logger.info("Model loaded successfully")
    
except Exception as e:
    logger.error(f"Failed to load model: {e}")
    logger.error("Consider using a smaller model or ensure sufficient GPU memory")
    sys.exit(1)

try:
    end_tag = "</think>"
    eos_id = tok.convert_tokens_to_ids(end_tag)
    if eos_id == tok.unk_token_id:
        logger.warning("</think> token not found in vocabulary, using eos_token_id instead")
        eos_id = tok.eos_token_id
    
    logger.info(f"Using eos_id: {eos_id} for {end_tag}")
except NameError:
    logger.error("Tokenizer not initialized. Cannot proceed with generation.")
    sys.exit(1)

def build_prompt(prompt, answer):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹é–¢æ•°

    ä¸ãˆã‚‰ã‚ŒãŸ User ã‚¯ã‚¨ãƒªã¨çŸ­ã„ Assistant å›ç­”ã«å¯¾ã—ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®æ¨è«–ã‚’ <think> ã¨ </think> ã®é–“ã«å¿…ãšè¨˜è¿°ã•ã›ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ç”Ÿæˆä¾‹ã‚‚ä¸ãˆã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚°ã‚’å«ã‚€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚"""
    if not prompt or not answer:
        raise ValueError("Both prompt and answer must be non-empty")

    # few-shot ä¾‹ç¤º
    example_user = "What is 2 + 2?"
    example_answer = "4"
    example_reason  = "We add the two numbers 2 and 2 to obtain 4."

    template = (
        "You are an advanced large language model. "
        "Fill in the reasoning between <think> and </think> and be sure to CLOSE with </think>.\n\n"
        "### Example\n"
        f"User: {example_user}\n"
        f"Assistant: <think>{example_reason}</think>{example_answer}\n\n"
        "### Task\n"
        "You are given another pair below. Replace the empty placeholder with your reasoning.\n\n"
        f"User: {prompt}\n"
        f"Assistant: <think></think>{answer}\n\n"
        "Write your reasoning now. Output **only** the reasoning followed by </think>.\n"
        "<think>"
    )
    return template

def gen_cot(batch_rows, max_new=512):
    """Chain-of-Thoughtã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    try:
        if len(batch_rows) == 0:
            logger.warning("Empty batch provided to gen_cot")
            return []
        
        prompts = []
        for _, r in batch_rows.iterrows():
            try:
                original_completion = r.completion.replace("<think></think>", "")
                prompt = build_prompt(r.vanilla, original_completion)
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ã¾ã‚Šã«é•·ã„å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹ï¼‰
                if len(prompt) > 4000:
                    logger.warning(f"Prompt too long ({len(prompt)} chars), truncating...")
                    # vanillaéƒ¨åˆ†ã‚’åˆ‡ã‚Šè©°ã‚ã‚‹
                    vanilla_truncated = r.vanilla[:1000] + "..."
                    prompt = build_prompt(vanilla_truncated, original_completion)
                
                prompts.append(prompt)
                logger.debug(f"Built prompt: {prompt[:200]}...")
                
            except Exception as e:
                logger.error(f"Failed to build prompt for row: {e}")
                prompts.append("")  # ç©ºã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ ã—ã¦ç¶™ç¶š
        
        # ç©ºã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_prompts = [p for p in prompts if p]
        if len(valid_prompts) == 0:
            logger.error("No valid prompts generated")
            return [""] * len(batch_rows)
        
       
        logger.debug(f"Generating CoT for {len(valid_prompts)} prompts")
        inputs = tok(valid_prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            try:
                outs = model.generate(
                    **inputs,
                    max_new_tokens=max_new,
                    do_sample=True,
                    temperature=0.3,  # ã‚ˆã‚Šä½ã„æ¸©åº¦ã§å®‰å®šã—ãŸç”Ÿæˆ
                    top_p=0.9,
                    eos_token_id=eos_id,
                    pad_token_id=tok.pad_token_id,
                    repetition_penalty=1.1,  # ç¹°ã‚Šè¿”ã—ã‚’é¿ã‘ã‚‹
                )
            except Exception as e:
                logger.error(f"Error during generation: {e}")
                return [""] * len(batch_rows)

        texts = tok.batch_decode(outs, skip_special_tokens=False)
        cots = []
        
        # CoTæŠ½å‡º
        for i, t in enumerate(texts):
            try:
                # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                input_tokens = inputs['input_ids'][i]
                non_pad_mask = input_tokens != tok.pad_token_id
                actual_input_length = non_pad_mask.sum().item()
                generated_tokens = outs[i][actual_input_length:]
                generated_text = tok.decode(generated_tokens, skip_special_tokens=False)
                
                # CoTæŠ½å‡º
                cot = ""
                if "<think>" in generated_text and "</think>" in generated_text:
                    start_idx = generated_text.find("<think>") + len("<think>")
                    end_idx = generated_text.find("</think>")
                    if start_idx < end_idx:
                        cot = generated_text[start_idx:end_idx].strip()
                
                # å…¨ä½“ã‹ã‚‰æŠ½å‡º
                if not cot and "<think>" in t and "</think>" in t:
                    start_idx = t.rfind("<think>") + len("<think>")
                    end_idx = t.find("</think>", start_idx)
                    if start_idx < end_idx and end_idx != -1:
                        cot = t[start_idx:end_idx].strip()

                cots.append(cot)
            except Exception as e:
                logger.error(f"CoTæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {i+1}: {e}")
                cots.append("")
        
        return cots
        
    except Exception as e:
        logger.error(f"Unexpected error in gen_cot: {e}")
        return [""] * len(batch_rows)

try:
    batch_size = 8
    save_period  = 10 
    total_batches = (len(df_selected) + batch_size - 1) // batch_size
    logger.info(f"Processing {len(df_selected)} samples in {total_batches} batches of size {batch_size}")
    
    processed_count = 0
    
    for i in range(0, len(df_selected), batch_size): 
        batch_num = i // batch_size + 1
        logger.info(f"Processing batch {batch_num}/{total_batches}")
        
        try:
            rows = df_selected.iloc[i:i+batch_size]
            cots = gen_cot(rows)
            
            if len(cots) != len(rows):
                logger.error(f"Mismatch: got {len(cots)} CoTs for {len(rows)} rows")
                # CoTæ•°èª¿æ•´
                while len(cots) < len(rows):
                    cots.append("")
                cots = cots[:len(rows)]
            
            # å„è¡Œã® completion ã‚’æ›´æ–°
            for j, (idx, row) in enumerate(rows.iterrows()):
                try:
                    original_completion = row.completion.replace('<think></think>', '')
                    if cots[j] and cots[j].strip():
                        new_completion = f"<think>{cots[j]}</think>{original_completion}"
                    else:
                        new_completion = original_completion
                    df_selected.loc[idx, "completion"] = new_completion
                    processed_count += 1
                except Exception as e:
                    logger.error(f"Failed to update completion for row {idx}: {e}")
                    processed_count += 1  # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ã‚«ã‚¦ãƒ³ãƒˆ
            
            # ãƒãƒƒãƒå‡¦ç†å®Œäº†å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆ10ãƒãƒƒãƒã”ã¨ã€ã¾ãŸã¯æœ€çµ‚ãƒãƒƒãƒï¼‰
            if batch_num % save_period == 0 or i + batch_size >= len(df_selected):
                snapshot = df_selected.iloc[:processed_count].copy()
                outfile = f"vanilla_with_cot_batch_{batch_num:03d}.jsonl"
                snapshot.to_json(
                    outfile,
                    orient="records",
                    force_ascii=False,
                    lines=True
                )
                logger.info(
                    f"ğŸ’¾ batch {batch_num}/{total_batches} ã¾ã§å®Œäº† "
                    f"({processed_count}/{len(df_selected)}) â†’ {outfile}"
                )
                
                # 10ãƒãƒƒãƒã”ã¨ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡ºåŠ›
                if len(snapshot) > 0:
                    sample_row = snapshot.iloc[-1]  # æœ€å¾Œã«å‡¦ç†ã—ãŸã‚µãƒ³ãƒ—ãƒ«
                    logger.info("=" * 80)
                    logger.info(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ› (batch {batch_num}):")
                    logger.info(f"ğŸ”¸ Vanilla: {sample_row['vanilla'][:200]}...")
                    logger.info(f"ğŸ”¸ CoTåŸ‹è¾¼Completion: {sample_row['completion'][:300]}...")
                    logger.info("=" * 80)
            
            logger.info(f"Completed batch {batch_num}/{total_batches} (ç·å‡¦ç†æ•°: {processed_count})")
            
        except Exception as e:
            logger.error(f"Error processing batch {batch_num}: {e}")
            continue
    
    # æœ€çµ‚çš„ãªå‡¦ç†æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
    if processed_count % 100 != 0:  # æœ€å¾Œã®100ä»¶æœªæº€ã®åˆ†ã‚‚ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ğŸ¯ æœ€çµ‚: {processed_count}ä»¶ã®CoTä¿å­˜ã—ã¾ã—ãŸï¼")
    
    # çµæœã®ä¿å­˜
    output_file = "vanilla_with_cot.jsonl"
    logger.info(f"Saving results to {output_file}")
    
    try:
        df_selected.to_json(output_file, orient="records", force_ascii=False, lines=True)
        logger.info(f"âœ… Successfully saved {len(df_selected)} samples to {output_file}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        output_path = Path(output_file)
        if output_path.exists():
            file_size = output_path.stat().st_size / 1024 / 1024  # MB
            logger.info(f"Output file size: {file_size:.2f} MB")
        
    except Exception as e:
        logger.error(f"Failed to save output file: {e}")
        # pickle ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        try:
            backup_file = "vanilla_with_cot_backup.pkl"
            df_selected.to_pickle(backup_file)
            logger.info(f"Saved backup as {backup_file}")
        except Exception as backup_e:
            logger.error(f"Failed to save backup: {backup_e}")
    
except Exception as e:
    logger.error(f"Fatal error during processing: {e}")
    sys.exit(1)

logger.info("Script completed successfully")

