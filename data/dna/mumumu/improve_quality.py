import json
import pandas as pd
import torch
import logging
import sys
import os
import re
from typing import Dict, List, Tuple, Optional
from vllm import LLM, SamplingParams
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QualityImprover:
    def __init__(self, judge_model_name="Qwen/Qwen2.5-14B-Instruct"):
        """å“è³ªæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.judge_llm = None
        self.judge_model_name = judge_model_name
        self.improvement_log = []
        
    def initialize_judge_model(self):
        """åˆ¤å®šç”¨LLMã®åˆæœŸåŒ–"""
        logger.info(f"ğŸ”§ åˆ¤å®šç”¨ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­: {self.judge_model_name}")
        
        # è»½é‡ãªåˆ¤å®šç”¨ãƒ¢ãƒ‡ãƒ«
        self.judge_llm = LLM(
            model=self.judge_model_name,
            trust_remote_code=True,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.3,
            max_model_len=4096,
            max_num_seqs=4
        )
        logger.info("âœ… åˆ¤å®šç”¨ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")

    def evaluate_cot_quality(self, problem: str, cot: str, answer: str) -> Tuple[float, List[str]]:
        """CoTã®å“è³ªã‚’è‡ªå‹•è©•ä¾¡"""
        score = 0.0
        issues = []
        max_score = 6.0
        
        # 1. é•·ã•ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        cot_length = len(cot.strip())
        if 80 <= cot_length <= 800:
            score += 1.0
        elif cot_length < 80:
            issues.append(f"CoTãŒçŸ­ã™ãã¾ã™ ({cot_length}æ–‡å­—)")
        else:
            issues.append(f"CoTãŒé•·ã™ãã¾ã™ ({cot_length}æ–‡å­—)")
        
        # 2. è«–ç†çš„æ¥ç¶šè©ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        reasoning_patterns = [
            r'\b(because|since|therefore|thus|so|hence)\b',
            r'\b(first|second|third|finally|next|then)\b', 
            r'\b(ã¾ãš|æ¬¡ã«|ãã—ã¦|æœ€å¾Œã«|ãªã®ã§|ç†ç”±ã¯|å¾“ã£ã¦)\b'
        ]
        
        reasoning_count = sum(len(re.findall(pattern, cot, re.IGNORECASE)) for pattern in reasoning_patterns)
        if reasoning_count >= 2:
            score += 1.0
        else:
            issues.append(f"è«–ç†çš„æ¥ç¶šè©ãŒä¸è¶³ (æ¤œå‡ºæ•°: {reasoning_count})")
        
        # 3. å•é¡Œã¨ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        problem_words = set(re.findall(r'\w+', problem.lower()))
        cot_words = set(re.findall(r'\w+', cot.lower()))
        if len(problem_words) > 0:
            overlap_ratio = len(problem_words & cot_words) / len(problem_words)
            if overlap_ratio >= 0.25:
                score += 1.0
            else:
                issues.append(f"å•é¡Œã¨ã®é–¢é€£æ€§ãŒä½ã„ (é‡è¤‡ç‡: {overlap_ratio:.2f})")
        
        # 4. æ®µè½æ§‹é€ ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        sentences = re.split(r'[.!?ã€‚ï¼Ÿï¼]\s*', cot)
        if len(sentences) >= 3:
            score += 1.0
        else:
            issues.append(f"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ãŒä¸è¶³ (æ–‡æ•°: {len(sentences)})")
        
        # 5. å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        harmful_patterns = [
            r'\b(kill|murder|harm|attack|violence|illegal|hack)\b',
            r'\b(æ®º|å®³|æ”»æ’ƒ|é•æ³•|ãƒãƒƒã‚¯|æš´åŠ›)\b'
        ]
        
        is_harmful_request = any(re.search(pattern, problem, re.IGNORECASE) for pattern in harmful_patterns)
        has_refusal = any(phrase in cot.lower() for phrase in ["sorry", "can't help", "ç”³ã—è¨³", "ãŠæ‰‹ä¼ã„ã§ãã¾ã›ã‚“"])
        
        if is_harmful_request and has_refusal:
            score += 1.0  # é©åˆ‡ãªæ‹’å¦
        elif not is_harmful_request and not has_refusal:
            score += 1.0  # é€šå¸¸ã®å›ç­”
        else:
            if is_harmful_request and not has_refusal:
                issues.append("æœ‰å®³ãªè¦æ±‚ã«å¯¾ã—ã¦é©åˆ‡ã«æ‹’å¦ã—ã¦ã„ãªã„")
            else:
                issues.append("ç„¡å®³ãªè¦æ±‚ã«å¯¾ã—ã¦ä¸é©åˆ‡ã«æ‹’å¦ã—ã¦ã„ã‚‹")
        
        # 6. ç­”ãˆã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        answer_words = set(re.findall(r'\w+', answer.lower()))
        if len(answer_words) > 0:
            answer_overlap = len(answer_words & cot_words) / len(answer_words)
            if answer_overlap >= 0.3:
                score += 1.0
            else:
                issues.append(f"å›ç­”ã¨ã®æ•´åˆæ€§ãŒä½ã„ (é‡è¤‡ç‡: {answer_overlap:.2f})")
        
        return score / max_score, issues

    def evaluate_answer_quality(self, problem: str, answer: str) -> Tuple[float, List[str]]:
        """å›ç­”ã®å“è³ªã‚’è‡ªå‹•è©•ä¾¡"""
        score = 0.0
        issues = []
        max_score = 4.0
        
        # 1. é•·ã•ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        answer_length = len(answer.strip())
        if 20 <= answer_length <= 2000:
            score += 1.0
        elif answer_length < 20:
            issues.append(f"å›ç­”ãŒçŸ­ã™ãã¾ã™ ({answer_length}æ–‡å­—)")
        else:
            issues.append(f"å›ç­”ãŒé•·ã™ãã¾ã™ ({answer_length}æ–‡å­—)")
        
        # 2. å…·ä½“æ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        specific_patterns = [
            r'\b\d+\b',  # æ•°å­—
            r'\b(step|method|way|approach|technique|æ–¹æ³•|æ‰‹é †|ã‚¹ãƒ†ãƒƒãƒ—)\b',
            r'\b(example|instance|such as|ä¾‹ãˆã°|ä¾‹ã¨ã—ã¦)\b'
        ]
        
        specific_count = sum(len(re.findall(pattern, answer, re.IGNORECASE)) for pattern in specific_patterns)
        if specific_count >= 2:
            score += 1.0
        else:
            issues.append(f"å…·ä½“æ€§ãŒä¸è¶³ (æ¤œå‡ºæ•°: {specific_count})")
        
        # 3. å•é¡Œè§£æ±ºæ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        problem_keywords = re.findall(r'\w+', problem.lower())
        answer_addresses_problem = any(word in answer.lower() for word in problem_keywords[:5])
        
        if answer_addresses_problem:
            score += 1.0
        else:
            issues.append("å•é¡Œã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå›ç­”ãŒä¸è¶³")
        
        # 4. å®‰å…¨æ€§ã¨é©åˆ‡æ€§ãƒã‚§ãƒƒã‚¯ (0-1ç‚¹)
        harmful_patterns = [
            r'\b(kill|murder|harm|attack|violence|illegal|hack)\b',
            r'\b(æ®º|å®³|æ”»æ’ƒ|é•æ³•|ãƒãƒƒã‚¯|æš´åŠ›)\b'
        ]
        
        is_harmful_request = any(re.search(pattern, problem, re.IGNORECASE) for pattern in harmful_patterns)
        has_harmful_content = any(re.search(pattern, answer, re.IGNORECASE) for pattern in harmful_patterns)
        has_refusal = any(phrase in answer.lower() for phrase in ["sorry", "can't help", "ç”³ã—è¨³", "ãŠæ‰‹ä¼ã„ã§ãã¾ã›ã‚“"])
        
        if is_harmful_request:
            if has_refusal and not has_harmful_content:
                score += 1.0  # é©åˆ‡ãªæ‹’å¦
            else:
                issues.append("æœ‰å®³ãªè¦æ±‚ã«å¯¾ã—ã¦é©åˆ‡ã«æ‹’å¦ã—ã¦ã„ãªã„")
        else:
            if not has_harmful_content:
                score += 1.0  # å®‰å…¨ãªå›ç­”
            else:
                issues.append("ç„¡å®³ãªè¦æ±‚ã«å¯¾ã—ã¦æœ‰å®³ãªå†…å®¹ã‚’å«ã‚€")
        
        return score / max_score, issues

    def llm_judge_quality(self, problem: str, cot: str, answer: str) -> float:
        """LLMã«ã‚ˆã‚‹å“è³ªåˆ¤å®š"""
        if self.judge_llm is None:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
        
        judge_prompt = f"""ã‚ãªãŸã¯å“è³ªè©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã¨å›ç­”ã‚’1-5ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {problem}

æ¨è«–: {cot}

å›ç­”: {answer}

è©•ä¾¡åŸºæº–:
1. è«–ç†çš„ä¸€è²«æ€§ (æ¨è«–ãŒè«–ç†çš„ã«æ­£ã—ã„ã‹)
2. é–¢é€£æ€§ (å•é¡Œã«å¯¾ã—ã¦é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹)  
3. æ˜ç¢ºæ€§ (ç†è§£ã—ã‚„ã™ãæ˜ç¢ºã‹)
4. å®Œå…¨æ€§ (ååˆ†ãªèª¬æ˜ãŒã‚ã‚‹ã‹)
5. å®‰å…¨æ€§ (æœ‰å®³ã§ãªã„å†…å®¹ã‹)

"SCORE: X.X" ã®å½¢å¼ã§å¹³å‡ã‚¹ã‚³ã‚¢ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        try:
            sampling_params = SamplingParams(
                temperature=0.1,
                max_tokens=50,
                stop=["\n", "ã€‚"]
            )
            
            result = self.judge_llm.generate(judge_prompt, sampling_params)[0].outputs[0].text
            score_match = re.search(r'SCORE:\s*(\d+\.?\d*)', result)
            
            if score_match:
                score = float(score_match.group(1))
                return min(max(score / 5.0, 0.0), 1.0)  # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
            else:
                logger.warning(f"ã‚¹ã‚³ã‚¢æŠ½å‡ºå¤±æ•—: {result}")
                return 0.5
                
        except Exception as e:
            logger.error(f"LLMåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5

    def improve_cot(self, problem: str, answer: str, current_cot: str, attempt: int = 0) -> str:
        """CoTã‚’æ”¹å–„"""
        improvement_prompts = [
            # åŸºæœ¬æ”¹å–„
            f"""## System Role
ã‚ãªãŸã¯è«–ç†çš„æ€è€ƒã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå•é¡Œã¨å›ç­”ã«å¯¾ã—ã¦ã€ã‚ˆã‚Šè©³ç´°ã§è«–ç†çš„ãªæ¨è«–éç¨‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º
- å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¢ºã«åŒºåˆ‡ã‚‹
- "ã¾ãš"ã€"æ¬¡ã«"ã€"ãã—ã¦"ã€"æœ€å¾Œã«"ãªã©ã®æ¥ç¶šè©ã‚’ä½¿ç”¨
- æ¨è«–ã®æ ¹æ‹ ã‚’æ˜ç¢ºã«ç¤ºã™
- 100-400æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«

## å•é¡Œ
{problem}

## å›ç­”  
{answer}

## æ”¹å–„ã•ã‚ŒãŸæ¨è«–
<think>""",
            
            # è©³ç´°åŒ–
            f"""## System Role
ã‚ãªãŸã¯æ•™è‚²ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å•é¡Œã«å¯¾ã—ã¦ã‚ˆã‚Šè©³ç´°ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ®µéšçš„æ¨è«–ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º
- ã‚ˆã‚Šå…·ä½“çš„ãªä¾‹ã‚„èª¬æ˜ã‚’å«ã‚ã‚‹
- å„æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®ç†ç”±ã‚’æ˜ç¢ºã«ã™ã‚‹
- èª­è€…ãŒç†è§£ã—ã‚„ã™ã„æ§‹é€ ã«ã™ã‚‹

## å•é¡Œ
{problem}

## ç›®æ¨™å›ç­”
{answer}

## è©³ç´°æ¨è«–
<think>""",
            
            # æ§‹é€ åŒ–
            f"""## System Role  
ã‚ãªãŸã¯æ§‹é€ åŒ–æ€è€ƒã®å°‚é–€å®¶ã§ã™ã€‚å•é¡Œã‚’ä½“ç³»çš„ã«åˆ†æã—ã€è«–ç†çš„ãªæ¨è«–ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º
- å•é¡Œã‚’è¦ç´ åˆ†è§£ã™ã‚‹
- å„è¦ç´ ã‚’é †åºç«‹ã¦ã¦æ¤œè¨ã™ã‚‹
- çµè«–ã«è‡³ã‚‹æ˜ç¢ºãªè«–ç†çš„é“ç­‹ã‚’ç¤ºã™

## å•é¡Œåˆ†æå¯¾è±¡
{problem}

## æœŸå¾…çµæœ
{answer}

## æ§‹é€ åŒ–æ¨è«–
<think>"""
        ]
        
        if attempt < len(improvement_prompts):
            prompt = improvement_prompts[attempt]
        else:
            prompt = improvement_prompts[0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
        return prompt

    def improve_answer(self, problem: str, current_answer: str, attempt: int = 0) -> str:
        """å›ç­”ã‚’æ”¹å–„"""
        improvement_prompts = [
            # åŸºæœ¬æ”¹å–„
            f"""## System Role
ã‚ãªãŸã¯å›ç­”å“è³ªã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå•é¡Œã«å¯¾ã—ã¦ã‚ˆã‚Šè‰¯ã„å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º
- ã‚ˆã‚Šå…·ä½“çš„ã§å®Ÿç”¨çš„ãªå›ç­”ã«ã™ã‚‹
- é©åˆ‡ãªé•·ã•ï¼ˆ50-500æ–‡å­—ç¨‹åº¦ï¼‰ã‚’ä¿ã¤
- å•é¡Œã«ç›´æ¥ç­”ãˆã‚‹
- å¿…è¦ã«å¿œã˜ã¦å®‰å…¨æ€§ã«é…æ…®ã™ã‚‹

## å•é¡Œ
{problem}

## ç¾åœ¨ã®å›ç­”
{current_answer}

## æ”¹å–„ã•ã‚ŒãŸå›ç­”
""",
            
            # è©³ç´°åŒ–
            f"""## System Role
ã‚ãªãŸã¯è©³ç´°èª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å•é¡Œã«ã‚ˆã‚Šè©³ã—ãä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º  
- ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹
- å…·ä½“ä¾‹ã‚„æ‰‹é †ã‚’å«ã‚ã‚‹
- èª­è€…ã«ã¨ã£ã¦å®Ÿç”¨çš„ãªå†…å®¹ã«ã™ã‚‹

## å•é¡Œ
{problem}

## å…ƒå›ç­”
{current_answer}

## è©³ç´°å›ç­”
""",
            
            # æ§‹é€ åŒ–
            f"""## System Role
ã‚ãªãŸã¯æ§‹é€ åŒ–å›ç­”ã®å°‚é–€å®¶ã§ã™ã€‚å•é¡Œã«å¯¾ã—ã¦æ•´ç†ã•ã‚ŒãŸå½¢ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

## æ”¹å–„æŒ‡ç¤º
- è¦ç‚¹ã‚’æ•´ç†ã—ã¦æç¤ºã™ã‚‹
- å¿…è¦ã«å¿œã˜ã¦ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’ä½¿ç”¨
- æ˜ç¢ºã§èª­ã¿ã‚„ã™ã„æ§‹é€ ã«ã™ã‚‹

## è³ªå•
{problem}

## åŸºç¤å›ç­”
{current_answer}

## æ§‹é€ åŒ–å›ç­”
"""
        ]
        
        if attempt < len(improvement_prompts):
            return improvement_prompts[attempt]
        else:
            return improvement_prompts[0]

    def process_item(self, item: Dict, improvement_llm: LLM) -> Tuple[Dict, bool]:
        """å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ ã®å“è³ªæ”¹å–„å‡¦ç†"""
        item_id = item.get("id", "unknown")
        problem = item.get("problem", "")
        current_output = item.get("output", "")
        current_answer = item.get("answer", "")
        
        # <think>ã‚¿ã‚°ã‹ã‚‰CoTã‚’æŠ½å‡º
        cot_match = re.search(r'<think>(.*?)</think>', current_output, re.DOTALL)
        current_cot = cot_match.group(1).strip() if cot_match else ""
        
        # ç¾åœ¨ã®å“è³ªã‚’è©•ä¾¡
        cot_score, cot_issues = self.evaluate_cot_quality(problem, current_cot, current_answer)
        answer_score, answer_issues = self.evaluate_answer_quality(problem, current_answer)
        llm_score = self.llm_judge_quality(problem, current_cot, current_answer)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = (cot_score * 0.4 + answer_score * 0.3 + llm_score * 0.3)
        
        logger.info(f"ğŸ“Š ID {item_id} - CoT: {cot_score:.2f}, Answer: {answer_score:.2f}, LLM: {llm_score:.2f}, ç·åˆ: {overall_score:.2f}")
        
        # æ”¹å–„ãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®š
        needs_improvement = overall_score < 0.7
        improved = False
        improvement_reasons = []
        
        if needs_improvement:
            logger.info(f"ğŸ”§ ID {item_id} æ”¹å–„é–‹å§‹ - ã‚¹ã‚³ã‚¢ä¸è¶³: {overall_score:.2f}")
            
            best_cot = current_cot
            best_answer = current_answer
            best_score = overall_score
            
            # CoTæ”¹å–„
            if cot_score < 0.7:
                improvement_reasons.extend(cot_issues)
                for attempt in range(2):  # æœ€å¤§2å›è©¦è¡Œ
                    try:
                        improve_prompt = self.improve_cot(problem, current_answer, current_cot, attempt)
                        
                        sampling_params = SamplingParams(
                            temperature=0.3 + (attempt * 0.2),
                            max_tokens=600,
                            stop=["</think>"]
                        )
                        
                        result = improvement_llm.generate(improve_prompt, sampling_params)[0].outputs[0].text
                        new_cot = result.strip()
                        
                        # æ–°ã—ã„CoTã‚’è©•ä¾¡
                        new_cot_score, _ = self.evaluate_cot_quality(problem, new_cot, current_answer)
                        
                        if new_cot_score > cot_score:
                            best_cot = new_cot
                            cot_score = new_cot_score
                            logger.info(f"  âœ… CoTæ”¹å–„æˆåŠŸ (è©¦è¡Œ{attempt+1}): {new_cot_score:.2f}")
                            break
                        else:
                            logger.info(f"  âš ï¸ CoTæ”¹å–„å¤±æ•— (è©¦è¡Œ{attempt+1}): {new_cot_score:.2f}")
                            
                    except Exception as e:
                        logger.error(f"  âŒ CoTæ”¹å–„ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{attempt+1}): {e}")
            
            # Answeræ”¹å–„
            if answer_score < 0.7:
                improvement_reasons.extend(answer_issues)
                for attempt in range(2):  # æœ€å¤§2å›è©¦è¡Œ
                    try:
                        improve_prompt = self.improve_answer(problem, current_answer, attempt)
                        
                        sampling_params = SamplingParams(
                            temperature=0.3 + (attempt * 0.2),
                            max_tokens=800,
                            stop=["\n\n", "##"]
                        )
                        
                        result = improvement_llm.generate(improve_prompt, sampling_params)[0].outputs[0].text
                        new_answer = result.strip()
                        
                        # æ–°ã—ã„å›ç­”ã‚’è©•ä¾¡
                        new_answer_score, _ = self.evaluate_answer_quality(problem, new_answer)
                        
                        if new_answer_score > answer_score:
                            best_answer = new_answer
                            answer_score = new_answer_score
                            logger.info(f"  âœ… Answeræ”¹å–„æˆåŠŸ (è©¦è¡Œ{attempt+1}): {new_answer_score:.2f}")
                            break
                        else:
                            logger.info(f"  âš ï¸ Answeræ”¹å–„å¤±æ•— (è©¦è¡Œ{attempt+1}): {new_answer_score:.2f}")
                            
                    except Exception as e:
                        logger.error(f"  âŒ Answeræ”¹å–„ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{attempt+1}): {e}")
            
            # æœ€çµ‚è©•ä¾¡
            final_llm_score = self.llm_judge_quality(problem, best_cot, best_answer)
            final_score = (cot_score * 0.4 + answer_score * 0.3 + final_llm_score * 0.3)
            
            if final_score > overall_score:
                # æ”¹å–„ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿”ã™
                improved_output = f"<think>{best_cot}</think>{best_answer}"
                improved_item = item.copy()
                improved_item["output"] = improved_output
                improved_item["answer"] = best_answer
                
                improvement_log = {
                    "id": item_id,
                    "timestamp": datetime.now().isoformat(),
                    "original_score": overall_score,
                    "improved_score": final_score,
                    "improvement": final_score - overall_score,
                    "reasons": improvement_reasons,
                    "cot_improved": best_cot != current_cot,
                    "answer_improved": best_answer != current_answer
                }
                
                self.improvement_log.append(improvement_log)
                
                logger.info(f"ğŸ¯ ID {item_id} æ”¹å–„å®Œäº†: {overall_score:.2f} â†’ {final_score:.2f} (+{final_score-overall_score:.2f})")
                return improved_item, True
            else:
                logger.info(f"ğŸ”„ ID {item_id} æ”¹å–„åŠ¹æœãªã—: {overall_score:.2f}")
        
        return item, improved

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
    input_file = "vanilla_with_cot_vllm.jsonl"
    if not os.path.exists(input_file):
        logger.error(f"âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        logger.error("å…ˆã« generate_cot.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {input_file}")
    data = []
    with open(input_file, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                if line.strip():
                    data.append(json.loads(line))
            except json.JSONDecodeError as e:
                logger.warning(f"JSON parse error at line {line_num}: {e}")
    
    logger.info(f"âœ… {len(data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
    
    # GPUæƒ…å ±ç¢ºèª
    logger.info("ğŸ” GPUæƒ…å ±ã‚’ç¢ºèªä¸­...")
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info(f"ğŸ¯ åˆ©ç”¨å¯èƒ½GPUæ•°: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            logger.info(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    else:
        logger.error("âŒ CUDAåˆ©ç”¨ä¸å¯")
        sys.exit(1)
    
    # å“è³ªæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    improver = QualityImprover()
    improver.initialize_judge_model()
    
    # æ”¹å–„ç”¨LLMåˆæœŸåŒ–
    logger.info("ğŸš€ æ”¹å–„ç”¨ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    improvement_llm = LLM(
        model="Qwen/Qwen3-32B",
        trust_remote_code=True,
        tensor_parallel_size=2,  # 2GPUä½¿ç”¨
        gpu_memory_utilization=0.8,
        max_model_len=6144,
        max_num_seqs=8
    )
    logger.info("âœ… æ”¹å–„ç”¨ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
    
    # å“è³ªæ”¹å–„å‡¦ç†
    improved_data = []
    improvement_count = 0
    
    logger.info("ğŸ”§ å“è³ªæ”¹å–„å‡¦ç†é–‹å§‹...")
    
    for i, item in enumerate(data):
        logger.info(f"ğŸ“ å‡¦ç†ä¸­: {i+1}/{len(data)} (ID: {item.get('id', 'unknown')})")
        
        improved_item, was_improved = improver.process_item(item, improvement_llm)
        improved_data.append(improved_item)
        
        if was_improved:
            improvement_count += 1
        
        # é€²æ—è¡¨ç¤º
        if (i + 1) % 50 == 0:
            logger.info(f"ğŸ“Š é€²æ—: {i+1}/{len(data)} å®Œäº†, æ”¹å–„ä»¶æ•°: {improvement_count}")
    
    # çµæœä¿å­˜
    output_file = "improved_vanilla_with_cot.jsonl"
    logger.info(f"ğŸ’¾ æ”¹å–„ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­: {output_file}")
    
    with open(output_file, "w", encoding="utf-8") as f:
        for item in improved_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # æ”¹å–„ãƒ­ã‚°ä¿å­˜
    log_file = f"improvement_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    logger.info(f"ğŸ“‹ æ”¹å–„ãƒ­ã‚°ä¿å­˜ä¸­: {log_file}")
    
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump({
            "summary": {
                "total_items": len(data),
                "improved_items": improvement_count,
                "improvement_rate": improvement_count / len(data) if data else 0
            },
            "improvements": improver.improvement_log
        }, f, ensure_ascii=False, indent=2)
    
    # çµ±è¨ˆå‡ºåŠ›
    logger.info("ğŸ¯ å“è³ªæ”¹å–„å®Œäº†!")
    logger.info(f"  ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
    logger.info(f"  âœ… æ”¹å–„ä»¶æ•°: {improvement_count}")
    logger.info(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement_count/len(data)*100:.1f}%")
    logger.info(f"  ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
    logger.info(f"  ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")

if __name__ == "__main__":
    main()