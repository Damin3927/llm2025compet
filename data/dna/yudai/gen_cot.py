#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fill_think_qwen3_both_from_answer.py
-------------------------------------------
- å˜ä¸€JSON(dict or list) / JSONL ã®ä¸¡å¯¾å¿œ
- preferred_output / non_preferred_output ã®ä¸¡æ–¹ã«ã€è§£ç­”ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å°å‡ºã—ãŸCoT(<think>â€¦</think>)ã‚’åŸ‹ã‚ã‚‹
- æ—¢ã«éç©ºã® <think>â€¦</think> ãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„ï¼ˆç©ºã®ã¿åŸ‹ã‚ã‚‹ or ç„¡ã„å ´åˆã¯å…ˆé ­ä»˜ä¸ï¼‰
- Qwen/Qwen3-32Bï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰ã‚’ vLLM ã§ä½¿ç”¨
-------------------------------------------
"""

import os, json, re, time, hashlib, traceback
from pathlib import Path
from typing import Dict, Optional, List, Any, Set

from vllm import LLM, SamplingParams

# =========================
# ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆã“ã“ã ã‘ç·¨é›†ï¼‰
# =========================
CONFIG = {
    "INPUT_PATH": "/home/Competition2025/P02/P02U011/test/inputs/out_00000.jsonl",
    "OUTPUT_PATH": "/home/Competition2025/P02/P02U011/test/outputs/train_part_00000_filled.jsonl",

    # vLLM / ãƒ¢ãƒ‡ãƒ«
    "MODEL": "Qwen/Qwen3-32B",
    "TP": 8,
    "MAX_MODEL_LEN": 8192,
    "MAX_NUM_SEQS": 4,
    "GPU_UTIL": 0.90,

    # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    "MAX_TOKENS": 512,       # CoTç”¨ãªã®ã§çŸ­ã‚ã§OK
    "TEMPERATURE": 0.2,
    "TOP_P": 0.9,
    "SEED": 42,

    # ã©ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†ã™ã‚‹ã‹ï¼ˆé †ç•ªå³å®ˆï¼‰
    "FIELDS": ["preferred_output", "non_preferred_output"],

    # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
    "RETRY_MAX": 3,
    "RETRY_SLEEP": 2.0,  # ç§’ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
}

os.environ.setdefault("PYTHONUNBUFFERED", "1")

# =========================
# ãƒ˜ãƒ«ãƒ‘
# =========================
THINK_OPEN = r"<think>"
THINK_CLOSE = r"</think>"

def has_nonempty_think(s: str) -> bool:
    if not isinstance(s, str):
        return False
    m = re.search(r"<think>(.*?)</think>", s, flags=re.DOTALL|re.IGNORECASE)
    return bool(m and m.group(1).strip())

def has_empty_think(s: str) -> bool:
    if not isinstance(s, str):
        return False
    m = re.search(r"<think>(.*?)</think>", s, flags=re.DOTALL|re.IGNORECASE)
    return bool(m and not m.group(1).strip())

def strip_all_think_blocks(s: str) -> str:
    """ã™ã¹ã¦ã® <think>â€¦</think> ã‚’é™¤å»ã—ã¦æœ€çµ‚å‡ºåŠ›ã®ã¿ã‚’è¿”ã™ã€‚"""
    if not isinstance(s, str):
        return ""
    return re.sub(r"<think>.*?</think>", "", s, flags=re.DOTALL|re.IGNORECASE).strip()

def insert_or_replace_think(answer: str, think_text: str) -> str:
    """
    - æ—¢ã«éç©ºã® <think> ãŒã‚ã‚‹: ãã®ã¾ã¾è¿”ã™ï¼ˆä¸Šæ›¸ãã—ãªã„ï¼‰
    - ç©ºã® <think></think> ãŒã‚ã‚‹: æœ€åˆã®ç©º think ã‚’åŸ‹ã‚ã‚‹
    - think ãŒãªã„: å…ˆé ­ã« <think>â€¦</think> ã‚’ä»˜ä¸
    """
    if not isinstance(answer, str):
        answer = "" if answer is None else str(answer)

    if has_nonempty_think(answer):
        return answer

    if has_empty_think(answer):
        # æœ€åˆã®ç©ºthinkã ã‘ç½®æ›
        def repl_once(m):
            inner = m.group(1)
            if inner.strip() == "":
                return f"<think>{think_text}</think>"
            return m.group(0)
        return re.sub(r"<think>(.*?)</think>", repl_once, answer, count=1, flags=re.DOTALL|re.IGNORECASE)

    # ç„¡ã„å ´åˆã¯å…ˆé ­ã«ä»˜ä¸
    return f"<think>{think_text}</think>" + (answer if answer.startswith("\n") else ("\n" + answer if answer else ""))

def valid_reasoning(text: str) -> bool:
    if not text or not text.strip():
        return False
    t = text.strip()
    if len(t) < 15:
        return False
    # ã‚¿ã‚°æ··å…¥ã¯NGï¼ˆã“ã¡ã‚‰ã§ <think> ã‚’ä»˜åŠ ã™ã‚‹ï¼‰
    if re.search(r"</?think>|</?final>", t, flags=re.IGNORECASE):
        return False
    return True

def build_prompt_for_answer(answer_text: str) -> str:
    """
    ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‘ã‘ï¼ˆãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã€‚
    - å›ç­”æ–‡ã®å†…å®¹ã«åŸºã¥ã â€œæ‰‹é †ãƒ»æ ¹æ‹ ã®çŸ­ã„ã‚¹ãƒ†ãƒƒãƒ—è¦ç´„â€ ã‚’ä½œã‚‰ã›ã‚‹
    - æ–°ã—ã„äº‹å®Ÿã‚’è¿½åŠ ã—ãªã„ã“ã¨ã‚’å¼·èª¿
    - ã‚¿ã‚°ç¦æ­¢
    """
    return (
        "Write ONLY the hidden step-by-step reasoning that led to the following answer.\n"
        "- Use ONLY information present in the answer; do NOT introduce new facts.\n"
        "- Focus on brief numbered steps or concise plan (setup -> settings -> test -> safety/validation).\n"
        "- Output ONLY the reasoning text (no XML/HTML tags, no final answer, no headings).\n\n"
        "Answer:\n"
        f"{answer_text}\n\n"
        "Reasoning:"
    )

def batched_generate(llm: LLM, sp: SamplingParams, prompts: List[str]) -> List[str]:
    outs = llm.generate(prompts, sampling_params=sp)
    texts = []
    for o in outs:
        t = o.outputs[0].text.strip() if o.outputs else ""
        texts.append(t)
    return texts

def gen_reasoning_with_retry(llm: LLM, sp: SamplingParams, answer_text: str,
                             retry_max: int, retry_sleep: float) -> str:
    prompt = build_prompt_for_answer(strip_all_think_blocks(answer_text))
    last = ""
    for i in range(retry_max):
        out = batched_generate(llm, sp, [prompt])[0]
        last = out
        if valid_reasoning(out):
            return out
        time.sleep(retry_sleep * (2 ** i))
    return last  # æœ€å¾Œã®å‡ºåŠ›ã‚’è¿”ã™ï¼ˆä¸‹æµã§validityãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def atomic_append(path: Path, line: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(line + "\n")
        f.flush()
        os.fsync(f.fileno())

def load_existing_ids(output_path: Path) -> Set[Any]:
    ids: Set[Any] = set()
    if output_path.exists():
        with output_path.open("r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    obj = json.loads(line)
                    if "id" in obj:
                        ids.add(obj["id"])
                except Exception:
                    continue
    return ids

def derive_output_from_input(input_path: Path) -> Path:
    stem = input_path.stem
    return input_path.with_name(f"{stem}_filled_qwen3-32B.jsonl")

# =========================
# æœ¬ä½“
# =========================
def main():
    in_path = Path(CONFIG["INPUT_PATH"])
    out_path = Path(CONFIG["OUTPUT_PATH"]) if CONFIG["OUTPUT_PATH"] else derive_output_from_input(in_path)

    print(f"[INFO] INPUT : {in_path}", flush=True)
    print(f"[INFO] OUTPUT: {out_path}", flush=True)

    # vLLM åˆæœŸåŒ–
    llm = LLM(
        model=CONFIG["MODEL"],
        trust_remote_code=True,
        tensor_parallel_size=CONFIG["TP"],
        gpu_memory_utilization=CONFIG["GPU_UTIL"],
        max_model_len=CONFIG["MAX_MODEL_LEN"],
        max_num_seqs=CONFIG["MAX_NUM_SEQS"],
    )
    sp = SamplingParams(
        temperature=CONFIG["TEMPERATURE"],
        top_p=CONFIG["TOP_P"],
        max_tokens=CONFIG["MAX_TOKENS"],
        # ã‚¿ã‚°ã‚’å‡ºã•ã›ãªã„ãƒ»æ€è€ƒã ã‘çŸ­ã‚ã«åˆ‡ã‚‹
        stop=["</think>", "<think>", "<final>", "</final>"],
        seed=CONFIG["SEED"],
    )

    existing_ids = load_existing_ids(out_path)
    print(f"[INFO] already processed: {len(existing_ids)} (skip)", flush=True)

    is_jsonl = in_path.suffix.lower() == ".jsonl"
    n_in = n_ok = n_skip = n_fail = 0

    def handle_one(rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        changed_any = False
        for key in CONFIG["FIELDS"]:
            if key not in rec or not isinstance(rec[key], str):
                continue

            text = rec[key]
            # ã™ã§ã«ä¸­èº«ã‚ã‚Šã® think ã¯è§¦ã‚‰ãªã„
            if has_nonempty_think(text):
                continue

            # CoTç”Ÿæˆï¼ˆå›ç­”ã‹ã‚‰å°å‡ºï¼‰
            reasoning = gen_reasoning_with_retry(
                llm, sp, text, CONFIG["RETRY_MAX"], CONFIG["RETRY_SLEEP"]
            )
            if not valid_reasoning(reasoning):
                # åŸ‹ã‚å¤±æ•—ï¼ˆæ¬¡ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¸ï¼‰
                continue

            rec[key] = insert_or_replace_think(text, reasoning)
            changed_any = True

        # å°‘ãªãã¨ã‚‚ preferred_output ã¯éç©ºthinkã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’OKæ¡ä»¶ã«
        ok_pref = ("preferred_output" in rec) and has_nonempty_think(rec["preferred_output"])
        ok_nonp = True
        if "non_preferred_output" in CONFIG["FIELDS"]:
            ok_nonp = ("non_preferred_output" in rec) and has_nonempty_think(rec["non_preferred_output"])

        return rec if (ok_pref and ok_nonp) else (rec if changed_any else None)

    try:
        if is_jsonl:
            with in_path.open("r", encoding="utf-8") as fin:
                for line in fin:
                    if not line.strip():
                        continue
                    n_in += 1
                    try:
                        rec = json.loads(line)
                        rec_id = rec.get("id", sha1(line))
                        if rec_id in existing_ids:
                            n_skip += 1
                            continue

                        out = handle_one(rec)
                        if out is None:
                            n_fail += 1
                            continue

                        atomic_append(out_path, json.dumps(out, ensure_ascii=False))
                        existing_ids.add(rec_id)
                        n_ok += 1
                        if n_ok % 10 == 0:
                            print(f"[PROGRESS] ok={n_ok} skip={n_skip} fail={n_fail}", flush=True)
                    except Exception:
                        traceback.print_exc()
                        n_fail += 1
        else:
            # å˜ä¸€JSONï¼ˆdict or listï¼‰ â†’ 1è¡Œãšã¤JSONLã«è¿½è¨˜
            with in_path.open("r", encoding="utf-8") as f:
                obj = json.load(f)

            if isinstance(obj, dict):
                n_in = 1
                rec = obj
                rec_id = rec.get("id", sha1(json.dumps(rec, ensure_ascii=False)))
                if rec_id in existing_ids:
                    n_skip += 1
                else:
                    try:
                        out = handle_one(rec)
                        if out is None:
                            n_fail += 1
                        else:
                            atomic_append(out_path, json.dumps(out, ensure_ascii=False))
                            existing_ids.add(rec_id)
                            n_ok += 1
                    except Exception:
                        traceback.print_exc()
                        n_fail += 1
            elif isinstance(obj, list):
                for rec in obj:
                    if not isinstance(rec, dict):
                        continue
                    n_in += 1
                    rec_id = rec.get("id", sha1(json.dumps(rec, ensure_ascii=False)))
                    if rec_id in existing_ids:
                        n_skip += 1
                        continue
                    try:
                        out = handle_one(rec)
                        if out is None:
                            n_fail += 1
                        else:
                            atomic_append(out_path, json.dumps(out, ensure_ascii=False))
                            existing_ids.add(rec_id)
                            n_ok += 1
                            if n_ok % 10 == 0:
                                print(f"[PROGRESS] ok={n_ok} skip={n_skip} fail={n_fail}", flush=True)
                    except Exception:
                        traceback.print_exc()
                        n_fail += 1
            else:
                print("Unsupported JSON root type (dict/list expected).", flush=True)
    finally:
        print(f"âœ… Done: in={n_in} ok={n_ok} skip={n_skip} fail={n_fail}", flush=True)
        print(f"ğŸ“ Output: {out_path}", flush=True)

if __name__ == "__main__":
    main()
